[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 355: Regression Analysis",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is the center for online portions of Regression 355, taught by Kelsey Carlston at Gonzaga University."
  },
  {
    "objectID": "02_assignments/lab-03.html",
    "href": "02_assignments/lab-03.html",
    "title": "Lab 3: Introduction to R",
    "section": "",
    "text": "In this lab, you will learn\n\nhow to use RStudio\nhow to make a plot with R\nhow to do math in R, create objects, use functions, etc.,\nins and outs of a typical workflow in R\n\n\n\n\nNo additional packages required this week.\n\n\n\n\nhouse prices\ncounty elections\n\n\n\n\nFor this assignment, the only thing you will turn in on Canvas is a brief write up detailed at the end of the assignment. You will use R to create some statistics based on a new dataset."
  },
  {
    "objectID": "02_assignments/lab-03.html#working-in-rstudio",
    "href": "02_assignments/lab-03.html#working-in-rstudio",
    "title": "Lab 3: Introduction to R",
    "section": "Working in RStudio",
    "text": "Working in RStudio\n\n\nIf you are going to do anything with R, RStudio is hands-down the best place to do it. RStudio is an open-source integrated development environment (or IDE) that makes programming in R simpler, more efficient, and most importantly, more reproducible. Some of its more user-friendly features are syntax highlighting (it displays code in different colors depending on what it is or does, which makes it easier for you to navigate the code that you’ve written), code completion (it will try to guess what code you are attempting to write and write it for you), and keyboard shortcuts for the more repetitive tasks.\n\nPane layout\nWhen you first open RStudio, you should see three window panes: the Console, the Environment, and the Viewer. If you open an R script, a fourth Source pane will also open. The default layout of these panes is shown in the figure above.\n\nSource. The Source pane provides basic text editing functionality, allowing you to create and edit R scripts. Importantly, you cannot execute the code in these scripts directly, but you can save the scripts that you write as simple text files. A dead give away that you have an R script living on your computer is the .R extension, for example, my_script.R.\n\nConsole. The Console pane, as its name suggests, provides an interface to the R console, which is where your code actually gets run. While you can type R code directly into the console, you can’t save the R code you write there into an R script like you can with the Source editor. That means you should reserve the console for non-essential tasks, meaning tasks that are not required to replicate your results.\nEnvironment. The Environment pane is sort of like a census of your digital zoo, providing a list of its denizens, i.e., the objects that you have created during your session. This pane also has the History tab, which shows the R code you have sent to the console in the order that you sent it.\n\nViewer. The Viewer pane is a bit of a catch-all, including a Files tab, a Plots tab, a Help tab, and a Viewer tab.\n\nThe Files tab works like a file explorer. You can use it to navigate through folders and directories. By default, it is set to your working directory.\nThe Plots tab displays any figures you make with R.\nThe Help tab is where you can go to find helpful R documentation, including function pages and vignettes.\n\n\nLet’s try out a few bits of code just to give you a sense of the difference between Source and Console.\n\nAs you work through this lab, you can practice running code in the Console, but make sure to do the actual exercises in an R script.\n\n\n\nExercises\n\nFirst, let’s open a new R script. To open an R script in RStudio, just click File > New File > R Script (or hit Ctrl + Shift + N, Cmd + Shift + N on Mac OS).\nCopy this code into the console and hit Enter.\n\n\nrep(\"Boba Fett\", 5)\n\n\nNow, copy that code into the R script you just opened and hit Enter again. As you see, the code does not run. Instead, the cursor moves down to the next line. To actually run the code, put the cursor back on the line with the code, and hit Ctrl + Enter (CMD + Enter on Mac OS)."
  },
  {
    "objectID": "02_assignments/lab-03.html#setting-up-your-r-script",
    "href": "02_assignments/lab-03.html#setting-up-your-r-script",
    "title": "Lab 3: Introduction to R",
    "section": "Setting up your R script",
    "text": "Setting up your R script\nWorking from an R script rather than the console is often preferred for several key reasons:\n\nReproducibility: An R script allows you to save all the code you write. This means you can easily rerun analyses, share your work, or troubleshoot without retyping commands. The console doesn’t keep a persistent record of the work unless you manually save it.\nEfficiency: You can run large chunks of code or the entire script at once from an R script. In contrast, the console requires you to run each command one by one, which can slow you down, especially when working on complex analyses.\nError Tracking and Debugging: R scripts make it easier to track where things go wrong. You can add comments and sections, making the code easier to read and understand. If something fails, you can simply tweak a few lines and rerun without needing to rewrite everything.\nVersion Control: Scripts can be saved with comments about changes, making it easier to keep track of different versions of an analysis. The console provides no such history, meaning it’s harder to track revisions or recover earlier work.\nDocumentation and Collaboration: Writing code in an R script allows you to add detailed comments to explain your reasoning and the steps involved. This is especially useful when collaborating with others or when you return to a project after some time.\nOrganization: An R script enables you to organize your code logically, group tasks together, and maintain an overview of the entire workflow. The console can feel disorganized with scattered commands and no easy way to structure the work.\n\nFor students, especially those learning R, it’s a good habit to get comfortable with scripts early on. When you start a new assignment or project, you should create a header that includes a description of the code in the comments. To create comments (lines in the code that do not run), use the pound sign (#).\n\n#### Lab 3: Introduction to R ####\n# This code will use some basic functions in R and create a plot"
  },
  {
    "objectID": "02_assignments/lab-03.html#r-basics",
    "href": "02_assignments/lab-03.html#r-basics",
    "title": "Lab 3: Introduction to R",
    "section": "R Basics",
    "text": "R Basics\n\nR is a calculator\nYou can just do math with it:\n\n300 * (2/25)\n\n[1] 24\n\n3^2 + 42\n\n[1] 51\n\nsin(17)\n\n[1] -0.9613975\n\n\n\n\nObjects and Functions\nBut, R is more than just a calculator. There are a lot of things you can make with R, and a lot of things you can do with it. The things that you make are called objects, and the things that you do with objects are called functions. Any complex statistical operation you want to conduct in R will almost certainly involve the use of one or more functions.\n\nCalling functions\nTo use a function, we call it like this:\n\nfunction_name(arg1 = value1, arg2 = value2, ...)\n\nTry calling the seq() function.\n\nseq(from = 1, to = 5)\n\n[1] 1 2 3 4 5\n\n\nAs you can see, this generates a sequence of numbers starting at 1 and ending at 5. There are two things to note about this. First, we do not have to specify the arguments explicitly, but they must be in the correct order:\n\nseq(1, 5) \n\n[1] 1 2 3 4 5\n\nseq(5, 1)\n\n[1] 5 4 3 2 1\n\n\nSecond, the seq() function has additional arguments you can specify, like by and length. While we do not have to specify these because they have default values, you can change one or the other (but not at the same time!):\n\nseq(1, 10, by = 2)\n\n[1] 1 3 5 7 9\n\nseq(1, 10, length = 3)\n\n[1]  1.0  5.5 10.0\n\n\n\n\nCreating objects\nTo make an object in R, you use the arrow, <-, like so:\n\nobject_name <- value\n\nTry creating an object with value 5.137 and assigning it to the name bob, like this:\n\nbob <- 5.137\n\nThere are three things to note here. First, names in R must start with a letter and can only contain letters, numbers, underscores, and periods.\n\n# Good\nwinter_solder <- \"Buckey\"\nobject4 <- 23.2\n\n# Bad\nwinter soldier <- \"Buckey\" # spaces not allowed\n4object <- 23.2            # cannot start with a number\n\nSecond, when you create an object with <-, it ends up in your workspace or environment (you can see it in the RStudio environment pane). Finally, it is worth noting that the advantage of creating objects is that we can take the output of one function and pass it to another.\n\nx <- seq(1, 5, length = 3)\n\nlogx <- log(x)\n\nexp(logx)\n\n[1] 1 3 5\n\n\n\n\n\nExercises\n\nUse seq() to generate a sequence of numbers from 3 to 12.\nUse seq() to generate a sequence of numbers from 3 to 12 with length 25.\nWhy doesn’t this code work?\n\n\nseq(1, 5, by = 2, length = 10)\n\n\nUse <- to create an object with value 25 and assign it to a name of your choice.\nNow try to create another object with a different value and name.\nWhat is wrong with this code?\n\n\n2bob <- 10"
  },
  {
    "objectID": "02_assignments/lab-03.html#load-in-some-data",
    "href": "02_assignments/lab-03.html#load-in-some-data",
    "title": "Lab 3: Introduction to R",
    "section": "Load in some data",
    "text": "Load in some data\nWe will load in the data “house_prices.csv”, which is posted on Canvas. Save the data into a folder, preferably the same folder as where you are saving your R-script.\n\nSet the working directory\nYou need to tell R what file folder you will be working out of. You can do this in two ways: 1. Go to the “Files” tab in the Viewer pane. Navigate to your folder, then hit the “More” icon, then select “Set As Working Directory”. Since you will want anybody who runs your code to set their working directory, copy the code from the console into your\n2. Type the following code, but instead of your_file_path type the actual folder. Make sure your slashes are forward slashes ( / ) and not back slashes ( \\ ).\n\nsetwd(\"your_file_path\")\n\nFor example, in my script I will type the following, since that’s the folder I’m keeping my data in.\n\nsetwd(\"G:/My Drive/Classes/Regression Analysis\")\n\n\n\nLoad in the data\nNow that R knows where we have the data stored, we can load the data in.\n\nhouse <- read.csv(\"data/house_prices.csv\")\n\nNote that I have my data in a subfolder of my working directory called “data”. Your code may look more like this:\n\nhouse <- read.csv(\"house_prices.csv\")\n\nThis tells R to read in the csv and call the data frame “house”. You should see the data pop up in your environment. You can click on the word “house” and it will open up the data in the data viewer for you to explore. Or, if you just want a quick look, you can hit the little blue circle with the arrow next to the word “house”.\n\n\nWe can also ask R to print the first few lines of a dataframe to show us what it looks like. In this case, each row represents a house.\n\nhead(house, n = 5) # The 5 tells R to print 5 rows\n\n   Price Living.Area Bathrooms Age Fireplace Bedrooms\n1 142212        1982         1 133         0        3\n2 134865        1676         2  14         1        3\n3 118007        1694         2  15         1        3\n4 138297        1800         1  49         1        2\n5 129470        2088         1  29         1        3\n\n\nThe Price is how much the house sold for. The Living.Area is the number of square feet the house had. We also have the number of Bathrooms and Bedrooms. Finally, we have the Age of the house and whether it has a Fireplace.\n\n\nCreate some descriptive statistics\nR can work as a calculator. Let’s try a few simple exercises. What is the mean and standard deviation of house price? Note that we have to tell R what data frame we’re getting the variable from (in this case “house”), then put a $ to let R know we’re retrieving an element of the data frame (in this case “Price”).\n\nmean(house$Price)\n\n[1] 167901.9\n\nsd(house$Price)\n\n[1] 77158.35"
  },
  {
    "objectID": "02_assignments/lab-03.html#make-your-first-plot",
    "href": "02_assignments/lab-03.html#make-your-first-plot",
    "title": "Lab 3: Introduction to R",
    "section": "Make Your First Plot!",
    "text": "Make Your First Plot!\nTo ease you into working with R, let’s visualize some data to answer a simple question: Is the price of a house related to its square footage? Don’t worry about understanding every bit of code! It’s just to give you a feel for the sort of graphics you can make with R. We’ll spend a future lab learning how to make even better graphics.\n\nThe plot() function\nThe base R graphics package provides a generic function for plotting, which - as you might have guessed - is called plot(). (“Base R” means it’s automatically loaded and you don’t have to install it.) To see how it works, try running this code:\n\nplot(house$Living.Area, house$Price)\n\n\n\n\n\n\nCustomizing your plot\nWith the plot() function, you can do a lot of customization to the resulting graphic. For instance, you can modify all of the following:\n\npch will change the point type,\nmain will change the main plot title,\nxlab and ylab will change the x and y axis labels,\ncex will change the size of shapes within the plot region,\npch will change the type of point used (you can use triangles, squares, or diamonds, among others),\ncol changes the color of the point (or its border), and\nbg changes the color of the point fill (depending on the type of point it is)\n\nFor instance, try running this code:\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\n\n\n\n\nExercises\n\nComplete the following line of code to preview only the first three rows of the house table.\n\n\nhead(house, n = )\n\n\nModify the code below to change the size (cex) of the points from 2 to 1.5.\n\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\nWhat does this plot tell us about the relationship between house size and price? Is it positive or negative? Or is there no relationship at all? If there is a relationship, what might explain it?\nComplete the code below to add “Scatter Plot of House Size and Price” as the main title.\n\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 1,\n  main = \n)\n\n\nComplete the code below to add “House size (sq. ft.)” as the x-axis label and “Price ($)” as the y-axis label.\n\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2,\n  main = \"Scatter Plot of House Size and Price\",\n  xlab = ,\n  ylab = \n)"
  },
  {
    "objectID": "02_assignments/lab-03.html#assignment",
    "href": "02_assignments/lab-03.html#assignment",
    "title": "Lab 3: Introduction to R",
    "section": "Assignment",
    "text": "Assignment\nNow it’s time to work on your own. Download the “County_Election.csv” data set from Canvas and put it in your working directory. Read in the data using read.csv(). Then, write a paragraph giving some information on the data that you find interesting. Include at least 3 statistics and one graph. Be sure to interpret what you think the significance of the statistic is.\nHere is a table describing the variables in the data set:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nstate\nState FIPS Code\n\n\ncounty\nCounty FIPS Code\n\n\ncounty_name\nCounty Name\n\n\nstate_name\nState Name\n\n\nfips\nCombined FIPS Code\n\n\npct_republican_2016\nPercent of voters in county that voted Republican in 2016 presidential election\n\n\nfrac_coll_plus2010\nPercent of adults 25 years or older who have a 4-year college degree or more in 2010\n\n\nforeign_share2010\nNumber of foreign born residents in the 2010 Census divided by the sum of native and foreign born residents.\n\n\nmed_hhinc2016\nMedian household income in 2016\n\n\npoor_share2010\nShare of families with incomes under the poverty line in 2010\n\n\nshare_black2010\nShare of people who are Black in 2010\n\n\nshare_hisp2010\nShare of people who are Hispanic in 2010\n\n\nshare_asian2010\nShare of people who are Asian in 2010\n\n\nrent_twobed2015\nThe median gross rent for renter-occupied housing units with two\n\n\npopdensity2010\nNumber of residents per square mile in 2010\n\n\nann_avg_job_growth_2004_2013\nAverage annualized job growth rate over the time period 2004 to 2013\n\n\n\nYou may want to do different statistics from what we did above. Here are some functions you can use. To get information about them, type a question mark followed by the function you are looking up into the console. Alternatively, look the function up online.\n\nStatistics\n\n\n\nStatistic\nFunction\n\n\n\n\nMinimum:\nmin()\n\n\nMaximum:\nmax()\n\n\nAverage:\nmean()\n\n\nStandard Deviation:\nsd()\n\n\nMedian:\nmedian()\n\n\nPercentiles:\nquantile()\n\n\nCorrelation Coefficient:\ncor()\n\n\nFrequency tables:\ntable()\n\n\nRelative Frequency tables:\nprop.table()\n\n\n\nNote that in frequency tables you can use more than one variable!\n\n\nGraphics\n\n\n\nPlot\nFunction\n\n\n\n\nBar chart:\nbarplot()\n\n\nHistogram:\nhist()\n\n\nBox plots:\nboxplot()\n\n\nScatter plot:\nplot()"
  },
  {
    "objectID": "02_assignments/lab-04.html#outline",
    "href": "02_assignments/lab-04.html#outline",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Outline",
    "text": "Outline\n\nObjectives\nIn this lab, you will learn how to:\n\nload R packages with library()\nsubset data\nmerge data\nvisualize data with the grammar of graphics and ggplot()\n\naesthetic mappings\ngeometric objects\nfacets\nscales\nthemes\n\n\n\n\nR Packages\nggplot2\nviridis\n\n\nData\n\nhouse prices\ncounty elections\n\n\n\nGrade\nFor this assignment, you will complete a quiz as you work through the lab."
  },
  {
    "objectID": "02_assignments/lab-04.html#the-library",
    "href": "02_assignments/lab-04.html#the-library",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "The Library",
    "text": "The Library\nR is an extensible programming language, meaning you can write R code to extend the functionality of base R. To share that code, R users will often bundle it into a package, a collection of functions, data, and documentation. You can think of packages as apps, but apps specifically designed for R. To make the functionality a package offers available in R, you have to load them in with the library() function (the technical term is attach).\nYou should always, always, always load all the packages you use at the beginning of a document. That way, people who read your code know exactly what packages you are using all at once and right away. To make this really, really explicit, I prefer to put it with other front matter that I call the “R Preamble.” In an R Script, it looks like this:\n\n### Lab 4: More Advanced R ###\n# This code manipulates data and creates graphs using ggplot\n\nsetwd(\"G:/My Drive/Teaching/regression/02_assignments\")\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nOf course, these aren’t just automatically on your computer, so you have to install the packages first. Then you can open them in R. To do that, you use the function install.packages(). For the packages used today, you can use this call just once like so:\n\ninstall.packages(\n  c(\"ggplot2\", \"viridis\")\n)\n\nNote that you only need to run this once, so don’t put this as a line in your R script document, which you might render multiple times. Just run it in the console.\n\nExercises\nOpen a new R script and add the R Preamble with an R code chunk with the library calls that load the R packages required for this lab. Now actually run each library() call. You can do that by either highlighting them and hitting Ctrl + Enter (Cmd + Enter) or by clicking the run button. If you don’t highlight, R will run the code from the line your cursor is on."
  },
  {
    "objectID": "02_assignments/lab-04.html#step-1-load-in-the-data",
    "href": "02_assignments/lab-04.html#step-1-load-in-the-data",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Step 1: Load in the data",
    "text": "Step 1: Load in the data\nYou should have already set up your working directory at the top of the sheet. In that folder, you should have the two datasets we will be using in this lab: “NBA_Data.csv” and “Five38_Data.csv”, which you can find on Canvas. Load them in using the read.csv() function.\n\nNBA <- read.csv(\"data/NBA_Data.csv\")\nFive38 <- read.csv(\"data/Five38_Data.csv\")\n\nIf you look in your environment, you should have two data frames. Explore the data a little bit using the functions head(NBA) and summary(NBA). You can use the same functions for the Five38 dataset.\nThe NBA dataset has statistics for all 30 NBA teams for the 2022-2023 season (updated to 2/13/2023), including wins, losses, total points, free throws, assists, turnovers, etc. It also includes whether the team was in the playoffs last year.\nThe Five38 dataset has the conference each team is in, plus the Five38 Elo predictions for whether the team will make playoffs, make finals, and win finals. It also includes the number of times each team has been in the playoffs since 1946.\n\nMake a new column\nYou may notice that the NBA dataset has wins and losses, but it doesn’t have the win percent. We will make that ourselves! To create a new column, we just have to tell R what to do. Remember that we have to tell R which data frame we are using, then put a dollar sign to say we are accessing a specific variable.\n\nNBA$win_pct = NBA$Wins/NBA$Games_Played\n\nEasy! We can explore this variable a little bit by typing:\n\nsummary(NBA$win_pct)\n\nNice! Now we know that the median team lost about 50 percent of its games. Even the best team only won 72 percent.\n\n\n\n\n\nExercises\n\nMake a new column that shows the average points per game, which is points (NBA$Points) divided by the total games (NBA$Games_Played).\nFind the mean and median of points scored per game.\nUse the cor() function to find the correlation between points per game and the win percent."
  },
  {
    "objectID": "02_assignments/lab-04.html#step-2-merge-data",
    "href": "02_assignments/lab-04.html#step-2-merge-data",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Step 2: Merge data",
    "text": "Step 2: Merge data\nTo merge data, we need a unique identifier that matches between the two datasets, just like in any program. Take a look at the team names. What’s the problem?\nThe team names in the Five38 dataset are split to place and name. We can combine those columns using the following code:\n\nFive38$Team <- paste(Five38$Place, Five38$Name, sep = \" \")\n\nThe paste function tells R that we are combining two strings. The sep = \" \" option tells R that we want a space between the two strings, but we could put a comma or no space if we wanted.\nNow we have a unique ID, and we can use it to merge our datasets. We will create a new dataset called BBall with our merged data.\n\nBBall <- merge(NBA, Five38, by = \"Team\")\n\nThis new dataset has the same 30 teams, but 31 variables rather than 7 or 25. (It’s not 7 + 25 = 32 because R doesn’t repeat the identifier.)"
  },
  {
    "objectID": "02_assignments/lab-04.html#step-3-subset-data",
    "href": "02_assignments/lab-04.html#step-3-subset-data",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Step 3: Subset Data",
    "text": "Step 3: Subset Data\nA common thing to do with data is subset the data into a smaller group of data. For instance, we might want to remove some columns if there are columns we don’t need. Or, we might want to do some calculations on only teams in the Western Conference. Or, maybe we want to do calculations on the teams that have over 50% win percent.\nIf we are subsetting the whole dataframe, we put square brackets next to the name of the dataframe and give it the [rows, columns]. For example, if we want the second row of the first column, we would type BBall[2,1]. If we want the entire 4th column, we would put BBall[,4]. And if we want the entire 8th row, we would write BBall[8,].\nWe can also subset a range of rows or columns. To get the first 5 columns, we would type BBall[,1:5].\nTo subset by the conference, we would use two equals signs to tell R that we are checking for equivalency. (One equals sign means we are assigning the value.) Let’s make a new dataframe that only includes the Western conference.\n\nwest <- BBall[BBall$Conference == \"West\",]\n\nNow, we could do whatever we wanted on the new “west” dataframe.\nWe don’t have to make a new dataframe to use functions. For example, let’s say we want the average number of rebounds for teams that have over a 50% win rate vs under or equal to a 50% win rate. We are going to call the mean function on rebounds. Since we are targeting just one column, we only have to put the row argument into the square brackets.\n\nmean(BBall$Rebounds[BBall$win_pct > 0.5])\nmean(BBall$Rebounds[BBall$win_pct <= 0.5])\n\nGreat! Now, let’s subset to find the mean number of turnovers for teams that are in the Eastern Conference and have at least 113 points per game. We will use the ampersand (&) to tell R that we want both conditions to be true to include it. We can compare that to the number of turnovers for teams in the Eastern Conference that have less than 113 points per game.\n\nmean(BBall$Turnovers[BBall$Conference == \"East\" & BBall$points_per_game >= 113])\nmean(BBall$Turnovers[BBall$Conference == \"East\" & BBall$points_per_game < 113])\n\nWe can also get columns by name. Let’s make a a new dataset that just includes the team name and our calculated columns.\n\nnew = BBall[,c(\"Team\", \"win_pct\", \"points_per_game\")]\n\nOne thing we might want to know is what the name of the maximum value is. We can use the function which.max(), which gives the position/row number of the biggest value. Then we can return that row number. For example, if we want to know which team has the biggest win percent we would use this code:\n\nBBall$Team[which.max(BBall$win_pct)]\n\n\nExercise\n\nWhat is the value in the 3rd row of the 9th column of the BBall dataset?\nWhat is the mean win percentage in the Western Conference?\nWhat is the average free throw percent for teams in the Western conference that have at least 1120 personal fouls?\nWhich team has been to the playoffs the most times?"
  },
  {
    "objectID": "02_assignments/lab-04.html#step-4-create-visualizations",
    "href": "02_assignments/lab-04.html#step-4-create-visualizations",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Step 4: Create Visualizations",
    "text": "Step 4: Create Visualizations\n\nThe Grammar of Graphics\nIt’s easy to imagine how you would go about with pen and paper drawing a bar chart of the number of games played in each conference. But, what if you had to dictate the steps to make that graph to another person, one you can’t see or physically interact with? All you can do is use words to communicate the graphic you want. How would you do it? The challenge here is that you and your illustrator must share a coherent vocabulary for describing graphics. That way you can unambiguously communicate your intent. That’s essentially what the grammar of graphics is, a language with a set of rules (a grammar) for specifying each component of a graphic.\nNow, if you squint just right, you can see that R has a sort of grammar built-in with the base graphics package. To visualize data, it provides the default plot() function, which you learned about in the last lab. This is a workhorse function in R that will give you a decent visualization of your data fast, with minimal effort. It does have its limitations though. For starters, the default settings are, shall we say, less than appealing. I mean, they’re fine if late-nineties styles are your thing, but less than satisfying if a more modern look is what you’re after. Second, taking fine-grained control over graphics generated with plot() can be quite frustrating, especially when you want to have a faceted figure (a figure with multiple plot panels).\nThat’s where the ggplot2 package comes in. It provides an elegant implementation of the grammar of graphics, one with more modern aesthetics and with a more standardized framework for fine-tuning figures, so that’s what we’ll be using here. From time to time, I’ll try to give you examples of how to do things with the plot() function, too, so you can speak sensibly to the die-hard holdouts, but we’re going to focus on learning ggplot.\n\n\n\n\n\n\nResources for ggplot2\n\n\n\nThere are several excellent sources of additional information on statistical graphics in R and statistical graphics in general that I would recommend.\n\nThe website for the ggplot2 package. This has loads of articles and references that will answer just about any question you might have.\nThe R graph gallery website. This has straightforward examples of how to make all sorts of different plot visualizations, both with base R and ggplot.\nClaus Wilke’s free, online book Fundamentals of Data Visualization, which provides high-level rules or guidelines for generating statistical graphics in a way that clearly communicates its meaning or intent and is visually appealing.\nThe free, online book ggplot2: Elegant Graphics for Data Analysis (3ed) by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen. This is a more a deep dive into the grammar of graphics than a cookbook, but it also has lots of examples of making figures with ggplot2.\n\n\n\nSo, to continue our analogy above, we’re going to treat R like our illustrator, and ggplot2 is the language we are going to speak to R to visualize our data. So, how do we do that? Well, let’s start with the basics. Suppose we want to know if there’s some kind of relationship between the field goal percent and the probability of making playoffs. Here’s how we would visualize that.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs)\n  )\n\nHere, we have created a scatterplot, a representation of the raw data as points on a Cartesian grid. There are several things to note about the code used to generate this plot.\n\nIt begins with a call to the ggplot() function. This takes a data argument. In this case, we say that we want to make a plot to visualize the basketball data.\nThe next function call is geom_point(). This is a way of specifying the geometry we want to plot. Here we chose points, but we could have used another choice (lines, for example, or polygons).\nThe geom_point() call takes a mapping argument. You use this to specify how variables in your data are mapped to properties of the graphic. Here, we chose to map the Field_Goal_Pct variable to the x-coordinates and the Making_Playoffs variable to the y-coordinates. Importantly, we use the aes() function to supply an aesthetic to the mapping parameter. This is always the case.\nThe labs() function allows us to specific axis lables and titles. In this instance, I’m only using it to create alternative text for accessibility.\nThe final thing to point out here is that we combined or connected these arguments using the plus-sign, +. You should read this literally as addition, as in “make this ggplot of the basketball data and add a point geometry to it.” Be aware that the use of the plus-sign in this way is unique to the ggplot2 package and won’t work with other graphical tools in R.\n\nWe can summarize these ideas with a simple template. All that is required to make a graph in R is to replace the elements in the bracketed sections with a dataset, a geometry function, and an aesthetic mapping.\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\nOne of the great things about ggplot, something that makes it stand out compared to alternative graphics engines in R, is that you can assign plots to a variable and call it in different places, or modify it as needed.\n\nplayoffs_plot <- ggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs)\n  )\n\nplayoffs_plot\n\n\n\nExercises\n\nRecreate the scatterplot above, but switch the axes. Put win percent on the x axis and times in playoffs on the y axis.\nNow create a scatterplot of win percent on the x axis and probability of making playoffs (Making_Playoffs) on the y axis.\n\n\n\nAesthetics\nIn the plot above, we only specified the position of the points (the x- and y-coordinates) in the aesthetic mapping, but there are many aesthetics (see the figure below), and we can map the same or other variables to those.\n\n\nConsider, for example, that we have two groups of teams: those that went to the playoffs last year and those that had no post season. Do we think that the relationship between the field goal percent and probability of making playoffs are the same for both? Let’s add conference to our aesthetic mapping (specifically to the color parameter) and see what happens.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\nNotice that ggplot2 automatically assigns a unique color to each category and adds a legend to the right that explains each color. In this way, the color doesn’t just change the look of the figure. It conveys information about the data. Rather than mapping a variable in the data to a specific aesthetic, though, we can also define an aesthetic manually for the geometry as a whole. In this case, the aesthetics do not convey information about the data. They merely change the look of the figure. The key to doing this is to move the specification outside the aes(), but still inside the geom_point() function.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs),\n    shape = 21,\n    size = 4,\n    color = \"darkred\",\n    fill = \"darkgoldenrod1\"\n  )\n\nNotice that we specified the shape with a number. R has 25 built-in shapes that you can specify with a number, as shown in the figure below. Some important differences in these shapes concern the border and fill colors. The hollow shapes (0-14) have a border that you specify with color, the solid shapes (15-20) have a border and fill, both specified with color, and the filled shapes (21-24) have separate border and fill colors, specified with color and fill respectively.\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you can use hexadecimal codes like #004F2D instead of “forestgreen” to specify a color. This also allows you to specify a much wider range of colors. See this color picker website for one way of exploring colors.\n\n\n\n\nExercises\n\nChange the code below to map the Last_Year variable to the x-axis (in addition to the color).\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\n\nWhat does this do to the position of the points?\nChange the code below to map the Last_Year variable to the shape aesthetic (in addition to the color).\n\n\n# hint: use shape = ...\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\n\nChange the code below to map the Last_Year variable to the size aesthetic (replacing color).\n\n\n# hint: use size = ...\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\n\nFor the following code, change the color, size, and shape aesthetics for the entire geometry (do not map them to the data).\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs),\n    color = , # <------- insert value here\n    size = ,  # <------- \n    shape =   # <------- \n  )\n\n\n\nGeometries\nHave a look at these two plots.\n\n\n\nBoth represent the same data and the same x and y variables, but they do so in very different ways. That difference concerns their different geometries. As the name suggests, these are geometrical objects used to represent the data. To change the geometry, simply change the geom_*() function. For example, to create the plots above, use the geom_point() and geom_smooth() functions.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  )\n\nggplot(data = BBall) +\n  geom_smooth(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  )\n\nWhile every geometry function takes a mapping argument, not every aesthetic works (or is needed) for every geometry. For example, there’s no shape aesthetic for lines, but there is a linetype. Conversely, points have a shape, but not a linetype.\n\nggplot(data = BBall) + \n  geom_smooth(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct, linetype = Last_Year),\n  )\n\nOne really important thing to note here is that you can add multiple geometries to the same plot to represent the same data. Simply add them together with +.\n\nggplot(data = BBall) +\n  geom_smooth(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct, linetype = Last_Year)\n  ) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct, color = Last_Year)\n  ) \n\nThat’s a hideous figure, though it should get the point across. While layering in this way is a really powerful tool for visualizing data, it does have one important drawback. Namely, it violates the DRY principle (Don’t Repeat Yourself), as it specifies the x and y variables twice. This makes it harder to make changes, forcing you to edit the same aesthetic parameters in multiple locations. To avoid this, ggplot2 allows you to specify a common set of aesthetic mappings in the ggplot() function itself. These will then apply globally to all the geometries in the figure.\n\nggplot(\n  data = BBall,\n  mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n) +\n  geom_smooth(mapping = aes(linetype = Last_Year)) +\n  geom_point(mapping = aes(color = Last_Year))\n\nNotice that you can still specify specific aesthetic mappings in each geometry function. These will apply only locally to that specific geometry rather than globally to all geometries in the plot. In the same way, you can specify different data for each geometry.\n\nggplot(\n  data = BBall,\n  mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n) +\n  geom_smooth(data = BBall[BBall$Last_Year == \"Playoffs\",]) +\n  geom_point(mapping = aes(color = Last_Year))\n\nSome of the more important geometries you are likely to use include:\n\ngeom_point()\ngeom_line()\ngeom_segment()\ngeom_polygon()\ngeom_boxplot()\ngeom_histogram()\ngeom_density()\n\nWe’ll actually cover those last three in the section on plotting distributions. For a complete list of available geometries, see the layers section of the ggplot2 website reference page.\n\n\nScales\nScales provide the basic structure that determines how data values get mapped to visual properties in a graph. The most obvious example is the axes because these determine where things will be located in the graph, but color scales are also important if you want your figure to provide additional information about your data. Here, we will briefly cover two aspects of scales that you will often want to change: axis labels and color palettes, in particular palettes that are colorblind safe.\n\nLabels\nBy default, ggplot2 uses the names of the variables in the data to label the axes. This, however, can lead to poor graphics as naming conventions in R are not the same as those you might want to use to visualize your data. Fortunately, ggplot2 provides tools for renaming the axis and plot titles. The one you are likely to use most often is probably the labs() function. Here is a standard usage:\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  )\n\n\n\nColor Pallettes\nWhen you map a variable to an aesthetic property, ggplot2 will supply a default color palette. This is fine if you are just wanting to explore the data yourself, but when it comes to publication-ready graphics, you should be a little more thoughtful. The main reason for this is that you want to make sure your graphics are accessible. For instance, the default ggplot2 color palette is not actually colorblind safe. To address this shortcoming, you can specify colorblind safe color palettes using the scale_color_viridis() function from the viridis package. It works like this:\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  scale_color_viridis(\n    option = \"viridis\", \n    discrete = TRUE\n  )\n\nI know it doesn’t look good like it is, but when you are working with more categories or different types of graphs, like maps or stacked bar charts, these kinds of pallettes are very useful.\nHere are the color palettes available in the viridis package: {fig-alt = “The colors available in the viridis package.”}\n\n\n\nExercises\n\nUsing the BBall dataset, plot Steals (y variable) by Personal_Fouls (x variable) and change the axis labels to reflect this.\nUsing the code below, try out some different colorblind safe palettes from the viridis package.\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Conference)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Probability of Making Playoffs\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  scale_color_viridis(\n    option = \"viridis\", #<------- change value here \n    discrete = TRUE\n  )\n\n\nTry adding a numeric variable, like win_pct, as the color input. (Make sure to change discrete = TRUE to discrete = FALSE.)\n\n\n\nThemes\nTo control the display of non-data elements in a figure, you can specify a theme. This is done with the theme() function. Using this can get pretty complicated, pretty quick, as there are many many elements of a figure that can be modified, so rather than elaborate on it in detail, I want to draw your attention to pre-defined themes that you can use to modify your plots in a consistent way.\nHere is an example of the black and white theme, which removes filled background grid squares, leaving only the grid lines.\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  theme_bw()\n\n\n\nExercises\n\nComplete the code below, trying out each separate theme: theme_minimal() theme_classic() theme_void()\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  theme_   # <------- complete function call to change theme"
  },
  {
    "objectID": "02_assignments/intro_to_R_blind.html",
    "href": "02_assignments/intro_to_R_blind.html",
    "title": "Introduction to R for Blind Users",
    "section": "",
    "text": "This assignment is meant to be an introduction to R that includes a few points that will be important for blind users. In particular, this will show you how to run R code, how to: * Create a Markdown document * Implement settings that help with a screen reader * Load the BrailleR package and use some of the basic functions"
  },
  {
    "objectID": "02_assignments/lab-05.html",
    "href": "02_assignments/lab-05.html",
    "title": "Lab 5: dplyr and ggplot2",
    "section": "",
    "text": "In this lab, you will\n\nget more practice merging data in R\nlearn some important functions in the dplyr package\npractice making gorgeous plots in ggplot2\nbe encouraged to use help pages and vignettes to learn how to use functions\n\n\n\n\ndplyr\nggplot2\n\n\n\n\nGDP by country\n\nContains GDP per capita (PPP) for each country from 1990 to 2023\n\nPopulation and life expectancy by country\n\nContains population and life expectancy variables for each country from 1950 to 2023\n\n\n\n\n\nFor this assignment, follow along with the quiz on Canvas."
  },
  {
    "objectID": "02_assignments/lab-05.html#dplyr-and-the-tidyverse",
    "href": "02_assignments/lab-05.html#dplyr-and-the-tidyverse",
    "title": "Lab 5: dplyr and ggplot2",
    "section": "Dplyr and the Tidyverse",
    "text": "Dplyr and the Tidyverse\nThere are several packages that make data manipulation in R much easier than in Excel or many other languages. The tidyverse is a collection of R packages designed for data science. It includes tools for data manipulation, visualization, and analysis. Core packages include dplyr for data wrangling, ggplot2 for visualization, tidyr for data tidying, and readr for data import. The tidyverse is built around the concept of tidy data, where datasets are organized in a consistent way, making analysis more intuitive and efficient.\ndplyr is a package used for data manipulation, providing a range of tools to transform, summarize, and manipulate data frames. It allows for operations like filtering rows (filter()), selecting columns (select()), arranging data (arrange()), creating new variables with functions of existing variables (mutate()), and summarizing data (summarize()). These functions make it easy to work with large datasets using clear, readable syntax.\nWe want to read in the data, then merge it to create a dataset that allows us to compare GDP and life expectancy. First, load the packages and read in the data.\n\nrm(list = ls())\nlibrary(tidyverse)\nlibrary(ggplot2)\npop <- read.csv(\"data/population_by_country.csv\") # The csv file is in my \"data\" folder\ngdp <- read.csv(\"data/gdp_by_country.csv\") # You may not need the \"data/\" part\n\nYou will notice that the population dataset has a lot more rows than the GDP dataset. There are two reasons for that:\n1) Population includes many years which we don’t need\n2) GDP is formatted “wide” rather than “long”\nWe will discuss how to adjust those now.\n\nFilter\nOne of the things we can do with dplyr is remove rows we don’t based on criteria we set. For instance, let’s say we want to subset rows so that the data frame only includes data from 1990 to 2023, the years in the GDP data. We can either use the code we previously learned using base R:\n\npop <- pop[pop$Year > 1989, ]\n\nOr, we can use the easier functionality that is a part of dplyr.\n\npop <- filter(pop, Year > 1989)\n\nUsually, rather than using functions in that way, we use something called the piping operator. In R, the piping operator (|>), provided by the magrittr package (and commonly used in the tidyverse), allows you to chain together a sequence of operations in a more readable and intuitive way. The pipe enables you to pass the output of one function directly into the next function as an input, without the need for nesting functions or creating intermediate variables.\nThe basic syntax is:\n\ndata |> function1() |> function2() |> function3()\n\nWhich is equivalent to:\n\nfunction3(function2(funciont1(data)))\n\nThe pipe operator takes the result of the left-hand side and feeds it as the first argument into the function on the right-hand side. This makes the code flow from left to right, mimicking how we read sentences, which often improves readability, especially when performing multiple operations.\nIn our example, the code would be:\n\npop <- pop |> \n  filter(Year > 1989)\n\n\n\nPivot Longer\nIn the GDP dataset, each column is a different year (wide format), while in the life expectancy dataset, there are rows for each year for each country (long format). Usually, long format data is more helpful, so we will change gdp to be long using pivot_longer().\nThe reference pages for tidyverse functions are usually very good, with many helpful examples. They are often vignettes rather than help pages, which means they have more information and are easier to read. Check out the pivot longer vignette to learn about it.\nWe want to turn every column that starts with “X” into a row, with the year in the column name as a new column. So we want this table:\n\n\n\nCountry\nX2000\nX2001\nX2002\n\n\n\n\nUSA\na\nb\nc\n\n\nCanada\nx\ny\nz\n\n\nMexico\nt\nu\nv\n\n\n\nTo turn into this table:\n\n\n\nCountry\nYear\nValue\n\n\n\n\nUSA\n2000\na\n\n\nUSA\n2001\nb\n\n\nUSA\n2002\nc\n\n\nCanada\n2000\nx\n\n\nCanada\n2001\ny\n\n\nCanada\n2002\nz\n\n\nMexico\n2000\nt\n\n\nMexico\n2001\nu\n\n\nMexico\n2002\nv\n\n\n\nWe will use the piping operator to use the function.\n\ngdp <- gdp |> \n  pivot_longer(\n    cols = starts_with(\"X\"),\n    names_to = \"Year\",\n    names_prefix = \"X\",\n    values_to = \"GDP_per_cap\",\n    values_drop_na = TRUE\n  )\n\n\n\n\n\n\n\nMore help with the pivot longer\n\n\n\nCheck out the video on Canvas for more help with pivot longer. There are also videos on Youtube, like this one.\n\n\nIn order to merge, we need to have a key to merge on so we can merge by a unique country/year. We could make a new variable that has both of those, but R allows us to merge based on two keys. The keys do have to be the same data type, so let’s change the Year variable in the GDP data frame to numeric. Then, we can use merge().\nmutate() is a function that allows us to create new row-wise variables within the dplyr framework. (Note that if you use <- in mutate() or summarize(), the variable names can be weird, so you must use =.)\n\ngdp <- gdp |> \n  mutate(Year = as.numeric(Year))\n\nworld <- merge(gdp, pop, by.x = c(\"Year\", \"Country.Code\"), by.y = c(\"Year\", \"iso3_code\"))\n\nNow we can do the analysis we want!"
  },
  {
    "objectID": "02_assignments/lab-05.html#gapminder-analysis",
    "href": "02_assignments/lab-05.html#gapminder-analysis",
    "title": "Lab 5: dplyr and ggplot2",
    "section": "Gapminder Analysis",
    "text": "Gapminder Analysis\nThe Gapminder project reported national income versus health for all of the world countries. The project had many great visualizations, but they stopped updating data in 2015. We are going to update the graphic with new data. Here is a scatterplot of GDP per capita (PPP) and health (life expectancy at birth):\n\n\nLet’s find out which recent years have data for many countries. We will use group_by() and summarize() to do so. The group_by() function in dplyr is used to group data by one or more variables in a data frame. This is useful when you want to perform operations (like summarizing or mutating) separately for each group, rather than on the entire dataset. Grouping doesn’t change the data itself, but it sets the stage for subsequent operations that will respect the groups.\nI’m going to create a new dataframe that groups data by year, and I’ll get the count and average life expectancy for each. Then, I’ll print the most recent years of data, and I’ll do a line plot of life expectancy over time.\n\nyear_data <- world |> \n  group_by(Year) |> \n  summarize(n_countries = n(),\n            avg_le = mean(as.numeric(life_exp_total)))\n\nyear_data[year_data$Year > 2019,]\n\nggplot(data = year_data) +\n  geom_line(aes(x = Year, y = avg_le)) +\n  xlab(\"Year\") +\n  ylab(\"Average Life Expectancy\") +\n  theme_classic() +\n  labs(alt = \"A line graph with year on the x axis and life expectancy on the y axis. Years range from 1990 to 2023, and the average life expecatancy increases from about 65 to about 73, with a dip around 2020.\") # It's a good idea to add alternative text for visually impaired users!\n\nIt looks like 2021 has almost all of the countries in the sample, so we will use that year.\n\nExercise\n\nChange the data type of life_exp_total and total_pop to numeric using as.numeric() within mutate() like we did with year in the GDP dataset.\nSubset the world data to just include the year 2021 using filter().\nTry grouping by “Continent”, and calculate the average life expectancy and GDP per capita for each continent using summarize().\nCalculate the total population by continent using group_by() and sum() within summarize().\nCreate the variable log_gdp by using log() for GDP_per_cap within mutate().\n\n\n\nCreating the scatterplot\nUse ggplot to create a scatterplot of logged GDP versus life expectancy for 2021. Make sure to do the following:\n\nUse arguments within the aes() mapping function to change the color of the points to reflect the continent, and the size of the point to reflect total population. You may need to use help pages or Google to figure it out.\nAdd x and y labels.\nChange the legend label to have a nicer title by using labs(size = \"Whatever you want your size label to be\").\n\n\n\nExercise\nFind out one more interesting thing about the dataset. Maybe you want to plot one of the other variables, like population density or fertility. Or, maybe you want to do some summary statistics like finding the correlation between infant mortality and life expectancy after age 15. What did you find? And why do you think it is interesting?"
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html",
    "href": "02_assignments/mini_lab-ttests.html",
    "title": "Mini Lab: T Tests in R",
    "section": "",
    "text": "In this short assignment, you will practice hypothesis testing in R. The graded portion of the assignment will only be to answer the questions in the Canvas quiz."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#load-data",
    "href": "02_assignments/mini_lab-ttests.html#load-data",
    "title": "Mini Lab: T Tests in R",
    "section": "Load Data",
    "text": "Load Data\nMost of the time we load data from csv files we have downloaded on our computer. Sometimes, R or R packages will have datasets included, so you do not need to load the data from a file, but instead can use the data() function. We will use a built-in dataset that includes arrest data for each US state in 1973.\nLoad the data with this code:\n\ndata(\"USArrests\")\n\nIf the data does not immediately show up in your environment, it might just need a little more time, or you might need to call the data first with a function. Try head(USArrests).\nThe data has 4 variables for 1973: the arrest rates for murder, assault, and rape, and the percent of each state’s population that is urban."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#hypothesis-testing",
    "href": "02_assignments/mini_lab-ttests.html#hypothesis-testing",
    "title": "Mini Lab: T Tests in R",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nExample\nSuppose I read an article that said that in 1973, the average percent urban in each state was 62 percent. I want to check that with this dataset with \\(\\alpha = 0.05\\). Here is how I would do it in R:\n1) Write down the null and alternative hypotheses.\n\\[\nH_0: \\mu = 62\n\\]\n\\[\nH_A: \\mu \\neq 62\n\\]\n2) Do the hypothesis test. I will use the following function in R.\n\nt.test(mydata$variable, alternative, mu, conf.level)\n\nWhere “mydata” is your data; “variable” is the variable you want to test; “alternative” can take on one of three values: “two.sided”, “greater”, “less”; mu is \\(\\mu\\); and conf.level is \\((1-\\alpha)\\). I’ll plug in my values:\n\nt.test(USArrests$UrbanPop, alternative = \"two.sided\", mu = 62, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  USArrests$UrbanPop\nt = 1.7293, df = 49, p-value = 0.09005\nalternative hypothesis: true mean is not equal to 62\n95 percent confidence interval:\n 61.42632 69.65368\nsample estimates:\nmean of x \n    65.54 \n\n\n3) Conclude. The p-value of 0.09 is greater than our significance level of 0.05. The p-value says that if the null hypothesis were true and the actual mean were 62, the probability we would see the data we do is 9%. That is likely enough that we don’t have enough evidence to reject the null hypothesis. The data do not provide enough evidence to say that the mean of 62 is not true.\n\n\nExercise\nSuppose I heard that in 2021, the arrest rate for assault was 135 per 100,000. I want to see if the average state-level rate 1973 was significantly different from that at the 99% confidence level.\na. Write down the null and alternative hypotheses.\nb. Perform the t test.\nc. Write down your conclusion."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#creating-a-dummy-variable",
    "href": "02_assignments/mini_lab-ttests.html#creating-a-dummy-variable",
    "title": "Mini Lab: T Tests in R",
    "section": "Creating a dummy variable",
    "text": "Creating a dummy variable\n\nExample\nSuppose I want to compare two groups of states: states in the west, and states not in the west. I need to create a dummy variable, which is a variable that equals one if a condition is true, and zero if it is not. The easiest way to do this in R is with ifelse(). The function ifelse() takes 3 arguments: the first is a test on a variable in the dataset, the second is the value the new variable will take if the test is true, and the third is the value the new variable will take if the test is false.\n\nUSArrests$west <- ifelse(rownames(USArrests) %in% c(\"California\", \"Washington\", \"Oregon\", \"Idaho\", \"Utah\", \"Nevada\", \"Arizona\", \"New Mexico\", \"Montana\", \"Wyoming\", \"Colorado\"), 1, 0)\n\nThe row names in this dataset are the name of the state. This code tests whether the row name is one of the states in the list, and if it is, then it assigns a 1 to the new variable west.\n\n\nExercise\nBreak the data into two groups: states with a high urban population, and states with a low urban population. UrbanPop is a variable that says the percent of the state which is urban. The following code tells us some summary statistics:\n\nsummary(USArrests$UrbanPop)\n\nThe median percent urban is 66. Use that as the cutoff for urban versus non-urban. Use ifelse() to create a variable that equals 1 if the UrbanPop variable is greater than 66, and 0 if it is not."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#two-sample-t-tests",
    "href": "02_assignments/mini_lab-ttests.html#two-sample-t-tests",
    "title": "Mini Lab: T Tests in R",
    "section": "Two sample t tests",
    "text": "Two sample t tests\n\nExample\nIn the examples we’ve done so far, we test our sample mean against the mean for a larger population or theoretical distribution. Sometimes, we want to test whether two groups are different based on two different samples. I will test whether states in the west have significantly lower murder rates.\nHypothesis testing for two samples is slightly different, and the calculation for t is slightly different.\n\nWrite down hypotheses. \\[ H_0: \\mu_{west} \\geq \\mu_{non-west} \\] \\[ H_1: \\mu_{west} < \\mu_{non-west} \\] Which is equivalent to: \\[ H_0: \\mu_{west} - \\mu_{non-west} \\leq 0\\] \\[ H_1: \\mu_{west} - \\mu_{non-west} > 0 \\]\nPerform the t test. If I were to be calculating this by hand, the formula would be: \\[ t= \\frac{(\\bar x_{west}-\\bar x_{non-west})-(\\mu_{west}-\\mu_{non-west})}{{\\sqrt{\\frac{s_{west}^2}{n_{west}} + \\frac{s_{non-west}^2}{n_{non-west}} }}}\\] If you are interested, you can calculate this on your own. The degrees of freedom would be \\(df = n_{west} + n_{non-west}-2\\).\n\nI could calculate the mean, standard deviation, and sample size for each group like so:\n\nmean(USArrests$Murder[USArrests$west == 1])\nsd(USArrests$Murder[USArrests$west == 1])\nlength(USArrests$Murder[USArrests$west == 1])\n\nHowever, we can use the t test formula in R as well. The formula for two-sample t tests is:\n\nt.test(data1, data2, alternative, conf.level)\n\nThe data you want to compare are different rows in the same dataframe. You will have to subset your data, either within the formula using brackets, or by making new data frames using brackets or dplyr.\nIn this case, I would use the code:\n\nt.test(USArrests$Murder[USArrests$west == 1],\n       USArrests$Murder[USArrests$west == 0],\n       \"less\", conf.level = 0.95)\n\nI use “less” because I’m doing a one-sided hypothesis test. If I were doing a one-sided hypothesis the other way, I would use “greater”.\n\nWrite down your conclusions. The p-value is 0.1374. This means that there is a 13.74% chance we’d observe the data we do if the null hypothesis were true. This is higher than our significance level of 5%, so we fail to reject the null hypothesis. There is not sufficient evidence to say that murder arrest rates in the west are less than murder arrest rates in non-western states.\n\n\n\nExercise\nTry doing a two sample t-test to see whether more urban states (states with more than the median percent urban, like the dummy variable) have higher murder rates than less-urban states."
  },
  {
    "objectID": "02_assignments/lab-06.html#outline",
    "href": "02_assignments/lab-06.html#outline",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Outline",
    "text": "Outline\n\nObjectives\nIn this lab, you will analyze two stocks, Apple and Disney, to see how their daily returns compare to the market as measured by the NASDAQ. You will learn:\n\nA practical application of simple linear regression\nHow to run a regression in R\nHow to use R and dplyr to analyze stock data\n\n\n\nR Packages\ndplyr\nggplot2\ntidyquant\ntidyr\n\n\nData\nStock data: Daily stock prices for Apple, Disney, and the NASDAQ.\n\n\nGrade\nYou will turn in a short reflection on Canvas at the end of the lab."
  },
  {
    "objectID": "02_assignments/lab-06.html#step-1-download-and-format-stock-data",
    "href": "02_assignments/lab-06.html#step-1-download-and-format-stock-data",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 1: Download and Format Stock Data",
    "text": "Step 1: Download and Format Stock Data\nWe will use the tidyquant package to download stock data. If you want to know more about this package, watch this video. Use the following code to download the stock prices from 2019 to 2024 for Apple (AAPL), Disney (DIS), and the NASDAQ.\nThe NASDAQ indicator typically refers to the NASDAQ Composite Index (symbol: ^IXIC), which tracks the performance of over 3,000 stocks listed on the NASDAQ stock exchange. This index is heavily weighted toward technology companies but also includes firms from sectors like healthcare, consumer services, and finance. Stock market indicators, such as the NASDAQ, Dow Jones Industrial Average, and S&P 500, serve as benchmarks for the overall performance of the stock market or specific sectors. These indicators provide insights into market trends, investor sentiment, and the health of the economy, helping investors make informed decisions.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidyquant)\n\n\nstocks <- tq_get(c(\"AAPL\", \"DIS\", \"^IXIC\"),\n                 get  = \"stock.prices\",\n                 from = \" 2019-01-01\",\n                 to = \"2024-01-01\")\n\nWe need to use pivot_wider() from the tidyr package to reshape the data so it has a column for the date, and the stock price on each date for each stock. This function is very similar to pivot_longer(), but it changes the data so there are more columns and fewer rows. Here is the vignette about pivot_wider.\n\nstocks <- stocks |> \n  select(symbol, date, open) |> \n  pivot_wider(names_from = symbol,\n              values_from = open) |> \n  rename(nasdaq =`^IXIC`) # rename the nasdaq to be less difficult"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-2-calculate-return",
    "href": "02_assignments/lab-06.html#step-2-calculate-return",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 2: Calculate Return",
    "text": "Step 2: Calculate Return\nWe want to get daily stock returns (changes) rather than levels (prices), so we need to calculate them using dplyr. The formula for returns is: \\[return_t = \\frac{price_t-price_{t-1}}{price_{t-1}}*100 = (\\frac{price_t}{price_{t-1}}-1)*100\\]\nTo calculate this, we will use the following code:\n\nstocks <- stocks |> \n  mutate(apple_return = (AAPL/lag(AAPL)-1)*100,\n         disney_return = (DIS/lag(DIS)-1)*100,\n         nasdaq_return = (nasdaq/lag(nasdaq)-1)*100)\n\nThis tells R to add new variables that are calculated from the value and lagged values from other variables. You can use head(stocks) to see the returns for the first 5 observations.\n\nExercise\n\nUse the summary() function to check out the stock returns for the two stocks and indicator. What is the average daily return?\nUse the following code to find out which day had the lowest return: stocks$date[which.min(stocks$nasdaq_return)]. Does that make sense?\nCalculate the total return over the 5 years with the following code: (tail(stocks$nasdaq, 1)/head(stocks$nasdaq, 1)-1)*100. Does this amount of growth surprise you? How does that compare to the 5 year growth of Disney and Apple?"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-3-plot-the-data",
    "href": "02_assignments/lab-06.html#step-3-plot-the-data",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 3: Plot the Data",
    "text": "Step 3: Plot the Data\nMake a scatterplot with the NASDAQ return on the x axis and Apple return on the y axis using ggplot() and geom_point(). Label the axes using xlab() and ylab() and use theme_classic() to make the plot look like the one below.\n\n\n\n\n\n\nExercise\n\nWhat is the relationship between the NASDAQ and Apple? Is it positive or negative? Is it strong? Is it a one-to-one increase, or is the slope different from 1?\nCreate the same plot for Disney.\nCompare the plots. Which one seems to have a stronger relationship with the market?"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-4-use-a-regression-to-formally-analyze",
    "href": "02_assignments/lab-06.html#step-4-use-a-regression-to-formally-analyze",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 4: Use a Regression to Formally Analyze",
    "text": "Step 4: Use a Regression to Formally Analyze\nRather than just exploring charts to determine how closely the stock follows the market, regression can provide formality and precision. In R, the function for regression is lm(), which stands for “linear model”. It generally takes the form lm(y ~ x, data = dataset_name). Run the following regression:\n\napple_model <- lm(apple_return ~ nasdaq_return, data = stocks)\nsummary(apple_model)\n\nThis is an important regression in finance. It is generally written as: \\[ stock return = \\alpha + \\beta * indicator return \\]\n\\(\\beta\\) is an indicator of how volatile the stock is compared to the market. If \\(\\beta\\) is above 1, that means the stock is more volatile or risky than the market, and if it is below 1, the stock is less volatile or risky than the market. Tech stocks tend to be high beta, because the stocks are based on potential success. Stocks like Proctor & Gamble, which makes many household goods and has been around for a long time, tend to be low beta because they are stable and low-growth.\n\\(\\alpha\\) is an indicator of how the stock performs compared to the market. If \\(\\alpha\\) is greater than 0, then, on average, the stock grows even when the market growth is 0. If \\(\\alpha\\) is less than 0, the stock does worse than the market. One thing you’ll hear investors say is that they are “chasing alpha”, or trying to find stocks that outperform the market.\n\nExercise\n\nWhat are the \\(\\alpha\\) and \\(\\beta\\) coefficients for Apple?\nRun the same regression for Disney instead of Apple. Compare the coefficients. Do either of them outperform the market? Which one is more volatile? Does that match what you saw in the graphs?"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-5-add-regression-line-to-plot",
    "href": "02_assignments/lab-06.html#step-5-add-regression-line-to-plot",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 5: Add Regression Line to Plot",
    "text": "Step 5: Add Regression Line to Plot\nTo add the regression line to the plot, use the same ggplot() code from before, but add another component. After one of the plus signs in your ggplot code, add this:\n\nstat_smooth(mapping = aes(x = nasdaq_return, y = apple_return), data = stocks,\n            method = \"lm\", geom = \"smooth\") +\n\nYou should get a regression line with confidence intervals in gray."
  },
  {
    "objectID": "02_assignments/lab-06.html#assignment",
    "href": "02_assignments/lab-06.html#assignment",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Assignment",
    "text": "Assignment\nWrite a few sentences describing how Disney and Apple stock compare to the overall market and to each other by interpreting their \\(\\alpha\\) and \\(\\beta\\) values. Turn this in on Canvas."
  },
  {
    "objectID": "02_assignments/lab-06.html#further-exploration",
    "href": "02_assignments/lab-06.html#further-exploration",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Further exploration",
    "text": "Further exploration\nHere are some more exercises if you want more practice. 1. Download the data for Tesla for the first 5 years it was public (it went public on June 29, 2010). Compare the \\(\\alpha\\) and \\(\\beta\\) to the recent data. 2. Explore a different stock of your choice, for example Petco (WOOF), Coca-Cola (KO), or Harley-Davidson (HOG). 3. Try creating a line graph of a stock over time using ggplot() and geom_line().\n\nMore Finance with R\nIf you want a longer introduction of how to do financial analysis with R, you can check out this site or this e-book."
  },
  {
    "objectID": "02_assignments/lab-07.html",
    "href": "02_assignments/lab-07.html",
    "title": "Lab 7: Multiple Regression in Life Expectancy and Homelessness",
    "section": "",
    "text": "The goals of this lab are to:\nThe first step in the lab is to download the two datasets we will use: Life Expectancy-Insurance Example Data.xlsx and homelessness_data.xlsx."
  },
  {
    "objectID": "02_assignments/lab-07.html#read-in-and-merge-data",
    "href": "02_assignments/lab-07.html#read-in-and-merge-data",
    "title": "Lab 7: Multiple Regression in Life Expectancy and Homelessness",
    "section": "Read In and Merge Data",
    "text": "Read In and Merge Data\nThe main sheet we will use as our base sheet is the life expectancy sheet. Let’s read it in and take a look.\n\n\n\n\nlife_exp <- readxl::read_xlsx(\"Life Expectancy-Insurance Example Data.xlsx\", sheet = 1)\nhead(life_exp)\n\nNote that the key that we will be merging on is the fips column.\nWe want to subset the data to only be 2010 and the columns for male and female life expectancy by county. We can also rename the variables to be easier to type. All of this is easy using dplyr and janitor. janitor has a function called clean_names() which makes all the variables lowercase and removes spaces and special characters. Remember that we use filter() to subset rows and select() to subset columns.\n\nlife_exp <- life_exp |> \n  filter(Year == 2010) |> \n  clean_names() |> \n  select(state, county, fips, female_le, male_le) \n\nNext, we’ll read in the income sheet. We need to create a fips code that matches the code from the other sheet, and we should subset the data so we aren’t merging too much extra information. From reading the information from the data source of all the data sets, I know that the fips code has the state code at the beginning, followed by 3 digits for the county.\n\nincome <- readxl::read_xlsx(\"data/Life Expectancy-Insurance Example Data.xlsx\", sheet = 2)\nincome$fips <- income$STATEA*1000 + income$COUNTYA\nincome <- income |> \n  clean_names() |> \n  select(fips, median_hh_income) \n\nWe then use the merge function to merge the data.\n\nlife_exp <- merge(life_exp, income, by = \"fips\")\n\nNext, read in the insurance sheet and take a look.\n\ninsurance <- readxl::read_xlsx(\"data/Life Expectancy-Insurance Example Data.xlsx\", sheet = 3)\n\nWe need to calculate the uninsured rate by county for people age 18 to 65. With this code, using group_by() and summarize(), we can get uninsured rate by fips code.\n\ninsurance$FIPS = 1000*insurance$STATEA + insurance$COUNTYA\nuninsured <- insurance |> \n  clean_names() |> \n  group_by(fips) |> \n  summarize(uninsured_rate = (male_under65_uninsured + female_under65_uninsured)/(male_under65_total + female_under65_total)) \n\nNow, let’s merge our uninsured data frame with the life expectancy data frame.\n\nlife_exp <- merge(life_exp, uninsured, by = \"fips\")\n\nLastly, we’ll load and merge the region data. For this merge, we use the state name as the key rather than the fips code. We’ll also remove all of the data frames that we don’t need anymore.\n\nregion <- readxl::read_xlsx(\"data/Life Expectancy-Insurance Example Data.xlsx\", sheet = 4)\nlife_exp <- merge(life_exp, region, by.x = \"state\", by.y = \"STATE\") # With different column names, need by.x and by.y\nrm(income, insurance, region, uninsured)\n\nNow the data is all merged, we can start running regressions!"
  },
  {
    "objectID": "02_assignments/lab-07.html#analysis",
    "href": "02_assignments/lab-07.html#analysis",
    "title": "Lab 7: Multiple Regression in Life Expectancy and Homelessness",
    "section": "Analysis",
    "text": "Analysis\nOur main question in this analysis will be: do places with more uninsured people have lower life expectancy? We might think that this relationship exists because people without health insurance don’t have access to healthcare and might not get the treatments they need.\nLet’s start with a scatterplot and a simple correlation:\n\ncor(life_exp$uninsured_rate, life_exp$female_le)\n\n[1] -0.4237297\n\nggplot(data = life_exp) +\n  geom_point(aes(x = uninsured_rate, y = female_le)) +\n  xlab(\"Uninsured Rate\") +\n  ylab(\"Female Life Expectancy\") +\n  theme_classic()\n\n\n\n\nThe correlation between life expectancy and the uninsured rate is strongly negative, and this is evident in the scatter plot. Let’s try a simple linear regression. Remember that the formula for a regression is lm(), which stands for “linear model”.\n\nsummary(lm(female_le ~ uninsured_rate, data = life_exp))\n\n\nCall:\nlm(formula = female_le ~ uninsured_rate, data = life_exp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4294 -1.1210  0.0163  1.1688  6.9117 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     82.0925     0.1235  664.89   <2e-16 ***\nuninsured_rate -13.7320     0.6853  -20.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.77 on 1835 degrees of freedom\nMultiple R-squared:  0.1795,    Adjusted R-squared:  0.1791 \nF-statistic: 401.6 on 1 and 1835 DF,  p-value: < 2.2e-16\n\n\nFrom this, we can see that the regression equation is: \\[ Female Life Expectancy = 82.5 - 13.2*Uninsured Rate \\] The uninsured rate has a large negative effect. The effect is statistically significant, as evidenced by the very low p-value. We are also concerned with practical significance - is the effect big enough to care about? Let’s try to put the coefficient into perspective by using some real values.\n\nsummary(life_exp$uninsured_rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.03419 0.12709 0.16389 0.16980 0.20645 0.49321 \n\n\nThe 25th percentile of the uninsured rate is 15%, and the 75th percentile is 25%. If we went from the 25th to 75th percentiles in uninsured rate, the expected life span in the county is 1.3 years lower. \\((25\\% - 15\\%)*13.2 = 1.32\\) . That’s pretty big!\n\nControlling for Variables\nThere’s a big omitted variable here. Income probably affects life expectancy and the uninsured rate. That means that our coefficient may be biased upward, seeming like the uninsured rate has a bigger effect than it really does. Let’s add income to the model.\n\nsummary(lm(female_le ~ uninsured_rate + median_hh_income, data = life_exp))\n\n\nCall:\nlm(formula = female_le ~ uninsured_rate + median_hh_income, data = life_exp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2601 -0.9845  0.0061  0.9900  5.5107 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       7.619e+01  2.278e-01 334.547  < 2e-16 ***\nuninsured_rate   -4.697e+00  6.476e-01  -7.253 5.99e-13 ***\nmedian_hh_income  9.299e-05  3.208e-06  28.990  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.466 on 1834 degrees of freedom\nMultiple R-squared:  0.4374,    Adjusted R-squared:  0.4368 \nF-statistic: 712.8 on 2 and 1834 DF,  p-value: < 2.2e-16\n\n\nWhen we include income, the effect of the uninsured rate is still statistically significant and negative, but now the magnitude is smaller. The formula is now: \\[ Female Life Expectancy = 76.8 - 5.5*Uninsured Rate + 0.00008*Median Household Income\\] If we go from the 25th to 75th percentiles of uninsured rate now, life expectancy is only 0.6 years lower. \\[ (25\\% - 15\\%)*5.5 = 0.6 \\]\nThe last thing we will do with this data is add region as an explanatory variable. Region may also be an omitted variable. Some regions, like the South, have lots of poverty and little government funding, meaning that the uninsured rate could be high and life expectancy could be lower. Let’s see if this makes a difference.\n\nsummary(lm(female_le ~ uninsured_rate + median_hh_income + Region, data = life_exp))\n\n\nCall:\nlm(formula = female_le ~ uninsured_rate + median_hh_income + \n    Region, data = life_exp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7885 -0.8462 -0.0313  0.8068  5.1216 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       7.622e+01  2.087e-01 365.270   <2e-16 ***\nuninsured_rate    7.690e-02  6.873e-01   0.112   0.9109    \nmedian_hh_income  8.827e-05  2.937e-06  30.051   <2e-16 ***\nRegionNortheast   1.858e-01  1.080e-01   1.720   0.0856 .  \nRegionSouth      -1.407e+00  8.180e-02 -17.197   <2e-16 ***\nRegionWest        2.338e-01  1.124e-01   2.081   0.0376 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.298 on 1831 degrees of freedom\nMultiple R-squared:  0.5596,    Adjusted R-squared:  0.5584 \nF-statistic: 465.3 on 5 and 1831 DF,  p-value: < 2.2e-16\n\n\nWhich regions are statistically significant? What is the reference group? (Hint: you can type unique(life_exp$region) into the console to see what the regions are.)\nThe coefficient on uninsured rate is now even smaller and barely statistically significant. Going from the 25th to 75th percentiles only decreases life expectancy by 0.1 years.\nNow we can see that using regression to “control for” variables gives us the opportunity to see what relationships between data really are after removing omitted variable bias."
  },
  {
    "objectID": "02_assignments/lab-08.html",
    "href": "02_assignments/lab-08.html",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "",
    "text": "Practice reading in, cleaning, merging, and reshaping data in R (things that are really hard in Excel!)\nMake some cool graphs\nRun a multiple regression analysis in R using state and year fixed effects\n\n\n\n\nThese sheets are included in the alcohol_deaths.xlsx file.\n\nAlcohol-Related Deaths – population, deaths from alcohol-related causes, and crude death rate from alcohol-related causes by state from 2010 to 2021 (CDC data accessed via CDC WONDER)\nMarital Status – 5-year averages of number of married, single, divorced, and widowed people age 15 and older by gender by state from 2010 to 2022 (Census data accessed via NHGIS)\nReligiosity – percent of people who are highly religious and percent of people who attend religious services every week by state in 2014 (Pew)\nUnemployment – annual unemployment by state by year from 2004 to 2023 (BLS)\n\nWe will load in the data, merge it, and run a regression predicting alcohol-related deaths based on unemployment, marital status, and religiosity.\n\n\n\n\nreadxl\ntidyverse\nggplot2\njanitor - package to clean dataset names\nstargazer - package to create nice-looking output tables\ncorrplot - package to visualize correlation between variables\n\n\n\n\nFor this assignment, you will complete a quiz as you work through the lab."
  },
  {
    "objectID": "02_assignments/lab-08.html#step-0-create-new-r-script",
    "href": "02_assignments/lab-08.html#step-0-create-new-r-script",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 0: Create New R Script",
    "text": "Step 0: Create New R Script\nCreate an R script, and use # to give the script a title. Then, load in the libraries you will use. You need readxl, tidyverse, ggplot2, stargazer, and corrplot. We haven’t used all these, so you may need to install them first. Also, set your working directory using setwd(). If you have a lot of objects in your environment, you may want to clear them with this code: rm(list = ls()). This information is called your header. I recommend labeling each of the following sections with section headers, like this: #### Read in Data ####. Here is what it might look like:\n\n#### Lab 8: Alcohol Deaths and Fixed Effects ####\nsetwd(\"[MY WORKING DIRECTORY]\")\nrm(list = ls())\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stargazer)\nlibrary(corrplot)\nlibrary(janitor)"
  },
  {
    "objectID": "02_assignments/lab-08.html#step-1-read-in-data",
    "href": "02_assignments/lab-08.html#step-1-read-in-data",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\nRead in the relevant sheets from the Excel file. Here is a table of the sheet names and the names I gave the data frames in R.\n\n\n\nSheet\nName\n\n\n\n\nAlcohol-Related Deaths\ndeaths\n\n\nMarital Status\nmarry\n\n\nReligiosity\nrelig\n\n\nUnemployment\nunemp\n\n\n\nUse the read_excel() function from the readxl library to read in the data. To read in each sheet, you will need to indicate which sheet you want. For example, to load in the “Alcohol-Related Deaths” sheet:\n\ndeaths <- read_excel(\"alcohol_deaths.xlsx\", sheet = \"Alcohol-Related Deaths\")\n\nNote that the Unemployment sheet has several header rows, and the actual column names don’t start until row 4. To ignore those rows, type skip = 3 as an option in your read_excel() function."
  },
  {
    "objectID": "02_assignments/lab-08.html#step-2-switch-data-from-wide-to-long",
    "href": "02_assignments/lab-08.html#step-2-switch-data-from-wide-to-long",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 2: Switch Data from Wide to Long",
    "text": "Step 2: Switch Data from Wide to Long\nEvery sheet except for marry is now organized with a row for each state and a column for each year. For example, type head(deaths) to see this. For merging and analysis, we would prefer the data be organized such that each state*year combination has a row, for example:\n\n\n\nState\nYear\nVariable 1\nVariable 2\n\n\n\n\nAlabama\n2010\n\n\n\n\nAlabama\n2011\n\n\n\n\nAlabama\n2012\n\n\n\n\nAlabama\n2013\n\n\n\n\n\nTo do this, we will use pivot_longer() from the tidyverse. This asks for columns to take values from, then puts the column headers as a new column. The easiest way to understand this is to look at the examples in the documentation here: https://tidyr.tidyverse.org/reference/pivot_longer.html.\nLet’s start with the deaths data frame. We only want the crude rate, not the total deaths or population. We will tell dplyr/tidyr that we want any columns that start with Crude_Rate_ to be smushed into one column called death_rate, and the year portion of the column names to go into a new column called year. Here’s the code with comments. (Remember that |> and %>% are equivalent.)\n\ndeaths <- deaths |> # Tell dplyr which data frame we're using\n  pivot_longer(\n    cols = starts_with(\"Crude_Rate_\"), # Columns we want data from\n    names_to = \"year\", # Column name for column headers\n    names_prefix = \"Crude_Rate_\", # Prefix of the column name\n    values_to = \"death_rate\" # What we want the column with data to be called\n  ) |> \n  select(State, `State Code`, year, death_rate) |>  # Only keep important columns\n  rename(state_fip = `State Code`) # Rename so no space\n\nTry this, then type head(deaths) to see how the data changed.\nWe need to do the same for unemp. Try using pivot_longer on your own. I recommend using clean_names() from the janitor package to fix the weird formatting: unemp |> clean_names(). If you can’t figure it out how to pivot_longer, there is a video tutorial online."
  },
  {
    "objectID": "02_assignments/lab-08.html#step-3-variable-calculations",
    "href": "02_assignments/lab-08.html#step-3-variable-calculations",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 3: Variable Calculations",
    "text": "Step 3: Variable Calculations\nBefore merging, we will do a few things to make sure we have the variables we want and the right merge keys. In the unemp data frame, we do not have a state code. However, in the Series ID variable, the state code is in the 6th & 7th characters. Use the function str_sub to get those 2 characters. str_sub is a function that wants the variable you are pulling from, the start of the characters you want to extract, and the end. For instance, if I wanted to make a new variable from the 2nd and 3rd characters of a variable named X1, I would type new_var = str_sub(X1, 2, 3). We want to make this numeric to match the state_fip in the death dataframe, so we need to do as.numeric(str_sub()). Make a new numeric variable called state from the 6th & 7th characters of Series ID. Reduce unemp to the columns state, year, and unemp using select.\nWe also need to calculate the percent married and percent divorced in the marry dataframe. This code shows you how to calculate pct_married, then has a spot for you to add the answer for how to calculate pct_divorced. Calculate these, and reduce to STATE, year, pct_married, and pct_divorced using select.\n\nmarry <- marry |> \n  rowwise() |> \n  mutate(year = str_sub(YEAR, -4, -1),\n         pct_married = sum(Male_Married, Female_Married)/\n           sum(across(c(starts_with(\"Male_\"), starts_with(\"Female_\")))),\n         pct_divorced = # Your answer here) |> \n  select()"
  },
  {
    "objectID": "02_assignments/lab-08.html#step-4-merge-data",
    "href": "02_assignments/lab-08.html#step-4-merge-data",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 4: Merge data",
    "text": "Step 4: Merge data\nUse the merge() function to merge the four dataframes together. I would merge them all into the deaths dataframe. Here is an example for the first one to get you started. Note that you have 2 keys to merge on, so they are both named in the by.x and by.y options. If you only have one key to merge on, like in the relig dataframe, you don’t need both variables.\n\ndeaths <- merge(deaths, marry, by.x = c(\"State\", \"year\"), by.y = c(\"STATE\", \"year\"))\n\n\n\n\nYou may want to remove all the extra dataframes to keep your environment clean:\n\nrm(marry, relig, unemp)"
  },
  {
    "objectID": "02_assignments/lab-08.html#step-5-descriptive-statistics",
    "href": "02_assignments/lab-08.html#step-5-descriptive-statistics",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 5: Descriptive Statistics",
    "text": "Step 5: Descriptive Statistics\nWe want to learn more about our data before we throw it in a regression. First, let’s get descriptive statistics using stargazer. The default output is into LaTeX, a typesetting software. We’ll output it to text instead using the type = \"text\" option.\n\nstargazer(deaths, type = \"text\")\n\nWe’ll also create a correlation table. This table represents the direction and strength of the relationship between each pair of variables using the color and size of circles.\n\ncorr_table <- cor(deaths[,4:9]) # Only select numeric variables\ncorrplot(corr_table, type = \"upper\")\n\nThis plot shows the correlations between the numeric variables."
  },
  {
    "objectID": "02_assignments/lab-08.html#step-6-plots",
    "href": "02_assignments/lab-08.html#step-6-plots",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 6: Plots",
    "text": "Step 6: Plots\nSometimes, visualizations can help us understand our data more easily than descriptive statistics.\nFirst, let’s make a column chart that shows the states with the highest alcohol-related death rates in 2021. We can use dplyr and ggplot2 in the same functions!\n\ndeaths |> \n  filter(year == \"2021\") |> # Only use data from 2021\n  mutate(death_quartile = percent_rank(death_rate)) |> # Calculate percentile ranks\n  filter(death_quartile > 0.80) |> # Filter to top 20 percentile death rates\n  ggplot(aes(x = reorder(State, -death_rate), y = death_rate)) + # Plot!\n    geom_col() +\n    xlab(\"State\") +\n    ylab(\"Alcohol-Related Death Rate\") +\n    theme_classic() \n\n\n\n\nTry doing the same plot for the lowest 20 percentile.\nLet’s also make scatter plots to find the association between our explanatory variables and the dependent variable, death rate.\n\nggplot(deaths, aes(x = unemp, y = death_rate)) +\n  geom_point() +\n  xlab(\"Unemployment Rate\") +\n  ylab(\"Alcohol-Related Death Rate\") +\n  theme_classic()\n\nDo the same plot for percent highly religious and percent divorced against death rate."
  },
  {
    "objectID": "02_assignments/lab-08.html#step-7-regression",
    "href": "02_assignments/lab-08.html#step-7-regression",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Step 7: Regression",
    "text": "Step 7: Regression\n\nModel 1\nWe are going to run several regressions. First, we’ll estimate the following model:\n\\[\ndeathrate_{i,t}=\\beta_0 + \\beta_1*unemployment_{i,t} +\\beta_2*\\%divorced_{i,t}+\\beta_3*\\%religious_i\n\\]\n\\(\\%Religious\\) is only from one year, so it does not have the \\(t\\) subscript.\nBefore you run the regression, what do you think the relationship would be between the 3 variables and the death rate from alcohol?\n\nreg1 <- lm(death_rate ~ unemp + pct_highly_religious + pct_divorced, data = deaths)\nsummary(reg1)\n\nWhat are the relationships between each of the variables? What are their sizes? How much of the variation does the model explain?\n\n\n\n\n\n\nTip\n\n\n\nWhen looking at the size of the effect, we have to think about how the variables are defined. All three independent variables should be percents; however, the religion and divorce variables are fractions. In the case of pct_divorced, a one-unit increase is associated with a 182.6 unit increase in death rate. However, a one-unit increase is the same thing as a 100 percentage point increase!\nWhile the coefficient on divorce looks much bigger than the coefficient on unemployment, in reality, it is not. If we had both in terms of percents, a one percentage point increase in unemployment is associated with 0.5 lower death rate, and a one percentage point increase in divorced rate is associated with a 1.8 higher death rate.\nWe can also think about effect size in terms of what values the variables can take. % divorced goes from 8% to 14%, so going from the lowest to the highest value would be associated with about \\(1.8*(14-8)=10.8\\) change in death rate. On the other hand, the percent religious goes from 33% to 77%, so going from the lowest to the highest would be associated with a \\(0.2*(77-33)=8.8\\) change in death rate.\n\n\n\n\nModel 2\nWe might expect that the year will have an effect on alcohol deaths, particularly since this period includes the pandemic. We can include year as a factor variable to remove any variation that is part of a larger trend.\n\\[\ndeathrate_{i,t}=\\beta_0 + \\beta_1*unemployment_{i,t} +\\beta_2*\\%divorced_{i,t}+\\beta_3*\\%religious_i +\\alpha *year_t\n\\]\nI use \\(\\alpha\\) to represent the year coefficients because, since this a factor variable, there is actually a dummy for each year besides 2010, so there are 11 coefficients. Year is already defined as a factor variable, so we don’t need to do anything special. Do the same model as before, but add year as an explanatory variable. Call this model reg2.\n\n\n\n\n\nModel 3\nWe may also expect that there are differences between the states that are not captured in our model. We can do something called a state fixed effect, where we include a dummy variable for each state. Note that if we try to use state_fip as our dummy variable, it will be read as a numeric variable, which is not what we want! The numbers in state_fip don’t actually mean anything. Do the regression model from before, but add State as an explanatory variable. Call this model reg3.\n\n\n\nAlso, if we use the summary(reg3) function, we will get a huge output with coefficients for 50 states and 10 years. You can do this and take a look at the coefficients - which have the highest coefficient? Lowest? The reference category is Alabama. Do these results look the same as the column charts we used before?\nIn most papers, authors just report the other explanatory variables, and indicate in their tables that they used fixed effects. We can do that here and include the three regressions in one table so we can compare using stargazer.\n\nstargazer(reg1, # Specify which regressions\n          reg2, \n          reg3, \n          omit = c(\"year\", \"State\"), # Remove fixed effects\n          omit.labels = c(\"Year FE\", \"State FE\"), # Put labels for fixed effects\n          type = \"text\") # Indicate not to do LaTeX output\n\n\n\n\n\n\n\nWarning\n\n\n\nSome versions of stargazer will not correctly fill out the Fixed Effects part of the table. For instance, in mine, model 3 says it does not have state fixed effects. You would have to change this to “Yes” if you put it in a paper.\n\n\nThis table helps us compare models really easily! Look at the coefficients - do any of them change? What about the \\(R^2\\)?"
  },
  {
    "objectID": "02_assignments/lab-08.html#conclusion",
    "href": "02_assignments/lab-08.html#conclusion",
    "title": "Lab 8: Alcohol Deaths and Fixed Effects",
    "section": "Conclusion",
    "text": "Conclusion\nThe methods used in this lab should be very helpful for your final project. When you start analyzing your data, I recommend following this lab as a guide."
  },
  {
    "objectID": "02_assignments/lab-09.html",
    "href": "02_assignments/lab-09.html",
    "title": "Lab 9: Transformations and Interactions",
    "section": "",
    "text": "Perform and interpret a polynomial regression\nLog transform a variable, incorporate it into a regression, and interpret it\nUtilize and interpret interaction terms in regression\n\n\n\n\nWe will load the CASchools dataset from an R package. This dataset includes data about 420 schools in California in 1996-1998. This website has information about each of the variables.\n\n\n\n\nAER: has the data we will use in the lab\nggplot2: used for graphing\n\n\n\n\nAt the end of the demonstration, there is an assignment for you to do with different data. You will answer associated questions on Canvas."
  },
  {
    "objectID": "02_assignments/lab-09.html#step-1-load-in-the-data",
    "href": "02_assignments/lab-09.html#step-1-load-in-the-data",
    "title": "Lab 9: Transformations and Interactions",
    "section": "Step 1: Load in the data",
    "text": "Step 1: Load in the data\nTo do the lab, we will load in the data from the AER package. We also need to calculate the average class size, and create a score that is the average of the reading and math score for the school. This score will be our primary outcome.\n\nrm(list=ls()) # Clears the R environment\n\n# Read in libraries and data\nlibrary(ggplot2)\nlibrary(AER)\ndata(CASchools)\n\n# Calculate the number of students per teacher\nCASchools$size <- CASchools$students/CASchools$teachers\n# Calculate the average of math and reading score\nCASchools$score <- (CASchools$read + CASchools$math) / 2"
  },
  {
    "objectID": "02_assignments/lab-09.html#step-2-income-and-school-quality-analysis---polynomials",
    "href": "02_assignments/lab-09.html#step-2-income-and-school-quality-analysis---polynomials",
    "title": "Lab 9: Transformations and Interactions",
    "section": "Step 2: Income and School Quality Analysis - Polynomials",
    "text": "Step 2: Income and School Quality Analysis - Polynomials\n\nLinear Regression\nThe first thing we are going to explore with the data is how the average income in the district is related to the reading and math score. What do you think the relationship between income and test scores would be?\nWe can first test this with a simple correlation: cor(CASchools$score, CASchools$income). There is a positive relationship! But let’s run a regression and plot the points to be more thorough.\n\n# fit a simple linear model\nlinear_model <- lm(score ~ income, data = CASchools)\nsummary(linear_model)\n\n\nCall:\nlm(formula = score ~ income, data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.574  -8.803   0.603   9.032  32.530 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 625.3836     1.5324  408.11   <2e-16 ***\nincome        1.8785     0.0905   20.76   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.39 on 418 degrees of freedom\nMultiple R-squared:  0.5076,    Adjusted R-squared:  0.5064 \nF-statistic: 430.8 on 1 and 418 DF,  p-value: < 2.2e-16\n\n\nGreat! If the average income in a district is $1,000 higher, the expected score is 1.88 higher. Income ranges from 5,000 to 55,000, and the scores range from 605 to 707, so that is a pretty big effect! Going from the 1st quartile ($10,639) to the 3rd quartile ($17,629) would increase the score by two thirds of a standard deviation.\nNow, let’s plot the data to see what it looks like.\n\n# plot the observations\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~x, se = F, color = \"steelblue\") +\n  theme_classic()\n\n\n\n\nIn a linear regression, X and Y are supposed to be linearly related. That means that the effect that X has on Y should be the same at every value of X. However, in this example, it is clearly not the case! The points look curved, but the linear regression line is, well, linear. The regression predicts too high of test scores for low- and high-income districts, but too low of test scores for middle income districts.\nIncome tends to be strongly right skewed, meaning that most of the observations are grouped around the median, but there are some very high values that make the mean high.\nLet’s plot a histogram of income to see if this income variable is right skewed.\n\nggplot(CASchools, aes(x = income)) +\n  geom_histogram() +\n  xlab(\"Income\") +\n  theme_classic()\n\n\n\n\nThe data is right skewed, so we probably need to correct for that. We can try a few different specifications that will fit the data better.\n\n\nQuadratic Regression\nWe can model test scores as a function of income and income squared. The corresponding regression model is:\n\\[ TestScore_i = \\beta_0 + \\beta_1income_i + \\beta_2income_i^2 + \\varepsilon_i \\] This is called a quadratic regression model. \\(income^2\\) is treated as an additional explanatory variable. Note that since \\(income^2\\) is not a linear transformation of \\(income\\), it does not create perfect multicollinearity.\nIf the relationship between test scores and district income is linear, then the coefficient on \\(income^2\\) will not be significantly different from 0. If the relationship is quadratic, then the relationship will be significantly different from zero. This corresponds to the hypothesis test:\n\\[ H_0: \\beta_2 = 0 \\] \\[H_A: \\beta_2 \\neq 0 \\]\nThe p-value we use to test this hypothesis is in the last column of the summary table.\nWe will use the poly(x, k) function, which will turn whatever x variable we input into a kth order polynomial.\n\n\n\nCall:\nlm(formula = score ~ poly(income, 2), data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.416  -9.048   0.440   8.347  31.639 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)      654.1565     0.6209 1053.633  < 2e-16 ***\npoly(income, 2)1 277.8568    12.7238   21.838  < 2e-16 ***\npoly(income, 2)2 -85.9935    12.7238   -6.758 4.71e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.72 on 417 degrees of freedom\nMultiple R-squared:  0.5562,    Adjusted R-squared:  0.554 \nF-statistic: 261.3 on 2 and 417 DF,  p-value: < 2.2e-16\n\n\nThe regression equation is: \\[\\hat{TestScore}_i = 654.2 +277.9*income-85.99*income^2\\]\nWe now draw the same scatter plot as for the linear model and add the regression line for the quadratic model.\n\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~x, se = F, color = \"steelblue\") +\n  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F, color = \"red\") +\n  theme_classic()\n\n\n\n\nThe quadratic function clearly fits the data much better than the linear model.\n\n\nPolynomial Regression\nWe could generalize the quadratic model to include any number of polynomial degrees. \\[ \\hat{Y}_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i^2 + \\beta_3X_i^3 + ... + \\beta_kX_i^k \\] A cubic model, for instance, is a polynomial model with the square and cube of the variable, so k = 3.\n\n# estimate a cubic model\ncubic_model <- lm(score ~ poly(income, 3), data = CASchools)\nsummary(cubic_model)\n\n\nCall:\nlm(formula = score ~ poly(income, 3), data = CASchools)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44.28  -9.21   0.20   8.32  31.16 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)        654.16       0.62 1055.034  < 2e-16 ***\npoly(income, 3)1   277.86      12.71   21.867  < 2e-16 ***\npoly(income, 3)2   -85.99      12.71   -6.767 4.47e-11 ***\npoly(income, 3)3    18.46      12.71    1.452    0.147    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.71 on 416 degrees of freedom\nMultiple R-squared:  0.5584,    Adjusted R-squared:  0.5552 \nF-statistic: 175.4 on 3 and 416 DF,  p-value: < 2.2e-16\n\n\nOne problem with including an extra polynomial is that, as we add additional dimensions, the model becomes harder and harder to interpret. Additionally, we only want to include predictors that make our model better. What happened to the \\(Adjusted R^2\\) between the quadratic regression and the cubic regression? Is the cubic term significant?\nSee if you can add the line from the cubic equation to your scatterplot.\n\n\nInterpreting Polynomial Models\nPolynomials are difficult to interpret. One method for interpreting polynomials is to take the derivative of the equation. For instance, the cubic model has the following regression equation: \\[ score_i = 654 + 278*income_i -86*income_i^2 +19*income_i^3 +\\varepsilon_i \\]\nThe derivative of the equation with respect to income would be: \\[ \\frac{\\partial{score_i}}{\\partial{income_i}} = 278-86*2*income_i+19*3*income_i^2\\] \\[=278-172*income_i+57*income_i^2 \\]\nWe would get the following interpretation: “A one-unit increase in income would lead to a \\(278-172income_i+57income_i^2\\) increase in expected test score.” Confusing!\nAlternatively, you can just show a graph of the relationship and predict the effects at a few different values. Let’s predict what happens with the quadratic model when we change income at a few different levels. First, we will test what happens when income goes from 10,000 to 11,000. (Remember that in the data, income is in 1,000s.)\n\n# set up data for prediction\nnew_data <- data.frame(income = c(10, 11))\n# do the prediction\nY_hat <- predict(quadratic_model, newdata = new_data)\n# compute the difference\ndiff(Y_hat)\n\n       2 \n2.962517 \n\n\nHow does that compare to the change if income goes from 40 to 41?"
  },
  {
    "objectID": "02_assignments/lab-09.html#step-3-income-and-school-quality-analysis---logarithms",
    "href": "02_assignments/lab-09.html#step-3-income-and-school-quality-analysis---logarithms",
    "title": "Lab 9: Transformations and Interactions",
    "section": "Step 3: Income and School Quality Analysis - Logarithms",
    "text": "Step 3: Income and School Quality Analysis - Logarithms\nAnother way to specify a nonlinear regression function is to use the natural logarithm of X and/or Y. Logarithms convert changes in variables into percentage changes. This is convenient as many relationships are naturally expressed in terms of percentages. Often, if you are using a right-skewed variable like income, you should automatically take the logarithm.\nThere are three different kinds of models with log transformations:\n\nLevel-Log: Transform X to be log(X), but do not transform Y (use Y’s level).\nLog-Level: Transform Y to be log(Y), but do not transform X (use X’s level).\nLog-Log: Transform X and Y so that log(Y) ~ log(X).\n\nThe interpretation of the regression coefficients is different in each case. This table shows the interpretation by case:\n\n\n\n\n\n\n\n\nCase\nEquation\nInterpretation of \\(\\beta_1\\)\n\n\n\n\nLevel-Log\n\\[                                                                                               \n                                                                 \\hat{Y}_i=\\beta_0+\\beta_1ln(X_i)              \n                                                                 \\]\nA 1% change in X is associated with a \\(0.01*\\beta_1\\) change in Y.\n\n\nLog-Level\n\\[                                                                                               \n                                                                 ln(\\hat{Y_i})=\\beta_0+\\beta_1X_i              \n                                                                 \\]\nA 1 unit change in X is associated with a \\(100*\\beta_1\\%\\) change in Y.\n\n\nLog-Log\n\\[                                                                                               \n                                                                 ln(\\hat{Y_i})=\\beta_0+\\beta_1ln(X_i)          \n                                                                 \\]\nA 1% change in X is associated with a \\(\\beta_1\\%\\) change in Y. This is also the elasticity of Y with respect to X.\n\n\n\n\nCase 1: Log X, Level Y\nThe regression form of this model is: \\[ \\hat{Y}_i=\\beta_0 + \\beta_1ln(X_i) \\]\nWe do not have to create a new variable to use lm(). We can simply let R know that we want to log-transform X within the function.\n\n# estimate a level-log model\nLinearLog <- lm(score ~ log(income), data = CASchools) # log() is the natural log\nsummary(LinearLog)\n\n\nCall:\nlm(formula = score ~ log(income), data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.256  -9.050   0.078   8.230  31.214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  557.832      4.200  132.81   <2e-16 ***\nlog(income)   36.420      1.571   23.18   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.62 on 418 degrees of freedom\nMultiple R-squared:  0.5625,    Adjusted R-squared:  0.5615 \nF-statistic: 537.4 on 1 and 418 DF,  p-value: < 2.2e-16\n\n# Plot the function\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~log(x), se = F, color = \"purple\") +\n  theme_classic()\n\n\n\n\nUsing log transformations allows us to interpret the model using percent changes rather than unit changes. For instance, if the coefficient were 42.6, we would say that “a 1% increase in income is associated with a 0.01*36.4=.364 expected increase in average score”.\nIf we wanted, we could also calculate the estimated effect at different values of X like we did for polynomials.\n\n# set up new data\nnew_data <- data.frame(income = c(10, 11, 40, 41))\n# predict the outcomes\nY_hat <- predict(LinearLog, newdata = new_data)\n# compute the expected difference\nY_hat_matrix <- matrix(Y_hat, nrow = 2, byrow = TRUE)\nY_hat_matrix[, 2] - Y_hat_matrix[, 1]\n\n[1] 3.471166 0.899297\n\n\nGoing from 10 to 11 thousand dollars has a bigger effect than going from 40 to 41 thousand dollars.\n\n\nCase 2: Level X, Log Y\nWe can just log the dependent variable and keep x level. The model is: \\[ ln(Y_i) = \\beta_0 + \\beta_1X_i +\\varepsilon_i \\]\n\n# estimate a log-linear model\nLogLinear <- lm(log(score) ~ income, data = CASchools)\nsummary(LogLinear)\n\n\nCall:\nlm(formula = log(score) ~ income, data = CASchools)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.06285 -0.01340  0.00114  0.01413  0.04913 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.4393623  0.0023638 2724.16   <2e-16 ***\nincome      0.0028441  0.0001396   20.37   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02065 on 418 degrees of freedom\nMultiple R-squared:  0.4982,    Adjusted R-squared:  0.497 \nF-statistic:   415 on 1 and 418 DF,  p-value: < 2.2e-16\n\n\nThe estimated regression function is: \\[ ln(TestScore_i) = 6.439 + 0.00284*income_i+\\varepsilon_i \\] This means that if income increases by 1 unit ($1,000), then expected test score increases by \\(100*0.00284=0.284\\%\\).\n\n\nCase 3: Log X, Log Y\nThe log-log regression model is:\n\\[\nln(Y_i) = \\beta_0+\\beta_1ln(X_i)=\\varepsilon_i\n\\]\n\n# estimate the log-log model\nLogLog <- lm(log(score) ~ log(income), data = CASchools)\nsummary(LogLog)\n\n\nCall:\nlm(formula = log(score) ~ log(income), data = CASchools)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.066458 -0.013658  0.000508  0.012903  0.047856 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.336349   0.006453  981.90   <2e-16 ***\nlog(income) 0.055419   0.002414   22.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01938 on 418 degrees of freedom\nMultiple R-squared:  0.5578,    Adjusted R-squared:  0.5567 \nF-statistic: 527.2 on 1 and 418 DF,  p-value: < 2.2e-16\n\n\nThe equation from this model is: \\[ ln(TestScore_i) = 6.34 + 0.06*ln(income_i) +\\varepsilon_i \\]\nA 1% increase in thousands of dollars of income is associated with a 0.06 % increase in test score.\nThe following code plots all of the log models on top of the scatterplot.\n\nCASchools$pred_log_level <- exp(predict(LogLinear))\nCASchools$pred_log_log <- exp(predict(LogLog))\n\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y ~ x, se = F, color = \"green4\") +\n  geom_smooth(method = 'lm', formula = y ~ log(x), se = F, color = \"purple\") +\n  geom_line(aes(y = pred_log_level), color = \"blue\", size = 1) +\n  geom_line(aes(y = pred_log_log), color = \"red3\", size = 1) +\n  theme_classic()\n\n\n\n\nThe log-level and level-level models look essential the same, while the level-log and log-log models look the same."
  },
  {
    "objectID": "02_assignments/lab-09.html#step-4-picking-the-best-model",
    "href": "02_assignments/lab-09.html#step-4-picking-the-best-model",
    "title": "Lab 9: Transformations and Interactions",
    "section": "Step 4: Picking the Best Model",
    "text": "Step 4: Picking the Best Model\nWe can compare the adjusted R2 to see if any model has a significantly better fit than the rest.\n\n# compute the adj. R^2 for the nonlinear models\nadj_R2 <-rbind(\"linear\" = summary(linear_model)$adj.r.squared,\n\"quadratic\" = summary(quadratic_model)$adj.r.squared,\n\"cubic\" = summary(cubic_model)$adj.r.squared,\n\"LinearLog\" = summary(LinearLog)$adj.r.squared,\n\"LogLinear\" = summary(LogLinear)$adj.r.squared,\n\"LogLog\" = summary(LogLog)$adj.r.squared\n)\n# assign column names\ncolnames(adj_R2) <- \"adj_R2\"\nadj_R2\n\n             adj_R2\nlinear    0.5063795\nquadratic 0.5540444\ncubic     0.5552279\nLinearLog 0.5614605\nLogLinear 0.4970106\nLogLog    0.5567251\n\n\nThe Linear and Log-Linear models both have lower \\(R^2\\) values, but the others are pretty similar. So how do we choose a model?\nLet’s look at a plot that shows both the linear-log and quadratic models, potentially our two best models.\n\n# plot the observations\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~poly(x,2), se = F, color = \"steelblue\") +\n  geom_smooth(method = 'lm', formula = y~log(x), se = F, color = \"red\") +\n  theme_classic()\n\n\n\n\nNeither appears to be more appropriate for the data. I would then choose based on theory.\n\nIt’s easier to interpret the linear-log model, and it makes sense that income would be interpreted as percent changes since it is right skewed.\nIn the quadratic model, if we continued to higher values of income, scores would eventually go down, which doesn’t make much sense.\n\nI would go with the linear-log model."
  },
  {
    "objectID": "02_assignments/lab-09.html#step-5-class-size-region-and-scores-dummy-variable-interactions",
    "href": "02_assignments/lab-09.html#step-5-class-size-region-and-scores-dummy-variable-interactions",
    "title": "Lab 9: Transformations and Interactions",
    "section": "Step 5: Class Size, Region, and Scores: Dummy Variable Interactions",
    "text": "Step 5: Class Size, Region, and Scores: Dummy Variable Interactions\nThe last section dealt with transformations to make non-linear data into linear data. Now, we’ll look at interactions.\nI’m interested in whether class size affects test scores. I also think that the effect will vary based on region - in some regions like the bay area, students will be privileged enough that class size won’t matter as much, but in poorer regions like the central valley, I think class size will have a larger effect.\nI will create a region variable, then do a regression with the interaction.\n\n# Create region variable\nCASchools$region <- \"Northern CA\"\nCASchools$region[CASchools$county %in% c(\"Los Angeles\",\n                                         \"San Bernardino\",\n                                         \"Riverside\",\n                                         \"San Diego\",\n                                         \"Imperial\")] <- \"Southern CA\"\nCASchools$region[CASchools$county %in% c(\"Alameda\", \n                                         \"Contra Costa\", \n                                         \"Marin\", \n                                         \"San Francisco\", \n                                         \"San Mateo\", \n                                         \"Santa Clara\", \n                                         \"Solano\")] <- \"Bay Area\"\nCASchools$region[CASchools$county %in% c(\"Fresno\", \n                                         \"Inyo\", \n                                         \"Kern\", \n                                         \"Kings\", \n                                         \"Tulare\", \n                                         \"Alpine\", \n                                         \"Amador\", \n                                         \"Calaveras\", \n                                         \"Madera\", \n                                         \"Mariposa\", \n                                         \"Merced\", \n                                         \"Mono\",\n                                         \"San Joaquin\",\n                                         \"Stanislaus\", \n                                         \"Tuolumne\")] <- \"Central CA\"\nCASchools$region[CASchools$county %in% c(\"Monterey\",\n                                         \"San Benito\",\n                                         \"San Luis Obispo\",\n                                         \"Santa Barbara\",\n                                         \"Santa Cruz\",\n                                         \"Ventura\")] <- \"Central Coast\"\ndummy_interaction <- lm(score ~ size + region + region*size, data = CASchools)\nsummary(dummy_interaction)\n\n\nCall:\nlm(formula = score ~ size + region + region * size, data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.385 -11.779   0.117  11.505  39.283 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               798.139     24.954  31.985  < 2e-16 ***\nsize                       -6.745      1.312  -5.142 4.22e-07 ***\nregionCentral CA         -146.829     30.003  -4.894 1.42e-06 ***\nregionCentral Coast       -40.919     41.065  -0.996   0.3196    \nregionNorthern CA        -148.694     28.380  -5.239 2.58e-07 ***\nregionSouthern CA         -86.391     36.227  -2.385   0.0175 *  \nsize:regionCentral CA       6.294      1.556   4.046 6.22e-05 ***\nsize:regionCentral Coast    1.760      2.079   0.847   0.3978    \nsize:regionNorthern CA      7.206      1.489   4.840 1.85e-06 ***\nsize:regionSouthern CA      3.768      1.819   2.071   0.0390 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.22 on 410 degrees of freedom\nMultiple R-squared:  0.2906,    Adjusted R-squared:  0.275 \nF-statistic: 18.66 on 9 and 410 DF,  p-value: < 2.2e-16\n\n\nThe reference category is the “Bay Area” because it is first alphabetically. If we wanted to use a different reference level, we can change that using the relevel() function. For isntance, if we want to change the reference level to be Central California, we would run:\n\nCASchools$region <- relevel(as.factor(CASchools$region), ref = \"Central CA\")\n\nHowever, we will stick with the Bay Area as our reference for this example.\n\nDummy Variables\nWe’ll first look at the dummy variables. The coefficient on “Central California” is -146.8, and it is very significant. What this says is that, for every class size, the expected test score is 146.8 points lower in Central California than in the Bay Area. The range of the score variable is 605 to 707, so this is a huge difference! Northern California and Southern California also have significantly lower scores than the Bay Area.\n\n\nNumeric Variable\nThe coefficient on class size is -6.7. For every additional child per teacher on average, the expected score lowers by almost 7 points. This makes sense because we know class size is an important determinant of student outcomes. Class size ranges from 14 to 26, so if a class jumps from the lowest class size to the highest, expected scores would decline by about 7*8 = 56 points.\nHowever, since we have interactions, the only place that has the coefficient -6.7 on class size is the Bay Area.\n\n\nInteractions\nThe interaction between the Northern California region and class size is 7.2. That means that, in a district in Northern California, the coefficient on class size is \\(\\beta_1+\\beta_3=-6.7+7.2=0.5\\). If average class size increases by one student, the expected test score increases by half a point. (In reality, if we ran a regression on Northern California alone, this probably wouldn’t be a significant variable.)\nWhat we can take away from this is that larger classes have the most negative effect in the Bay Area, while in Northern California and Central California, they don’t seem to make a difference. The opposite of my hypothesis!"
  },
  {
    "objectID": "02_assignments/lab-09.html#step-6-poverty-expenditures-per-student-and-scores-continuous-interactions",
    "href": "02_assignments/lab-09.html#step-6-poverty-expenditures-per-student-and-scores-continuous-interactions",
    "title": "Lab 9: Transformations and Interactions",
    "section": "Step 6: Poverty, Expenditures per Student, and Scores: Continuous Interactions",
    "text": "Step 6: Poverty, Expenditures per Student, and Scores: Continuous Interactions\nIf we have an interaction between two continuous variables, the interpretation is a little more confusing. Let’s try. I hypothesize that in schools where more of the students are on free or reduced lunch, expenditures will have a more positive effect.\n\ninteraction <- lm(score ~ lunch + expenditure + lunch*expenditure, data = CASchools)\nsummary(interaction)\n\n\nCall:\nlm(formula = score ~ lunch + expenditure + lunch * expenditure, \n    data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.975  -5.489   0.024   6.075  34.192 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        6.474e+02  6.127e+00 105.656  < 2e-16 ***\nlunch             -2.949e-01  1.280e-01  -2.304   0.0217 *  \nexpenditure        6.295e-03  1.116e-03   5.639 3.16e-08 ***\nlunch:expenditure -5.705e-05  2.341e-05  -2.436   0.0153 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.027 on 416 degrees of freedom\nMultiple R-squared:  0.7771,    Adjusted R-squared:  0.7755 \nF-statistic: 483.5 on 3 and 416 DF,  p-value: < 2.2e-16\n\n\n\nThe coefficient on lunch is -0.29, which means that if the percent of students on free lunch goes up by one percentage point, the expected score goes down by 0.29 points.\nThe coefficient on expenditure per student is 0.006, which means that if expenditure per student goes up by $1,000, the expected score goes up by 6.\n\nHow do we interpret the interaction term? The term is very small and negative. This means that if there\nare more students on free lunch, the effect of more expenditures is less positive. Or, alternatively, if\nexpenditure is higher, then the effect of having more students on free lunch is more negative. (I would\nguess the first interpretation is more accurate.)\nJust like with polynomials, we could also use derivatives to calculate the effect.\n\\[\n\\hat{Y}=\\beta_0+\\beta_1X+\\beta_2Z+\\beta_3X*Z\n\\]\nTaking the derivative with respect to X:\n\\[\n\\frac{\\partial{Y}}{\\partial{X}}=\\beta_1+\\beta_3*Z\n\\]\nSo, the effect of X is dependent on Z. Let’s calculate the effect of expenditure at the median value of free school lunch percent, 41.8.\n\\[\nY=647.4+(-0.29-0.000057*41.8)*X=647.4-0.29X\n\\]\nPractically, this effect isn’t significant, even if it’s statistically significant. (See those little stars?) However, this still gives you the idea of what the point of interaction terms is. You should only include interaction terms if you think there is a good reason for them to be included.\nIf I were to report these results to a policy expert, I would tell them that in areas with more students on free or reduced lunch, it will take more resources to get similar outcomes."
  },
  {
    "objectID": "02_assignments/lab-09.html#assignment",
    "href": "02_assignments/lab-09.html#assignment",
    "title": "Lab 9: Transformations and Interactions",
    "section": "Assignment",
    "text": "Assignment\nDo a similar analysis to the above using the life expectancy/insurance data. Follow these steps, and answer the questions on Canvas.\n\nStep 1: Load in the data\nYou should have the code to load and merge the data from lab 7. (If you do not, you can email me telling me the importance of saving your R scripts, and I will give you the code or data.) You will need all of the sheets loaded and merged: life expectancy, income, insurance, and region. You will also need to calculate the uninsured rate like you did before.\n\n\nStep 2: Income and Life Expectancy: Polynomial Regressions\nGraph the relationship between income per capita (x axis) and female life expectancy (y axis), like we did in step 2. Run a linear regression predicting female life expectancy from income per capita. Then, run a quadratic regression.\n\n\nStep 3: Income and Life Expectancy: Log Transformations\nRun a regression where you log transform median household income, and one where you transform both median household income and life expectancy.\n\n\nStep 4: Picking the Best Model\nCompare all the models you ran.\n\n\nStep 5: Dummy-Numeric Interactions\nPerform a regression with female life expectancy as the outcome variable and region, uninsured rate, and the interaction between region and uninsured rate as your explanatory variables.\n\n\nStep 6: Numeric-Numeric Interactions\nPerform a regression with life expectancy as the outcome variable and uninsured rate, median household income, and the interaction between uninsured rate and median household income as the predictor variables. (Do not take the log of income, even though you probably would in a real analysis.)"
  },
  {
    "objectID": "02_assignments/lab-10.html",
    "href": "02_assignments/lab-10.html",
    "title": "Lab 10: Regression Assumptions",
    "section": "",
    "text": "Review regression assumptions\nLearn how to test assumptions and fix issues\n\n\n\n\nWe will use the Prestige and Hartnagel datasets from the car package, and the emissions dataset from Canvas.\n\n\n\ntidyverse\nggplot2\nlmtest\ncarData\ncar\nskedastic\nnortest\n\n\n\nAt the end of the demonstration, there is an assignment for you to do with different data."
  },
  {
    "objectID": "02_assignments/lab-10.html#load-in-data",
    "href": "02_assignments/lab-10.html#load-in-data",
    "title": "Lab 10: Regression Assumptions",
    "section": "Load in Data",
    "text": "Load in Data\nWe will use a few datasets in this lab. For the demonstration, we will use the Prestige data from the carData package. This data has the prestige of various occupations based on a survey from the 1960s, the average income of people with the occupation, and the average education of people with the occupation. We will also run a regression with the data.\n\ndata(Prestige)\n\npmodel <- lm(prestige ~ income, data = Prestige)\nsummary(pmodel)\n\n\nCall:\nlm(formula = prestige ~ income, data = Prestige)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.007  -8.378  -2.378   8.432  32.084 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.714e+01  2.268e+00   11.97   <2e-16 ***\nincome      2.897e-03  2.833e-04   10.22   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.09 on 100 degrees of freedom\nMultiple R-squared:  0.5111,    Adjusted R-squared:  0.5062 \nF-statistic: 104.5 on 1 and 100 DF,  p-value: < 2.2e-16\n\n\nJobs with higher income generally have higher prestige. For every dollar increase in income, prestige increases by 0.0029. Let’s test the regression assumptions:"
  },
  {
    "objectID": "02_assignments/lab-10.html#assumption-1-linearity",
    "href": "02_assignments/lab-10.html#assumption-1-linearity",
    "title": "Lab 10: Regression Assumptions",
    "section": "Assumption 1: Linearity",
    "text": "Assumption 1: Linearity\nOur first assumption is that x and y have to be linearly related. To test this, we can either plot the relationship between x and y, or we can use the Ramsey RESET test, which tests the null hypothesis that the relationship between x and y is linear.\n\nggplot(Prestige) +\n  geom_point(aes(x = income, y = prestige)) +\n  theme_classic() +\n  xlab(\"Income\") + \n  ylab(\"Prestige\")\n\n\n\nresettest(pmodel)\n\n\n    RESET test\n\ndata:  pmodel\nRESET = 10.339, df1 = 2, df2 = 98, p-value = 8.433e-05\n\n\nThe p-value for the RESET test is very small, which means we can reject the null hypothesis that income and prestige are linearly related."
  },
  {
    "objectID": "02_assignments/lab-10.html#assumption-2-normality-of-errors",
    "href": "02_assignments/lab-10.html#assumption-2-normality-of-errors",
    "title": "Lab 10: Regression Assumptions",
    "section": "Assumption 2: Normality of Errors",
    "text": "Assumption 2: Normality of Errors\nThe second assumption is that errors are normally distributed, because otherwise the t-tests are not appropriate for hypothesis testing. Note that this does not mean that all the variables have to be normally distributed. Graphically, we can test this by visually observing a histogram of the residuals, or we can do an Anderson-Darling test for normality, which tests the null hypothesis that the distribution is normal.\n\nhist(pmodel$residuals)\n\n\n\nnortest::ad.test(pmodel$residuals)\n\n\n    Anderson-Darling normality test\n\ndata:  pmodel$residuals\nA = 1.0777, p-value = 0.00757\n\n\nThe histogram is slightly right skewed, and the p-value for the Anderson-Darling test is 0.008, which is quite significant. The residuals are not normally distributed."
  },
  {
    "objectID": "02_assignments/lab-10.html#assumption-3-homoskedasticity",
    "href": "02_assignments/lab-10.html#assumption-3-homoskedasticity",
    "title": "Lab 10: Regression Assumptions",
    "section": "Assumption 3: Homoskedasticity",
    "text": "Assumption 3: Homoskedasticity\nThe homoskedasticity assumption says that the variance of the error term is the same at all values of X. The intuition behind why this is bad is that if there are different errors at different values of x and the model has heteroskedasticity, there is a pattern in the errors that could be modeled, so there is another estimator that will do a better job of modeling than least squares.\nTo test for heteroskedasticity, plot the fitted values versus the residuals to see the residual at each predicted value of y. This will give overall heteroskedasticity for the model, though you can also do heteroskedasticity for each of the x variables by plotting the x value versus the residuals.\nThe numerical test is the white test for heteroskedasticity in the skedastic package, which tests the null hypothesis that the errors are homoskedastic.\n\nplot(pmodel$fitted.values, pmodel$residuals)\nabline(h = 0)\n\n\n\nskedastic::white(pmodel, interactions = TRUE)\n\n# A tibble: 1 × 5\n  statistic p.value parameter method       alternative\n      <dbl>   <dbl>     <dbl> <chr>        <chr>      \n1      8.10  0.0175         2 White's Test greater    \n\n\nThe White test rejected the null hypothesis, meaning the errors are heteroskedastic."
  },
  {
    "objectID": "02_assignments/lab-10.html#addressing-unmet-assumptions",
    "href": "02_assignments/lab-10.html#addressing-unmet-assumptions",
    "title": "Lab 10: Regression Assumptions",
    "section": "Addressing unmet assumptions",
    "text": "Addressing unmet assumptions\nSince the model did not meet any of those assumptions, we need to figure out how to make the model better. Income is usually a right-skewed variable that we think of in percentages, so we should take the log of income. I’m going to make a new model with log of income instead of income, and do the same tests as above.\n\npmodel2 <- lm(prestige ~ log(income), data = Prestige)\n\n# Linearity\nggplot(Prestige) +\n  geom_point(aes(x = log(income), y = prestige)) +\n  theme_classic() +\n  xlab(\"Log Income\") + \n  ylab(\"Prestige\")\n\n\n\nresettest(pmodel2)\n\n\n    RESET test\n\ndata:  pmodel2\nRESET = 4.9781, df1 = 2, df2 = 98, p-value = 0.008728\n\n# Normality of errors\nhist(pmodel2$residuals)\n\n\n\nnortest::ad.test(pmodel2$residuals)\n\n\n    Anderson-Darling normality test\n\ndata:  pmodel2$residuals\nA = 0.94283, p-value = 0.01636\n\n# Homoskedasticity\nplot(pmodel2$fitted.values, pmodel2$residuals)\nabline(h = 0)\n\n\n\nskedastic::white(pmodel2, interactions = TRUE)\n\n# A tibble: 1 × 5\n  statistic p.value parameter method       alternative\n      <dbl>   <dbl>     <dbl> <chr>        <chr>      \n1      2.91   0.234         2 White's Test greater    \n\n\nThe relationship between the log of income and prestige is more or less linear (visually but not statistically), and there is no longer heteroskedasticity (both visually and statistically). The errors do not look normally distributed, but when we have large sample sizes, this usually doesn’t notably impact results. The second model is much better than the first and likely is good enough."
  },
  {
    "objectID": "02_assignments/lab-10.html#assumption-4-low-multicollinearity",
    "href": "02_assignments/lab-10.html#assumption-4-low-multicollinearity",
    "title": "Lab 10: Regression Assumptions",
    "section": "Assumption 4: Low Multicollinearity",
    "text": "Assumption 4: Low Multicollinearity\nWhen we do analysis with multiple variables, they cannot be too closely correlated, or we will get strange results.\nTo test this, we calculate variance inflation factors, which run regressions predicting each of our explanatory variables with the other explanatory variables. The \\(R^2\\) of each of those regressions is put into the following formula: \\(1/(1-R^2)\\). For instance, if our primary regression is predicting prestige using log of income, education, the percent of the occupation that is women, and the type of occupation, then the variance inflation factor for log income would be the \\(1/(1-R^2)\\) for the regression: \\[ log(income) = \\beta_0 + \\beta_1*education +\\beta_2*\\%women+\\beta_3*type \\] R does this for us automatically with the vif() function from the car package. We should look closely at any variables where the VIFs are above 8 or 10 and consider removing one or making an index.\n\npmod3 <- lm(prestige ~ log(income) + education + women + type, data = Prestige)\nsummary(pmod3)\n\n\nCall:\nlm(formula = prestige ~ log(income) + education + women + type, \n    data = Prestige)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.8762  -4.0579   0.5503   4.2129  16.6400 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -115.67219   18.80181  -6.152 1.96e-08 ***\nlog(income)   14.65518    2.31151   6.340 8.42e-09 ***\neducation      2.97384    0.60205   4.940 3.49e-06 ***\nwomen          0.08382    0.03223   2.601   0.0108 *  \ntypeprof       5.29186    3.55585   1.488   0.1401    \ntypewc        -3.21599    2.40654  -1.336   0.1847    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.44 on 92 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.8654,    Adjusted R-squared:  0.8581 \nF-statistic: 118.3 on 5 and 92 DF,  p-value: < 2.2e-16\n\ncar::vif(pmod3)\n\n                GVIF Df GVIF^(1/(2*Df))\nlog(income) 3.400339  1        1.844001\neducation   6.405871  1        2.530982\nwomen       2.392123  1        1.546649\ntype        6.801055  2        1.614894\n\n\nNone of the variance inflation factors are above 10, so we don’t need to worry about taking any of the variables out."
  },
  {
    "objectID": "02_assignments/lab-10.html#assumption-5-no-influential-points",
    "href": "02_assignments/lab-10.html#assumption-5-no-influential-points",
    "title": "Lab 10: Regression Assumptions",
    "section": "Assumption 5: No influential points",
    "text": "Assumption 5: No influential points\nIf you have outliers in your dataset, they can seriously affect regression outcomes because the regression line can swing widely to try to capture that value. If there are outliers causing problems in our data, we may want to adjust our dataset, though not always. Finding influential values and outliers can sometimes reveal errors, or unfortunate coding decisions like indicating missing values with 999,999. You can fix those. Alternatively, you may find that taking the log or otherwise transforming the data will reduce outliers. One option that can be helpful is to use percentiles rather than levels, for instance with income.\nTo test for influential points, we can look at the scatterplot and see if there are any points that look like outliers and appear to change the relationship. Alternatively, we can use the Cook’s distance, which measures the influence of each value on the fitted values. There is also a visualization built into R to visualize Cook’s distance. If the Cook’s distance is greater than 0.5, we should look into the data point, and if it is greater than 1, then it is likely influencing our results. We’ll start with the second model.\n\ncooks <- cooks.distance(pmodel2)\nwhich(cooks > 0.5)\n\nbabysitters \n         63 \n\nplot(pmodel2, which = 4)\n\n\n\n\nBabysitters have higher prestige than would be anticipated by their income. Babysitters typically are young, and may still be in school, so we might consider removing them from our model if those are not the kind of workers we are interested in. Let’s try again with the model with more predictors.\n\ncooks <- cooks.distance(pmod3)\nwhich(cooks > 0.5)\n\nnamed integer(0)\n\nplot(pmod3, which = 4)\n\n\n\n\nWhen we add the other predictors, none of the Cook’s distances are greater than 0.5, and we do not need to worry about outliers."
  },
  {
    "objectID": "02_assignments/lab-10.html#assumption-6-independence-of-errors",
    "href": "02_assignments/lab-10.html#assumption-6-independence-of-errors",
    "title": "Lab 10: Regression Assumptions",
    "section": "Assumption 6: Independence of errors",
    "text": "Assumption 6: Independence of errors\nTo explore independence of errors, we are going to use a different dataset that uses time series data, which is usually more susceptible to issues. The Hartnagel dataset has male and female crime rates, female labor force participation, female education, and fertility in Canada from 1931 to 1968. We will test whether female labor force participation is related to crime rates. We will also test for autocorrelation (non-independence of errors) using the Durbin-Watson tests, which tests the null that there is no autocorrelation.\n\ndata(\"Hartnagel\")\ncrime <- Hartnagel # Rename the dataset something easier\nrm(Hartnagel)\ncrime_model <- lm(fconvict ~ partic, data = crime)\nsummary(crime_model)\n\n\nCall:\nlm(formula = fconvict ~ partic, data = crime)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.971 -22.224  -8.449  15.112  79.129 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   0.7671    37.8094   0.020   0.9839  \npartic        0.3159     0.1410   2.241   0.0313 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.15 on 36 degrees of freedom\nMultiple R-squared:  0.1224,    Adjusted R-squared:  0.09805 \nF-statistic: 5.022 on 1 and 36 DF,  p-value: 0.03128\n\nggplot(crime) +\n  geom_line(aes(x = year, y = fconvict), color = \"darkblue\") +\n  geom_line(aes(x = year, y = partic), color = \"magenta3\") +\n  theme_classic()\n\n\n\ndwtest(crime_model)\n\n\n    Durbin-Watson test\n\ndata:  crime_model\nDW = 0.19748, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nThe model suggests that there is a weak relationship. However, when we look at the graph, they aren’t exactly related, and there are other things going on, like WWII during the 1940s, that probably explain the movements together. Since there is significant autocorrelation, let’s add a lag and also try percent change.\n\n# Add lag\ncrime_mod <- lm(fconvict ~ partic + lag(fconvict), data = crime)\nsummary(crime_mod)\n\n\nCall:\nlm(formula = fconvict ~ partic + lag(fconvict), data = crime)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-45.802  -6.593  -1.820   7.843  33.344 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.89678   16.51370   0.963    0.343    \npartic        -0.04105    0.06741  -0.609    0.547    \nlag(fconvict)  0.95563    0.07529  12.693 1.87e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.38 on 34 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.8469,    Adjusted R-squared:  0.8379 \nF-statistic: 94.02 on 2 and 34 DF,  p-value: 1.399e-14\n\ndwtest(crime_mod)\n\n\n    Durbin-Watson test\n\ndata:  crime_mod\nDW = 1.2751, p-value = 0.004195\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Do % change\ncrime <- crime |> \n  mutate(d_fconvict = 100*(fconvict - lag(fconvict))/lag(fconvict),\n         d_partic = 100*(partic - lag(partic))/lag(partic))\ncrime_mod2 <- lm(d_fconvict ~ d_partic, data = crime)\nsummary(crime_mod2)\n\n\nCall:\nlm(formula = d_fconvict ~ d_partic, data = crime)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.957  -7.174  -0.411  10.647  25.404 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   1.7267     2.1725   0.795    0.432\nd_partic      0.3277     0.3501   0.936    0.356\n\nResidual standard error: 12.97 on 35 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.02442,   Adjusted R-squared:  -0.003451 \nF-statistic: 0.8762 on 1 and 35 DF,  p-value: 0.3557\n\ndwtest(crime_mod2)\n\n\n    Durbin-Watson test\n\ndata:  crime_mod2\nDW = 1.1768, p-value = 0.003673\nalternative hypothesis: true autocorrelation is greater than 0\n\nggplot(crime) +\n  geom_line(aes(x = year, y = d_fconvict), color = \"darkblue\") +\n  geom_line(aes(x = year, y = d_partic), color = \"magenta3\") +\n  theme_classic()\n\n\n\n\nWhile the lag and percent change model both do not find a relationship between labor force participation and crime, there is still significant autocorrelation according to the Durbin-Watson test. This is what makes time-series analysis so challenging: we often would need to include lags of errors and more than one lag of the outcome variable."
  },
  {
    "objectID": "02_assignments/lab-10.html#how-important-are-regression-assumptions",
    "href": "02_assignments/lab-10.html#how-important-are-regression-assumptions",
    "title": "Lab 10: Regression Assumptions",
    "section": "How important are regression assumptions?",
    "text": "How important are regression assumptions?\nOrdinary least squares (OLS) regression, what we’ve been learning, is fairly robust to unmet assumptions. It’s good practice to do the tests and make sure none of the assumptions are wildly unmet, and to test different specifications of your model to see if things change. However, if your model doesn’t pass all of the statistical tests, but you’ve done the work to make it the best it can be, you are probably good to go. Just be transparent when you discuss the issues with your model."
  },
  {
    "objectID": "02_assignments/lab-10.html#assignment",
    "href": "02_assignments/lab-10.html#assignment",
    "title": "Lab 10: Regression Assumptions",
    "section": "Assignment",
    "text": "Assignment\nNow it’s your turn to test the assumptions for a different analysis. Perform the following and put your results in a Word document or the text box on Canvas. Try to do\n\nLoad the emissions data.\nSubset the data to only 2018 using dplyr’s filter.\nRun a model to predict emissions using GDP per capita.\nPerform plots and tests to test for linearity, normality of errors, and homoskedasticity. Report your results and what you should do.\nRun a test for influential points. Are any points problematic? What should you do?\nRun a model predicting emissions using GDP per capita, population, and continent. Test for multicollinearity. Are any of these a problem?\nReload the data and subset the data to USA for all years. Perform the model predicting emissions using GDP per capita, and use a Durbin-Watson test to test for autocollinearity. Then try again using a lag."
  },
  {
    "objectID": "02_assignments/lab-02.html",
    "href": "02_assignments/lab-02.html",
    "title": "Lab 2: Managing Data in Excel",
    "section": "",
    "text": "In this lab, you will:\n\nreview how to do row-wise calculations in Excel\ncreate new spreadsheets using data from other sheets\nmerge data using sumifs and index match\n\n\n\n\nThis lab will be done in Excel.\n\n\n\n\nLife Expectancy-Insurance Data\n\n\n\n\nFor this assignment, answer the associated questions on Canvas. You will not turn in your spreadsheet."
  },
  {
    "objectID": "02_assignments/lab-02.html#part-1-get-to-know-your-data",
    "href": "02_assignments/lab-02.html#part-1-get-to-know-your-data",
    "title": "Lab 2: Managing Data in Excel",
    "section": "Part 1: Get to know your data",
    "text": "Part 1: Get to know your data\n\nStep 1: Open the data\nDownload the Life Expectancy-Insurance data spreadsheet from above. Open the file in Excel or Google Sheets. The file has 4 sheets:\n\n“Life Expectancy” – life expectancy by county, state, and the entire US for the years 1985 to 2010; the data come from the Global Health Data Exchange\n“NHGIS HH Inc” – median household income in 2010; the data come from the Census, gathered through NHGIS\n“NHGIS Insurance” – population counts of insured and uninsured people by county, broken down by age and sex, in 2010; this data also comes from the Census, gathered through NHGIS\n“Region” – regions assigned to states There are several different data types represented in these sheets.\n\nWhat kinds of data types are each of the variables? (A tricky one here is the FIPS code for each county; even though it looks like a number, it is actually a categorical variable because the numbers represent categories.)"
  },
  {
    "objectID": "02_assignments/lab-02.html#part-2-perform-variable-calculations",
    "href": "02_assignments/lab-02.html#part-2-perform-variable-calculations",
    "title": "Lab 2: Managing Data in Excel",
    "section": "Part 2: Perform variable calculations",
    "text": "Part 2: Perform variable calculations\n\nStep 2: Calculate the uninsured rate\nWe want to find the uninsured rate for people under the age of 65, since people over the age of 65 are usually covered by Medicare. Go to the “NHGIS Insurance” sheet. Make an empty column between the COUNTYA column and Total column. To do this, click on the letter of the column to the right of where you want to make a column. (See Figure 1.)\n\n\n\nFigure 1: Where to click to select entire column\n\n\nNow that you have the whole column selected, you can add a new column in two ways: 1) Right-click the column letter again (same as Figure 1) and select “Insert”. 2) Use the keyboard shortcut: “Ctrl+Shift++” on Windows, or “Control+I” on Mac. The second ends up being a lot faster! The formula for uninsured rate is: \\[ uninsured\\ rate = \\frac{uninsured\\ population}{total\\ population} \\]\nAdd the . In cell G4, type =sum(, a function that adds all of the values inside the parentheses. Add the male and female uninsured under 65 columns. Close the parentheses. That is the numerator. Use a forward slash to indicate you want to divide, then the denominator will be the sum of the total population under age 65, which you need to add two columns together for. It should look like =sum(...)/sum(...), and once you hit “Enter”, the value should be 0.103735. The “Total_Pop” column includes those over age 65, so using that as the denominator will not get you the right answer.\n\n\nStep 3: Populate the column\nTo populate the entire column with the uninsured rate for each county, select cell G4, and either: 1) Click the little green box in the bottom right corner of the cell (see Figure 2) and drag it down, or 2) Double click the little box in the bottom right corner of the cell.\n\n\n\nFigure 2: Little green fill box\n\n\nWhile the whole column is selected, change the data type to percentage. In the ribbon at the top, go to “Home” and under “Number”, select the percent sign (see the red circle in Figure 3). You can display more or fewer decimal places by selecting the buttons circled in blue in Figure 3.\n\n\n\nFigure 3: Number format buttons\n\n\nAdd a variable name in cell G3 so you know what it is. I would put something like “unins_rate”.\n\n\nStep 4: Calculate some descriptive statistics for the uninsured rate\nExplore the uninsured rate a little bit. In an empty cell in the “NHGIS Insurance” sheet, type =AVERAGE(. Then, select all of the values for the uninsured rate. Select cell G4, then hit “Ctrl+Shift+Down Arrow” (or “Command+Shift+Down Arrow” for Mac). Then, hit enter. You should get the answer 16.7%. Also calculate the median using =MEDIAN()."
  },
  {
    "objectID": "02_assignments/lab-02.html#part-3-merge-data",
    "href": "02_assignments/lab-02.html#part-3-merge-data",
    "title": "Lab 2: Managing Data in Excel",
    "section": "Part 3: Merge data",
    "text": "Part 3: Merge data\n\nStep 5: Remove life expectancy for years besides 2010\nWe only need the life expectancy for 2010. We could go through and manually delete every year besides 2010, but this is a lot of work, and it’s bad form to delete data. (What if we figure out we need the data for 2008 later on?) Start a new sheet. At the bottom of the screen where there are the tabs with the sheets, click the circle with the plus sign inside (see the blue circle in Figure 4). This will start a new sheet called “Sheet1”. Rename it something descriptive (like “Life Exp 2010”) by right clicking the sheet tab (see highlighted area in Figure 4).\n\n\n\nFigure 4: New Sheet Button\n\n\nFrom the “Life Expectancy” sheet, copy the “fips” column by selecting the whole column (click the letter C at the top) and either right clicking and selecting “Copy” or using the keyboard shortcuts (“Ctrl+C” for Windows, “Command+C” for Mac). Paste the column in your new sheet by either selecting cell A1 and hitting the keyboard shortcut for paste (“Ctrl+V” for Windows or “Command+V” for Mac), or by right clicking cell A1 and selecting “Paste”.\n\n\n\n\n\n\nTip\n\n\n\nRemember that to merge data we need a key, or a variable that uniquely identifies each of our units of observations in both datasets. For instance, if we were managing a company we may want a customer ID in each of our datasets on sales and customer history so that we can match those exactly without matching observations that actually aren’t the same customer.\n\n\n“FIPS” is a good key for our data. A FIPS code is a commonly used code where the first one to two digits represent the state, and last 3 digits represent the county. This means that each county has one FIPS code, and no FIPS codes are the same. (Many counties have the same name! There are multiple “Montgomery Counties”, for instance.) To get rid of the duplicate codes, select the column with the pasted FIPS code, and in the top ribbon go to “Data” and “Remove Duplicates” (see Figure 5).\n\n\n\nFigure 5: Remove duplicates button\n\n\nIn column B, write a column name called “Female Life Expectancy” in cell B1. We will merge the 2010 values from the “Life Expectancy” sheet into the new sheet. The easiest way to do this is a =SUMIFS() function. We will ask it to sum any values that equal the county and year we want, which should just be one value. SUMIFS requires:\n\na sum_range (the values you are adding together)\ncriteria_ranges (which column or row has the criteria you want)\ncriteria (which values in the criteria range meet our condition)\n\nThe sum_range is column E from the sheet “Life Expectancy” - FemaleLE. To select this column, select cell E1 and hit “Control+Shift+Down Arrow” for Windows or “Command+Shit+Down Arrow” for Mac. This will select E1:E81692. We want to make sure the range stays the same even as we copy our function to different rows, so make the cell references into absolute references by putting dollar signs in front of them. You can either manually put them in, or before you do anything else, hit F4.\nNext, select a criteria range. Our first criteria will be the fips code. Select the fips column in the “Life Expectancy” sheet like you did for female life expectancy, and make the cell references absolute. The criteria is the fips code in the row we are merging to. Select cell A2 in the “Life Exp 2010” sheet. Keep this cell reference relative (no dollar signs), since as we copy the function down, we want the county fips criteria to change.\nOur second criteria is year: 2010. Select the year column in “Life Expectancy” as the criteria range, make the reference absolute, and for the criteria, just type “2010”. The final formula will look something like this: =SUMIFS('Life Expectancy'!$E$2:$E$81692,'Life Expectancy'!$C$2:$C$81692,'Life Exp 2010'!A2,'Life Expectancy'!$D$2:$D$81692,\"2010\") That’s long and confusing! If you get it the first time, then great job! You may have to trouble shoot for a while. Double click the box in the lower right corner to send the function down the line.\n\n\n\n\n\n\nMerging Tips\n\n\n\n\n\nFor merging, the sum_range and criteria_range come from the sheet we’re getting data from, and the criteria comes from the sheet we’re merge the data to.\nTroubleshooting\n\nAre your ranges the same? If one column has row 2 to 1,000 selected, then all of the columns should have those rows selected. You can’t have one range be “A:A”, and another be “B2:B2385”, and another be “C1:C2385”.\nDid you accidentally delete or double up the sheet name?\nDo you have a criteria associated with each of your criteria ranges?\nIf your criteria is typed in, is it in quote marks?\n\n\n\n\n\n\nStep 6: Add male life expectancy\nDo the same for male life expectancy. Now you have a sheet that has the FIPS code and male & female life expectancy for 2010.\n\n\nStep 7: Merge the other data\nMerge all of our variables onto the same sheet. We will use the sheet with the 2010 life expectancy as our master sheet. We want the following variables:\n\nCounty ID (FIPS)\nCounty Name\nState Name\nLife Expectancy – Women\nLife Expectancy – Men\nUninsured Rate\nRegion\n\nAdd two new columns between the FIPS column and life expectancy column like we did in step 2. Label these “County” and “State”. Because these are text variables, we need to use a different function than SUMIFS. We will use INDEX(MATCH()). MATCH is a function where you provide a value to look up, and it searches an array (list of values) and tells you the location (row number) of the first time that value appears. INDEX is a function that returns a value from an array at whatever location you tell it. So, MATCH finds the row that contains the right observation using the key, and INDEX returns the value you want from that observation.\n\n\n\n\n\n\nExtra INDEX MATCH Practice\n\n\n\n\n\nOn the “INDEX MATCH Practice” sheet, practice using MATCH and INDEX. You will find Kelsey’s ID #, and the name of person #34.\n\nIn F2, use MATCH to find the location of Kelsey’s ID using =MATCH(\"Kelsey\", A2:A19, 0).\nIn F4, use INDEX to find the ID# in the 11th row using =INDEX(B12:B19, 11).\nIn F6, combine the 2: =INDEX(B2:B19, MATCH(\"Kelsey\", A2:A19, 0)).\nDo the opposite now: fill out H2, H4, and H6 for finding the name associated with ID#34.\nIf you’re still confused, try a few more: find the ID# for Linda, and find the name of the person with ID# 95.\n\n\n\n\nFind the row number of the county you want by using MATCH, looking up the FIPS code from the row you are on in the “2010 Life Exp” sheet in the FIPS column in the “Life Expectancy” sheet. In the INDEX function, use that value to look in the County column in the “Life Expectancy sheet. For all of the columns, make sure they are absolute cell references. Your function should look like this:\n=INDEX('Life Expectancy'!$B$2:$B$81692,MATCH(A2,'Life Expectancy'!$C$2:$C$81692,0))\nClick the box in the lower right corner of the cell to copy the function down the column. Use a similar function to merge the state name as well.\nIn the “NHGIS Insurance” sheet, there is not a fips code. We will need to create a FIPS code column using the STATEA and COUNTYA columns. Create a new column just before the uninsured rate column. Treat the fips like a number and multiply STATEA by 1,000 and add it to COUNTYA. This is the easiest option, and then the first 1 or 2 digits represents the state, and the last 3 digits represent the county.\nOnce we’ve created the fips code, we can merge the uninsured rate into the “Life Exp 2010” sheet using either INDEX(MATCH) or SUMIFS.\n\n\n\n\n\n\nWhy can’t we just copy and paste?\n\n\n\n\n\nClick on the first life expectancy value and hit “CTRL+down arrow” to jump to the bottom. How many rows are there? Do the same for the uninsured rate. The number of rows isn’t the same! The insured population is only available for counties with more than 20,000 people, so there is missing data. This is why we can’t just copy and paste. This also means that you will see some rows with “#N/A” values after you merge.\n\n\n\nLast, add the region. Use INDEX(MATCH) to get region from the “Region” sheet.\n\n\nStep 8: Remove rows with missing data\nWhen data is missing, there are several things you can do – find replacement data, make educated guesses about what the right numbers are, or deleting rows. For this project, we will delete rows with missing data.\nBecause there are so many INDEX(MATCH) and SUMIFS, sorting screws things up. Change the spreadsheet so it has static data rather than formulas. Copy the columns and paste them in the same place by right clicking and selecting the Paste clipboard with “123” in the bottom corner. This will paste the values as values rather than a formula.\nSort the data by the uninsured rate by going to the “Data” tab in the ribbon and hitting the “Sort” button. Sort by the uninsured rate.\nYour data will either have zeroes or #N/As, and we want to delete these rows. Select row 2 by clicking the “2” on the left side of the window. Then, scroll down to the last row with missing data, and click the number next to that row while holding the Shift key. You can delete the data by right clicking and selecting “Delete” or by hitting “Ctrl+-” (or “Command+-” for Macs). You should delete over a thousand rows.\n\n\nWoohoo!\nThe data is now prepared to do the analysis! SAVE YOUR FILE because we will use this data in the future."
  },
  {
    "objectID": "02_assignments/lab-02.html#resources",
    "href": "02_assignments/lab-02.html#resources",
    "title": "Lab 2: Managing Data in Excel",
    "section": "Resources",
    "text": "Resources\nIf you need more help with absolute or mixed cell references, you can check out these resources:\nRelative and Absolute Cell References Video\nRelative and Absolute Cell References Description\nSUMIFS help"
  }
]