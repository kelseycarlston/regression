[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 355: Regression Analysis",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is the center for online portions of Regression 355, taught by Kelsey Carlston at Gonzaga University."
  },
  {
    "objectID": "02_assignments/lab-03.html",
    "href": "02_assignments/lab-03.html",
    "title": "Lab 3: Introduction to R",
    "section": "",
    "text": "In this lab, you will learn\n\nhow to use RStudio\nhow to make a plot with R\nhow to do math in R, create objects, use functions, etc.,\nins and outs of a typical workflow in R\n\n\n\n\nNo additional packages required this week.\n\n\n\n\nhouse prices\ncounty elections\n\n\n\n\nFor this assignment, the only thing you will turn in on Canvas is a brief write up detailed at the end of the assignment. You will use R to create some statistics based on a new dataset."
  },
  {
    "objectID": "02_assignments/lab-03.html#working-in-rstudio",
    "href": "02_assignments/lab-03.html#working-in-rstudio",
    "title": "Lab 3: Introduction to R",
    "section": "Working in RStudio",
    "text": "Working in RStudio\n\n\nIf you are going to do anything with R, RStudio is hands-down the best place to do it. RStudio is an open-source integrated development environment (or IDE) that makes programming in R simpler, more efficient, and most importantly, more reproducible. Some of its more user-friendly features are syntax highlighting (it displays code in different colors depending on what it is or does, which makes it easier for you to navigate the code that you’ve written), code completion (it will try to guess what code you are attempting to write and write it for you), and keyboard shortcuts for the more repetitive tasks.\n\nPane layout\nWhen you first open RStudio, you should see three window panes: the Console, the Environment, and the Viewer. If you open an R script, a fourth Source pane will also open. The default layout of these panes is shown in the figure above.\n\nSource. The Source pane provides basic text editing functionality, allowing you to create and edit R scripts. Importantly, you cannot execute the code in these scripts directly, but you can save the scripts that you write as simple text files. A dead give away that you have an R script living on your computer is the .R extension, for example, my_script.R.\n\nConsole. The Console pane, as its name suggests, provides an interface to the R console, which is where your code actually gets run. While you can type R code directly into the console, you can’t save the R code you write there into an R script like you can with the Source editor. That means you should reserve the console for non-essential tasks, meaning tasks that are not required to replicate your results.\nEnvironment. The Environment pane is sort of like a census of your digital zoo, providing a list of its denizens, i.e., the objects that you have created during your session. This pane also has the History tab, which shows the R code you have sent to the console in the order that you sent it.\n\nViewer. The Viewer pane is a bit of a catch-all, including a Files tab, a Plots tab, a Help tab, and a Viewer tab.\n\nThe Files tab works like a file explorer. You can use it to navigate through folders and directories. By default, it is set to your working directory.\nThe Plots tab displays any figures you make with R.\nThe Help tab is where you can go to find helpful R documentation, including function pages and vignettes.\n\n\nLet’s try out a few bits of code just to give you a sense of the difference between Source and Console.\n\nAs you work through this lab, you can practice running code in the Console, but make sure to do the actual exercises in an R script.\n\n\n\nExercises\n\nFirst, let’s open a new R script. To open an R script in RStudio, just click File > New File > R Script (or hit Ctrl + Shift + N, Cmd + Shift + N on Mac OS).\nCopy this code into the console and hit Enter.\n\n\nrep(\"Boba Fett\", 5)\n\n\nNow, copy that code into the R script you just opened and hit Enter again. As you see, the code does not run. Instead, the cursor moves down to the next line. To actually run the code, put the cursor back on the line with the code, and hit Ctrl + Enter (CMD + Enter on Mac OS)."
  },
  {
    "objectID": "02_assignments/lab-03.html#setting-up-your-r-script",
    "href": "02_assignments/lab-03.html#setting-up-your-r-script",
    "title": "Lab 3: Introduction to R",
    "section": "Setting up your R script",
    "text": "Setting up your R script\nWorking from an R script rather than the console is often preferred for several key reasons:\n\nReproducibility: An R script allows you to save all the code you write. This means you can easily rerun analyses, share your work, or troubleshoot without retyping commands. The console doesn’t keep a persistent record of the work unless you manually save it.\nEfficiency: You can run large chunks of code or the entire script at once from an R script. In contrast, the console requires you to run each command one by one, which can slow you down, especially when working on complex analyses.\nError Tracking and Debugging: R scripts make it easier to track where things go wrong. You can add comments and sections, making the code easier to read and understand. If something fails, you can simply tweak a few lines and rerun without needing to rewrite everything.\nVersion Control: Scripts can be saved with comments about changes, making it easier to keep track of different versions of an analysis. The console provides no such history, meaning it’s harder to track revisions or recover earlier work.\nDocumentation and Collaboration: Writing code in an R script allows you to add detailed comments to explain your reasoning and the steps involved. This is especially useful when collaborating with others or when you return to a project after some time.\nOrganization: An R script enables you to organize your code logically, group tasks together, and maintain an overview of the entire workflow. The console can feel disorganized with scattered commands and no easy way to structure the work.\n\nFor students, especially those learning R, it’s a good habit to get comfortable with scripts early on. When you start a new assignment or project, you should create a header that includes a description of the code in the comments. To create comments (lines in the code that do not run), use the pound sign (#).\n\n#### Lab 3: Introduction to R ####\n# This code will use some basic functions in R and create a plot"
  },
  {
    "objectID": "02_assignments/lab-03.html#load-in-some-data",
    "href": "02_assignments/lab-03.html#load-in-some-data",
    "title": "Lab 3: Introduction to R",
    "section": "Load in some data",
    "text": "Load in some data\nWe will load in the data “house_prices.csv”, which is posted on Canvas. Save the data into a folder, preferably the same folder as where you are saving your R-script.\n\nSet the working directory\nYou need to tell R what file folder you will be working out of. You can do this in two ways: 1. Go to the “Files” tab in the Viewer pane. Navigate to your folder, then hit the “More” icon, then select “Set As Working Directory”. Since you will want anybody who runs your code to set their working directory, copy the code from the console into your\n2. Type the following code, but instead of your_file_path type the actual folder. Make sure your slashes are forward slashes ( / ) and not back slashes ( \\ ).\n\nsetwd(\"your_file_path\")\n\nFor example, in my script I will type the following, since that’s the folder I’m keeping my data in.\n\nsetwd(\"G:/My Drive/Classes/Regression Analysis\")\n\n\n\nLoad in the data\nNow that R knows where we have the data stored, we can load the data in.\n\nhouse <- read.csv(\"data/house_prices.csv\")\n\nNote that I have my data in a subfolder of my working directory called “data”. Your code may look more like this:\n\nhouse <- read.csv(\"house_prices.csv\")\n\nThis tells R to read in the csv and call the data frame “house”. You should see the data pop up in your environment. You can click on the word “house” and it will open up the data in the data viewer for you to explore. Or, if you just want a quick look, you can hit the little blue circle with the arrow next to the word “house”.\n\n\nWe can also ask R to print the first few lines of a dataframe to show us what it looks like. In this case, each row represents a house.\n\nhead(house, n = 5) # The 5 tells R to print 5 rows\n\n   Price Living.Area Bathrooms Age Fireplace Bedrooms\n1 142212        1982         1 133         0        3\n2 134865        1676         2  14         1        3\n3 118007        1694         2  15         1        3\n4 138297        1800         1  49         1        2\n5 129470        2088         1  29         1        3\n\n\nThe Price is how much the house sold for. The Living.Area is the number of square feet the house had. We also have the number of Bathrooms and Bedrooms. Finally, we have the Age of the house and whether it has a Fireplace.\n\n\nCreate some descriptive statistics\nR can work as a calculator. Let’s try a few simple exercises. What is the mean and standard deviation of house price? Note that we have to tell R what data frame we’re getting the variable from (in this case “house”), then put a $ to let R know we’re retrieving an element of the data frame (in this case “Price”).\n\nmean(house$Price)\n\n[1] 167901.9\n\nsd(house$Price)\n\n[1] 77158.35"
  },
  {
    "objectID": "02_assignments/lab-03.html#make-your-first-plot",
    "href": "02_assignments/lab-03.html#make-your-first-plot",
    "title": "Lab 3: Introduction to R",
    "section": "Make Your First Plot!",
    "text": "Make Your First Plot!\nTo ease you into working with R, let’s visualize some data to answer a simple question: Is the price of a house related to its square footage? Don’t worry about understanding all of this! It’s just to give you a feel for the sort of graphics you can make with R. We’ll spend a future lab learning how to make even better graphics.\n\nThe plot() function\nThe base R graphics package provides a generic function for plotting, which - as you might have guessed - is called plot(). (“Base R” means it’s automatically loaded and you don’t have to install it.) To see how it works, try running this code:\n\nplot(house$Living.Area, house$Price)\n\n\n\n\n\n\nCustomizing your plot\nWith the plot() function, you can do a lot of customization to the resulting graphic. For instance, you can modify all of the following:\n\npch will change the point type,\nmain will change the main plot title,\nxlab and ylab will change the x and y axis labels,\ncex will change the size of shapes within the plot region,\npch will change the type of point used (you can use triangles, squares, or diamonds, among others),\ncol changes the color of the point (or its border), and\nbg changes the color of the point fill (depending on the type of point it is)\n\nFor instance, try running this code:\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\n\n\n\n\nExercises\n\nComplete the following line of code to preview only the first three rows of the house table.\n\n\nhead(house, n = )\n\n\nModify the code below to change the size (cex) of the points from 2 to 1.5.\n\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\nWhat does this plot tell us about the relationship between house size and price? Is it positive or negative? Or is there no relationship at all? If there is a relationship, what might explain it?\nComplete the code below to add “Scatter Plot of House Size and Price” as the main title.\n\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 1,\n  main = \n)\n\n\nComplete the code below to add “House size (sq. ft.)” as the x-axis label and “Price ($)” as the y-axis label.\n\n\nplot(\n  house$Living.Area,\n  house$Price,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2,\n  main = \"Scatter Plot of House Size and Price\",\n  xlab = ,\n  ylab = \n)"
  },
  {
    "objectID": "02_assignments/lab-03.html#r-basics",
    "href": "02_assignments/lab-03.html#r-basics",
    "title": "Lab 3: Introduction to R",
    "section": "R Basics",
    "text": "R Basics\n\nR is a calculator\nYou can just do math with it:\n\n300 * (2/25)\n\n[1] 24\n\n3^2 + 42\n\n[1] 51\n\nsin(17)\n\n[1] -0.9613975\n\n\n\n\nObjects and Functions\nBut, R is more than just a calculator. There are a lot of things you can make with R, and a lot of things you can do with it. The things that you make are called objects, and the things that you do with objects are called functions. Any complex statistical operation you want to conduct in R will almost certainly involve the use of one or more functions.\n\nCalling functions\nTo use a function, we call it like this:\n\nfunction_name(arg1 = value1, arg2 = value2, ...)\n\nTry calling the seq() function.\n\nseq(from = 1, to = 5)\n\n[1] 1 2 3 4 5\n\n\nAs you can see, this generates a sequence of numbers starting at 1 and ending at 5. There are two things to note about this. First, we do not have to specify the arguments explicitly, but they must be in the correct order:\n\nseq(1, 5) \n\n[1] 1 2 3 4 5\n\nseq(5, 1)\n\n[1] 5 4 3 2 1\n\n\nSecond, the seq() function has additional arguments you can specify, like by and length. While we do not have to specify these because they have default values, you can change one or the other (but not at the same time!):\n\nseq(1, 10, by = 2)\n\n[1] 1 3 5 7 9\n\nseq(1, 10, length = 3)\n\n[1]  1.0  5.5 10.0\n\n\n\n\nCreating objects\nTo make an object in R, you use the arrow, <-, like so:\n\nobject_name <- value\n\nTry creating an object with value 5.137 and assigning it to the name bob, like this:\n\nbob <- 5.137\n\nThere are three things to note here. First, names in R must start with a letter and can only contain letters, numbers, underscores, and periods.\n\n# Good\nwinter_solder <- \"Buckey\"\nobject4 <- 23.2\n\n# Bad\nwinter soldier <- \"Buckey\" # spaces not allowed\n4object <- 23.2            # cannot start with a number\n\nSecond, when you create an object with <-, it ends up in your workspace or environment (you can see it in the RStudio environment pane). Finally, it is worth noting that the advantage of creating objects is that we can take the output of one function and pass it to another.\n\nx <- seq(1, 5, length = 3)\n\nlogx <- log(x)\n\nexp(logx)\n\n[1] 1 3 5\n\n\n\n\n\nExercises\n\nUse seq() to generate a sequence of numbers from 3 to 12.\nUse seq() to generate a sequence of numbers from 3 to 12 with length 25.\nWhy doesn’t this code work?\n\n\nseq(1, 5, by = 2, length = 10)\n\n\nUse <- to create an object with value 25 and assign it to a name of your choice.\nNow try to create another object with a different value and name.\nWhat is wrong with this code?\n\n\n2bob <- 10"
  },
  {
    "objectID": "02_assignments/lab-03.html#assignment",
    "href": "02_assignments/lab-03.html#assignment",
    "title": "Lab 3: Introduction to R",
    "section": "Assignment",
    "text": "Assignment\nNow it’s time to work on your own. Download the “County_Election.csv” data set from Canvas and put it in your working directory. Read in the data using read.csv(). Then, write a paragraph giving some information on the data that you find interesting. Include at least 3 statistics and one graph. Be sure to interpret what you think the significance of the statistic is.\nHere is a table describing the variables in the data set:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nstate\nState FIPS Code\n\n\ncounty\nCounty FIPS Code\n\n\ncounty_name\nCounty Name\n\n\nstate_name\nState Name\n\n\nfips\nCombined FIPS Code\n\n\npct_republican_2016\nPercent of voters in county that voted Republican in 2016 presidential election\n\n\nfrac_coll_plus2010\nPercent of adults 25 years or older who have a 4-year college degree or more in 2010\n\n\nforeign_share2010\nNumber of foreign born residents in the 2010 Census divided by the sum of native and foreign born residents.\n\n\nmed_hhinc2016\nMedian household income in 2016\n\n\npoor_share2010\nShare of families with incomes under the poverty line in 2010\n\n\nshare_black2010\nShare of people who are Black in 2010\n\n\nshare_hisp2010\nShare of people who are Hispanic in 2010\n\n\nshare_asian2010\nShare of people who are Asian in 2010\n\n\nrent_twobed2015\nThe median gross rent for renter-occupied housing units with two\n\n\npopdensity2010\nNumber of residents per square mile in 2010\n\n\nann_avg_job_growth_2004_2013\nAverage annualized job growth rate over the time period 2004 to 2013\n\n\n\nYou may want to do different statistics from what we did above. Here are some functions you can use. To get information about them, type a question mark followed by the function you are looking up into the console. Alternatively, look the function up online.\n\nStatistics\n\n\n\nStatistic\nFunction\n\n\n\n\nMinimum:\nmin()\n\n\nMaximum:\nmax()\n\n\nAverage:\nmean()\n\n\nStandard Deviation:\nsd()\n\n\nMedian:\nmedian()\n\n\nPercentiles:\nquantile()\n\n\nCorrelation Coefficient:\ncor()\n\n\nFrequency tables:\ntable()\n\n\nRelative Frequency tables:\nprop.table()\n\n\n\nNote that in frequency tables you can use more than one variable!\n\n\nGraphics\n\n\n\nPlot\nFunction\n\n\n\n\nBar chart:\nbarplot()\n\n\nHistogram:\nhist()\n\n\nBox plots:\nboxplot()\n\n\nScatter plot:\nplot()"
  },
  {
    "objectID": "02_assignments/lab-04.html#outline",
    "href": "02_assignments/lab-04.html#outline",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Outline",
    "text": "Outline\n\nObjectives\nIn this lab, you will learn how to:\n\nload R packages with library()\nsubset data\nmerge data\nvisualize data with the grammar of graphics and ggplot()\n\naesthetic mappings\ngeometric objects\nfacets\nscales\nthemes\n\n\n\n\nR Packages\nggplot2\nviridis\n\n\nData\n\nhouse prices\ncounty elections\n\n\n\nGrade\nFor this assignment, you will complete a quiz as you work through the lab."
  },
  {
    "objectID": "02_assignments/lab-04.html#the-library",
    "href": "02_assignments/lab-04.html#the-library",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "The Library",
    "text": "The Library\nR is an extensible programming language, meaning you can write R code to extend the functionality of base R. To share that code, R users will often bundle it into a package, a collection of functions, data, and documentation. You can think of packages as apps, but apps specifically designed for R. To make the functionality a package offers available in R, you have to load them in with the library() function (the technical term is attach).\nYou should always, always, always load all the packages you use at the beginning of a document. That way, people who read your code know exactly what packages you are using all at once and right away. To make this really, really explicit, I prefer to put it with other front matter that I call the “R Preamble.” In an R Script, it looks like this:\n\n### Lab 4: More Advanced R ###\n# This code manipulates data and creates graphs using ggplot\n\nsetwd(\"G:/My Drive/Teaching/regression/02_assignments\")\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nOf course, these aren’t just automatically on your computer, so you have to install the packages first. Then you can open them in R. To do that, you use the function install.packages(). For the packages used today, you can use this call just once like so:\n\ninstall.packages(\n  c(\"ggplot2\", \"viridis\")\n)\n\nNote that you only need to run this once, so don’t put this as a line in your R script document, which you might render multiple times. Just run it in the console.\n\nExercises\nOpen a new R script and add the R Preamble with an R code chunk with the library calls that load the R packages required for this lab. Now actually run each library() call. You can do that by either highlighting them and hitting Ctrl + Enter (Cmd + Enter) or by clicking the run button. If you don’t highlight, R will run the code from the line your cursor is on."
  },
  {
    "objectID": "02_assignments/lab-04.html#load-in-the-data",
    "href": "02_assignments/lab-04.html#load-in-the-data",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Load in the data",
    "text": "Load in the data\nYou should have already set up your working directory at the top of the sheet. In that folder, you should have the two datasets we will be using in this lab: “NBA_Data.csv” and “Five38_Data.csv”, which you can find on Canvas. Load them in using the read.csv() function.\n\nNBA <- read.csv(\"data/NBA_Data.csv\")\nFive38 <- read.csv(\"data/Five38_Data.csv\")\n\nIf you look in your environment, you should have two data frames. Explore the data a little bit using the functions head(NBA) and summary(NBA). You can use the same functions for the Five38 dataset.\nThe NBA dataset has statistics for all 30 NBA teams for the 2022-2023 season (updated to 2/13/2023), including wins, losses, total points, free throws, assists, turnovers, etc. It also includes whether the team was in the playoffs last year.\nThe Five38 dataset has the conference each team is in, plus the Five38 Elo predictions for whether the team will make playoffs, make finals, and win finals. It also includes the number of times each team has been in the playoffs since 1946.\n\nMake a new column\nYou may notice that the NBA dataset has wins and losses, but it doesn’t have the win percent. We will make that ourselves! To create a new column, we just have to tell R what to do. Remember that we have to tell R which data frame we are using, then put a dollar sign to say we are accessing a specific variable.\n\nNBA$win_pct = NBA$Wins/NBA$Games_Played\n\nEasy! We can explore this variable a little bit by typing:\n\nsummary(NBA$win_pct)\n\nNice! Now we know that the median team lost about 50 percent of its games. Even the best team only won 72 percent.\n\n\n\n\n\nExercises\n\nMake a new column that shows the average points per game, which is points (NBA$Points) divided by the total games (NBA$Games_Played).\nFind the mean and median of points scored per game.\nUse the cor() function to find the correlation between points per game and the win percent."
  },
  {
    "objectID": "02_assignments/lab-04.html#merging-data",
    "href": "02_assignments/lab-04.html#merging-data",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Merging data",
    "text": "Merging data\nTo merge data, we need a unique identifier that matches between the two datasets, just like in any program. Take a look at the team names. What’s the problem?\nThe team names in the Five38 dataset are split to place and name. We can combine those columns using the following code:\n\nFive38$Team <- paste(Five38$Place, Five38$Name, sep = \" \")\n\nThe paste function tells R that we are combining two strings. The sep = \" \" option tells R that we want a space between the two strings, but we could put a comma or no space if we wanted.\nNow we have a unique ID, and we can use it to merge our datasets. We will create a new dataset called BBall with our merged data.\n\nBBall <- merge(NBA, Five38, by = \"Team\")\n\nThis new dataset has the same 30 teams, but 31 variables rather than 7 or 25. (It’s not 7 + 25 = 32 because R doesn’t repeat the identifier.)"
  },
  {
    "objectID": "02_assignments/lab-04.html#subsetting-data",
    "href": "02_assignments/lab-04.html#subsetting-data",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Subsetting Data",
    "text": "Subsetting Data\nA common thing to do with data is subset the data into a smaller group of data. For instance, we might want to remove some columns if there are columns we don’t need. Or, we might want to do some calculations on only teams in the Western Conference. Or, maybe we want to do calculations on the teams that have over 50% win percent.\nIf we are subsetting the whole dataframe, we put square brackets next to the name of the dataframe and give it the [rows, columns]. For example, if we want the second row of the first column, we would type BBall[2,1]. If we want the entire 4th column, we would put BBall[,4]. And if we want the entire 8th row, we would write BBall[8,].\nWe can also subset a range of rows or columns. To get the first 5 columns, we would type BBall[,1:5].\nTo subset by the conference, we would use two equals signs to tell R that we are checking for equivalency. (One equals sign means we are assigning the value.) Let’s make a new dataframe that only includes the Western conference.\n\nwest <- BBall[BBall$Conference == \"West\",]\n\nNow, we could do whatever we wanted on the new “west” dataframe.\nWe don’t have to make a new dataframe to use functions. For example, let’s say we want the average number of rebounds for teams that have over a 50% win rate vs under or equal to a 50% win rate. We are going to call the mean function on rebounds. Since we are targeting just one column, we only have to put the row argument into the square brackets.\n\nmean(BBall$Rebounds[BBall$win_pct > 0.5])\nmean(BBall$Rebounds[BBall$win_pct <= 0.5])\n\nGreat! Now, let’s subset to find the mean number of turnovers for teams that are in the Eastern Conference and have at least 113 points per game. We will use the ampersand (&) to tell R that we want both conditions to be true to include it. We can compare that to the number of turnovers for teams in the Eastern Conference that have less than 113 points per game.\n\nmean(BBall$Turnovers[BBall$Conference == \"East\" & BBall$points_per_game >= 113])\nmean(BBall$Turnovers[BBall$Conference == \"East\" & BBall$points_per_game < 113])\n\nWe can also get columns by name. Let’s make a a new dataset that just includes the team name and our calculated columns.\n\nnew = BBall[,c(\"Team\", \"win_pct\", \"points_per_game\")]\n\nOne thing we might want to know is what the name of the maximum value is. We can use the function which.max(), which gives the position/row number of the biggest value. Then we can return that row number. For example, if we want to know which team has the biggest win percent we would use this code:\n\nBBall$Team[which.max(BBall$win_pct)]\n\n\nExercise\n\nWhat is the value in the 3rd row of the 9th column of the BBall dataset?\nWhat is the mean win percentage in the Western Conference?\nWhat is the average free throw percent for teams in the Western conference that have at least 1120 personal fouls?\nWhich team has been to the playoffs the most times?"
  },
  {
    "objectID": "02_assignments/lab-04.html#graphics",
    "href": "02_assignments/lab-04.html#graphics",
    "title": "Lab 4: ggplot and Data Manipulation",
    "section": "Graphics",
    "text": "Graphics\n\nThe Grammar of Graphics\nIt’s easy to imagine how you would go about with pen and paper drawing a bar chart of the number of games played in each conference. But, what if you had to dictate the steps to make that graph to another person, one you can’t see or physically interact with? All you can do is use words to communicate the graphic you want. How would you do it? The challenge here is that you and your illustrator must share a coherent vocabulary for describing graphics. That way you can unambiguously communicate your intent. That’s essentially what the grammar of graphics is, a language with a set of rules (a grammar) for specifying each component of a graphic.\nNow, if you squint just right, you can see that R has a sort of grammar built-in with the base graphics package. To visualize data, it provides the default plot() function, which you learned about in the last lab. This is a workhorse function in R that will give you a decent visualization of your data fast, with minimal effort. It does have its limitations though. For starters, the default settings are, shall we say, less than appealing. I mean, they’re fine if late-nineties styles are your thing, but less than satisfying if a more modern look is what you’re after. Second, taking fine-grained control over graphics generated with plot() can be quite frustrating, especially when you want to have a faceted figure (a figure with multiple plot panels).\nThat’s where the ggplot2 package comes in. It provides an elegant implementation of the grammar of graphics, one with more modern aesthetics and with a more standardized framework for fine-tuning figures, so that’s what we’ll be using here. From time to time, I’ll try to give you examples of how to do things with the plot() function, too, so you can speak sensibly to the die-hard holdouts, but we’re going to focus on learning ggplot.\n\n\n\n\n\n\nResources for ggplot2\n\n\n\nThere are several excellent sources of additional information on statistical graphics in R and statistical graphics in general that I would recommend.\n\nThe website for the ggplot2 package. This has loads of articles and references that will answer just about any question you might have.\nThe R graph gallery website. This has straightforward examples of how to make all sorts of different plot visualizations, both with base R and ggplot.\nClaus Wilke’s free, online book Fundamentals of Data Visualization, which provides high-level rules or guidelines for generating statistical graphics in a way that clearly communicates its meaning or intent and is visually appealing.\nThe free, online book ggplot2: Elegant Graphics for Data Analysis (3ed) by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen. This is a more a deep dive into the grammar of graphics than a cookbook, but it also has lots of examples of making figures with ggplot2.\n\n\n\nSo, to continue our analogy above, we’re going to treat R like our illustrator, and ggplot2 is the language we are going to speak to R to visualize our data. So, how do we do that? Well, let’s start with the basics. Suppose we want to know if there’s some kind of relationship between the field goal percent and the probability of making playoffs. Here’s how we would visualize that.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs)\n  )\n\nHere, we have created a scatterplot, a representation of the raw data as points on a Cartesian grid. There are several things to note about the code used to generate this plot.\n\nIt begins with a call to the ggplot() function. This takes a data argument. In this case, we say that we want to make a plot to visualize the basketball data.\nThe next function call is geom_point(). This is a way of specifying the geometry we want to plot. Here we chose points, but we could have used another choice (lines, for example, or polygons).\nThe geom_point() call takes a mapping argument. You use this to specify how variables in your data are mapped to properties of the graphic. Here, we chose to map the Field_Goal_Pct variable to the x-coordinates and the Making_Playoffs variable to the y-coordinates. Importantly, we use the aes() function to supply an aesthetic to the mapping parameter. This is always the case.\nThe labs() function allows us to specific axis lables and titles. In this instance, I’m only using it to create alternative text for accessibility.\nThe final thing to point out here is that we combined or connected these arguments using the plus-sign, +. You should read this literally as addition, as in “make this ggplot of the basketball data and add a point geometry to it.” Be aware that the use of the plus-sign in this way is unique to the ggplot2 package and won’t work with other graphical tools in R.\n\nWe can summarize these ideas with a simple template. All that is required to make a graph in R is to replace the elements in the bracketed sections with a dataset, a geometry function, and an aesthetic mapping.\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\nOne of the great things about ggplot, something that makes it stand out compared to alternative graphics engines in R, is that you can assign plots to a variable and call it in different places, or modify it as needed.\n\nplayoffs_plot <- ggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs)\n  )\n\nplayoffs_plot\n\n\n\nExercises\n\nRecreate the scatterplot above, but switch the axes. Put win percent on the x axis and times in playoffs on the y axis.\nNow create a scatterplot of win percent on the x axis and probability of making playoffs (Making_Playoffs) on the y axis.\n\n\n\nAesthetics\nIn the plot above, we only specified the position of the points (the x- and y-coordinates) in the aesthetic mapping, but there are many aesthetics (see the figure below), and we can map the same or other variables to those.\n\n\nConsider, for example, that we have two groups of teams: those that went to the playoffs last year and those that had no post season. Do we think that the relationship between the field goal percent and probability of making playoffs are the same for both? Let’s add conference to our aesthetic mapping (specifically to the color parameter) and see what happens.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\nNotice that ggplot2 automatically assigns a unique color to each category and adds a legend to the right that explains each color. In this way, the color doesn’t just change the look of the figure. It conveys information about the data. Rather than mapping a variable in the data to a specific aesthetic, though, we can also define an aesthetic manually for the geometry as a whole. In this case, the aesthetics do not convey information about the data. They merely change the look of the figure. The key to doing this is to move the specification outside the aes(), but still inside the geom_point() function.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs),\n    shape = 21,\n    size = 4,\n    color = \"darkred\",\n    fill = \"darkgoldenrod1\"\n  )\n\nNotice that we specified the shape with a number. R has 25 built-in shapes that you can specify with a number, as shown in the figure below. Some important differences in these shapes concern the border and fill colors. The hollow shapes (0-14) have a border that you specify with color, the solid shapes (15-20) have a border and fill, both specified with color, and the filled shapes (21-24) have separate border and fill colors, specified with color and fill respectively.\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you can use hexadecimal codes like #004F2D instead of “forestgreen” to specify a color. This also allows you to specify a much wider range of colors. See this color picker website for one way of exploring colors.\n\n\n\n\nExercises\n\nChange the code below to map the Last_Year variable to the x-axis (in addition to the color).\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\n\nWhat does this do to the position of the points?\nChange the code below to map the Last_Year variable to the shape aesthetic (in addition to the color).\n\n\n# hint: use shape = ...\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\n\nChange the code below to map the Last_Year variable to the size aesthetic (replacing color).\n\n\n# hint: use size = ...\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  )\n\n\nFor the following code, change the color, size, and shape aesthetics for the entire geometry (do not map them to the data).\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs),\n    color = , # <------- insert value here\n    size = ,  # <------- \n    shape =   # <------- \n  )\n\n\n\nGeometries\nHave a look at these two plots.\n\n\n\nBoth represent the same data and the same x and y variables, but they do so in very different ways. That difference concerns their different geometries. As the name suggests, these are geometrical objects used to represent the data. To change the geometry, simply change the geom_*() function. For example, to create the plots above, use the geom_point() and geom_smooth() functions.\n\nggplot(data = BBall) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  )\n\nggplot(data = BBall) +\n  geom_smooth(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  )\n\nWhile every geometry function takes a mapping argument, not every aesthetic works (or is needed) for every geometry. For example, there’s no shape aesthetic for lines, but there is a linetype. Conversely, points have a shape, but not a linetype.\n\nggplot(data = BBall) + \n  geom_smooth(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct, linetype = Last_Year),\n  )\n\nOne really important thing to note here is that you can add multiple geometries to the same plot to represent the same data. Simply add them together with +.\n\nggplot(data = BBall) +\n  geom_smooth(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct, linetype = Last_Year)\n  ) +\n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct, color = Last_Year)\n  ) \n\nThat’s a hideous figure, though it should get the point across. While layering in this way is a really powerful tool for visualizing data, it does have one important drawback. Namely, it violates the DRY principle (Don’t Repeat Yourself), as it specifies the x and y variables twice. This makes it harder to make changes, forcing you to edit the same aesthetic parameters in multiple locations. To avoid this, ggplot2 allows you to specify a common set of aesthetic mappings in the ggplot() function itself. These will then apply globally to all the geometries in the figure.\n\nggplot(\n  data = BBall,\n  mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n) +\n  geom_smooth(mapping = aes(linetype = Last_Year)) +\n  geom_point(mapping = aes(color = Last_Year))\n\nNotice that you can still specify specific aesthetic mappings in each geometry function. These will apply only locally to that specific geometry rather than globally to all geometries in the plot. In the same way, you can specify different data for each geometry.\n\nggplot(\n  data = BBall,\n  mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n) +\n  geom_smooth(data = BBall[BBall$Last_Year == \"Playoffs\",]) +\n  geom_point(mapping = aes(color = Last_Year))\n\nSome of the more important geometries you are likely to use include:\n\ngeom_point()\ngeom_line()\ngeom_segment()\ngeom_polygon()\ngeom_boxplot()\ngeom_histogram()\ngeom_density()\n\nWe’ll actually cover those last three in the section on plotting distributions. For a complete list of available geometries, see the layers section of the ggplot2 website reference page.\n\n\nScales\nScales provide the basic structure that determines how data values get mapped to visual properties in a graph. The most obvious example is the axes because these determine where things will be located in the graph, but color scales are also important if you want your figure to provide additional information about your data. Here, we will briefly cover two aspects of scales that you will often want to change: axis labels and color palettes, in particular palettes that are colorblind safe.\n\nLabels\nBy default, ggplot2 uses the names of the variables in the data to label the axes. This, however, can lead to poor graphics as naming conventions in R are not the same as those you might want to use to visualize your data. Fortunately, ggplot2 provides tools for renaming the axis and plot titles. The one you are likely to use most often is probably the labs() function. Here is a standard usage:\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  )\n\n\n\nColor Pallettes\nWhen you map a variable to an aesthetic property, ggplot2 will supply a default color palette. This is fine if you are just wanting to explore the data yourself, but when it comes to publication-ready graphics, you should be a little more thoughtful. The main reason for this is that you want to make sure your graphics are accessible. For instance, the default ggplot2 color palette is not actually colorblind safe. To address this shortcoming, you can specify colorblind safe color palettes using the scale_color_viridis() function from the viridis package. It works like this:\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Last_Year)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  scale_color_viridis(\n    option = \"viridis\", \n    discrete = TRUE\n  )\n\nI know it doesn’t look good like it is, but when you are working with more categories or different types of graphs, like maps or stacked bar charts, these kinds of pallettes are very useful.\nHere are the color palettes available in the viridis package: {fig-alt = “The colors available in the viridis package.”}\n\n\n\nExercises\n\nUsing the BBall dataset, plot Steals (y variable) by Personal_Fouls (x variable) and change the axis labels to reflect this.\nUsing the code below, try out some different colorblind safe palettes from the viridis package.\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Making_Playoffs, color = Conference)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Probability of Making Playoffs\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  scale_color_viridis(\n    option = \"viridis\", #<------- change value here \n    discrete = TRUE\n  )\n\n\nTry adding a numeric variable, like win_pct, as the color input. (Make sure to change discrete = TRUE to discrete = FALSE.)\n\n\n\nThemes\nTo control the display of non-data elements in a figure, you can specify a theme. This is done with the theme() function. Using this can get pretty complicated, pretty quick, as there are many many elements of a figure that can be modified, so rather than elaborate on it in detail, I want to draw your attention to pre-defined themes that you can use to modify your plots in a consistent way.\nHere is an example of the black and white theme, which removes filled background grid squares, leaving only the grid lines.\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  theme_bw()\n\n\n\nExercises\n\nComplete the code below, trying out each separate theme: theme_minimal() theme_classic() theme_void()\n\n\nggplot(data = BBall) + \n  geom_point(\n    mapping = aes(x = Field_Goal_Pct, y = Three_pt_pct)\n  ) +\n  labs(\n    x = \"Field Goal Percent\",\n    y = \"Three Point Percent\",\n    title = \"NBA 2022-2023 Season\"\n  ) +\n  theme_   # <------- complete function call to change theme"
  },
  {
    "objectID": "02_assignments/intro_to_R_blind.html",
    "href": "02_assignments/intro_to_R_blind.html",
    "title": "Introduction to R for Blind Users",
    "section": "",
    "text": "This assignment is meant to be an introduction to R that includes a few points that will be important for blind users. In particular, this will show you how to run R code, how to: * Create a Markdown document * Implement settings that help with a screen reader * Load the BrailleR package and use some of the basic functions"
  },
  {
    "objectID": "02_assignments/lab-05.html",
    "href": "02_assignments/lab-05.html",
    "title": "Lab 5: dplyr and ggplot2",
    "section": "",
    "text": "In this lab, you will\n\nget more practice merging data in R\nlearn some important functions in the dplyr package\npractice making gorgeous plots in ggplot2\nbe encouraged to use help pages and vignettes to learn how to use functions\n\n\n\n\ndplyr\nggplot2\n\n\n\n\nGDP by country\n\nContains GDP per capita (PPP) for each country from 1990 to 2023\n\nPopulation and life expectancy by country\n\nContains population and life expectancy variables for each country from 1950 to 2023\n\n\n\n\n\nFor this assignment, follow along with the quiz on Canvas."
  },
  {
    "objectID": "02_assignments/lab-05.html#dplyr-and-the-tidyverse",
    "href": "02_assignments/lab-05.html#dplyr-and-the-tidyverse",
    "title": "Lab 5: dplyr and ggplot2",
    "section": "Dplyr and the Tidyverse",
    "text": "Dplyr and the Tidyverse\nThere are several packages that make data manipulation in R much easier than in Excel or many other languages. The tidyverse is a collection of R packages designed for data science. It includes tools for data manipulation, visualization, and analysis. Core packages include dplyr for data wrangling, ggplot2 for visualization, tidyr for data tidying, and readr for data import. The tidyverse is built around the concept of tidy data, where datasets are organized in a consistent way, making analysis more intuitive and efficient.\ndplyr is a package used for data manipulation, providing a range of tools to transform, summarize, and manipulate data frames. It allows for operations like filtering rows (filter()), selecting columns (select()), arranging data (arrange()), creating new variables with functions of existing variables (mutate()), and summarizing data (summarize()). These functions make it easy to work with large datasets using clear, readable syntax.\nWe want to read in the data, then merge it to create a dataset that allows us to compare GDP and life expectancy. First, load the packages and read in the data.\n\nrm(list = ls())\nlibrary(tidyverse)\nlibrary(ggplot2)\npop <- read.csv(\"data/population_by_country.csv\") # The csv file is in my \"data\" folder\ngdp <- read.csv(\"data/gdp_by_country.csv\") # You may not need the \"data/\" part\n\nYou will notice that the population dataset has a lot more rows than the GDP dataset. There are two reasons for that:\n1) Population includes many years which we don’t need\n2) GDP is formatted “wide” rather than “long”\nWe will discuss how to adjust those now.\n\nFilter\nOne of the things we can do with dplyr is remove rows we don’t based on criteria we set. For instance, let’s say we want to subset rows so that the data frame only includes data from 1990 to 2023, the years in the GDP data. We can either use the code we previously learned using base R:\n\npop <- pop[pop$Year > 1989, ]\n\nOr, we can use the easier functionality that is a part of dplyr.\n\npop <- filter(pop, Year > 1989)\n\nUsually, rather than using functions in that way, we use something called the piping operator. In R, the piping operator (|>), provided by the magrittr package (and commonly used in the tidyverse), allows you to chain together a sequence of operations in a more readable and intuitive way. The pipe enables you to pass the output of one function directly into the next function as an input, without the need for nesting functions or creating intermediate variables.\nThe basic syntax is:\n\ndata |> function1() |> function2() |> function3()\n\nWhich is equivalent to:\n\nfunction3(function2(funciont1(data)))\n\nThe pipe operator takes the result of the left-hand side and feeds it as the first argument into the function on the right-hand side. This makes the code flow from left to right, mimicking how we read sentences, which often improves readability, especially when performing multiple operations.\nIn our example, the code would be:\n\npop <- pop |> \n  filter(Year > 1989)\n\n\n\nPivot Longer\nIn the GDP dataset, each column is a different year (wide format), while in the life expectancy dataset, there are rows for each year for each country (long format). Usually, long format data is more helpful, so we will change gdp to be long using pivot_longer().\nThe reference pages for tidyverse functions are usually very good, with many helpful examples. They are often vignettes rather than help pages, which means they have more information and are easier to read. Check out the pivot longer vignette to learn about it.\nWe want to turn every column that starts with “X” into a row, with the year in the column name as a new column. So we want this table:\n\n\n\nCountry\nX2000\nX2001\nX2002\n\n\n\n\nUSA\na\nb\nc\n\n\nCanada\nx\ny\nz\n\n\nMexico\nt\nu\nv\n\n\n\nTo turn into this table:\n\n\n\nCountry\nYear\nValue\n\n\n\n\nUSA\n2000\na\n\n\nUSA\n2001\nb\n\n\nUSA\n2002\nc\n\n\nCanada\n2000\nx\n\n\nCanada\n2001\ny\n\n\nCanada\n2002\nz\n\n\nMexico\n2000\nt\n\n\nMexico\n2001\nu\n\n\nMexico\n2002\nv\n\n\n\nWe will use the piping operator to use the function.\n\ngdp <- gdp |> \n  pivot_longer(\n    cols = starts_with(\"X\"),\n    names_to = \"Year\",\n    names_prefix = \"X\",\n    values_to = \"GDP_per_cap\",\n    values_drop_na = TRUE\n  )\n\n\n\n\n\n\n\nMore help with the pivot longer\n\n\n\nCheck out the video on Canvas for more help with pivot longer. There are also videos on Youtube, like this one.\n\n\nIn order to merge, we need to have a key to merge on so we can merge by a unique country/year. We could make a new variable that has both of those, but R allows us to merge based on two keys. The keys do have to be the same data type, so let’s change the Year variable in the GDP data frame to numeric. Then, we can use merge().\nmutate() is a function that allows us to create new row-wise variables within the dplyr framework. (Note that if you use <- in mutate() or summarize(), the variable names can be weird, so you must use =.)\n\ngdp <- gdp |> \n  mutate(Year = as.numeric(Year))\n\nworld <- merge(gdp, pop, by.x = c(\"Year\", \"Country.Code\"), by.y = c(\"Year\", \"iso3_code\"))\n\nNow we can do the analysis we want!"
  },
  {
    "objectID": "02_assignments/lab-05.html#gapminder-analysis",
    "href": "02_assignments/lab-05.html#gapminder-analysis",
    "title": "Lab 5: dplyr and ggplot2",
    "section": "Gapminder Analysis",
    "text": "Gapminder Analysis\nThe Gapminder project reported national income versus health for all of the world countries. The project had many great visualizations, but they stopped updating data in 2015. We are going to update the graphic with new data. Here is a scatterplot of GDP per capita (PPP) and health (life expectancy at birth):\n\n\nLet’s find out which recent years have data for many countries. We will use group_by() and summarize() to do so. The group_by() function in dplyr is used to group data by one or more variables in a data frame. This is useful when you want to perform operations (like summarizing or mutating) separately for each group, rather than on the entire dataset. Grouping doesn’t change the data itself, but it sets the stage for subsequent operations that will respect the groups.\nI’m going to create a new dataframe that groups data by year, and I’ll get the count and average life expectancy for each. Then, I’ll print the most recent years of data, and I’ll do a line plot of life expectancy over time.\n\nyear_data <- world |> \n  group_by(Year) |> \n  summarize(n_countries = n(),\n            avg_le = mean(as.numeric(life_exp_total)))\n\nyear_data[year_data$Year > 2019,]\n\nggplot(data = year_data) +\n  geom_line(aes(x = Year, y = avg_le)) +\n  xlab(\"Year\") +\n  ylab(\"Average Life Expectancy\") +\n  theme_classic() +\n  labs(alt = \"A line graph with year on the x axis and life expectancy on the y axis. Years range from 1990 to 2023, and the average life expecatancy increases from about 65 to about 73, with a dip around 2020.\") # It's a good idea to add alternative text for visually impaired users!\n\nIt looks like 2021 has almost all of the countries in the sample, so we will use that year.\n\nExercise\n\nChange the data type of life_exp_total and total_pop to numeric using as.numeric() within mutate() like we did with year in the GDP dataset.\nSubset the world data to just include the year 2021 using filter().\nTry grouping by “Continent”, and calculate the average life expectancy and GDP per capita for each continent using summarize().\nCalculate the total population by continent using group_by() and sum() within summarize().\nCreate the variable log_gdp by using log() for GDP_per_cap within mutate().\n\n\n\nCreating the scatterplot\nUse ggplot to create a scatterplot of logged GDP versus life expectancy for 2021. Make sure to do the following:\n\nUse arguments within the aes() mapping function to change the color of the points to reflect the continent, and the size of the point to reflect total population. You may need to use help pages or Google to figure it out.\nAdd x and y labels.\nChange the legend label to have a nicer title by using labs(size = \"Whatever you want your size label to be\").\n\n\n\nExercise\nFind out one more interesting thing about the dataset. Maybe you want to plot one of the other variables, like population density or fertility. Or, maybe you want to do some summary statistics like finding the correlation between infant mortality and life expectancy after age 15. What did you find? And why do you think it is interesting?"
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html",
    "href": "02_assignments/mini_lab-ttests.html",
    "title": "Mini Lab: T Tests in R",
    "section": "",
    "text": "In this short assignment, you will practice hypothesis testing in R. The graded portion of the assignment will only be to answer the questions in the Canvas quiz."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#load-data",
    "href": "02_assignments/mini_lab-ttests.html#load-data",
    "title": "Mini Lab: T Tests in R",
    "section": "Load Data",
    "text": "Load Data\nMost of the time we load data from csv files we have downloaded on our computer. Sometimes, R or R packages will have datasets included, so you do not need to load the data from a file, but instead can use the data() function. We will use a built-in dataset that includes arrest data for each US state in 1973.\nLoad the data with this code:\n\ndata(\"USArrests\")\n\nIf the data does not immediately show up in your environment, it might just need a little more time, or you might need to call the data first with a function. Try head(USArrests).\nThe data has 4 variables for 1973: the arrest rates for murder, assault, and rape, and the percent of each state’s population that is urban."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#hypothesis-testing",
    "href": "02_assignments/mini_lab-ttests.html#hypothesis-testing",
    "title": "Mini Lab: T Tests in R",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nExample\nSuppose I read an article that said that in 1973, the average percent urban in each state was 62 percent. I want to check that with this dataset with \\(\\alpha = 0.05\\). Here is how I would do it in R:\n1) Write down the null and alternative hypotheses.\n\\[\nH_0: \\mu = 62\n\\]\n\\[\nH_A: \\mu \\neq 62\n\\]\n2) Do the hypothesis test. I will use the following function in R.\n\nt.test(mydata$variable, alternative, mu, conf.level)\n\nWhere “mydata” is your data; “variable” is the variable you want to test; “alternative” can take on one of three values: “two.sided”, “greater”, “less”; mu is \\(\\mu\\); and conf.level is \\((1-\\alpha)\\). I’ll plug in my values:\n\nt.test(USArrests$UrbanPop, alternative = \"two.sided\", mu = 62, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  USArrests$UrbanPop\nt = 1.7293, df = 49, p-value = 0.09005\nalternative hypothesis: true mean is not equal to 62\n95 percent confidence interval:\n 61.42632 69.65368\nsample estimates:\nmean of x \n    65.54 \n\n\n3) Conclude. The p-value of 0.09 is greater than our significance level of 0.05. The p-value says that if the null hypothesis were true and the actual mean were 62, the probability we would see the data we do is 9%. That is likely enough that we don’t have enough evidence to reject the null hypothesis. The data do not provide enough evidence to say that the mean of 62 is not true.\n\n\nExercise\nSuppose I heard that in 2021, the arrest rate for assault was 135 per 100,000. I want to see if the average state-level rate 1973 was significantly different from that at the 99% confidence level.\na. Write down the null and alternative hypotheses.\nb. Perform the t test.\nc. Write down your conclusion."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#creating-a-dummy-variable",
    "href": "02_assignments/mini_lab-ttests.html#creating-a-dummy-variable",
    "title": "Mini Lab: T Tests in R",
    "section": "Creating a dummy variable",
    "text": "Creating a dummy variable\n\nExample\nSuppose I want to compare two groups of states: states in the west, and states not in the west. I need to create a dummy variable, which is a variable that equals one if a condition is true, and zero if it is not. The easiest way to do this in R is with ifelse(). The function ifelse() takes 3 arguments: the first is a test on a variable in the dataset, the second is the value the new variable will take if the test is true, and the third is the value the new variable will take if the test is false.\n\nUSArrests$west <- ifelse(rownames(USArrests) %in% c(\"California\", \"Washington\", \"Oregon\", \"Idaho\", \"Utah\", \"Nevada\", \"Arizona\", \"New Mexico\", \"Montana\", \"Wyoming\", \"Colorado\"), 1, 0)\n\nThe row names in this dataset are the name of the state. This code tests whether the row name is one of the states in the list, and if it is, then it assigns a 1 to the new variable west.\n\n\nExercise\nBreak the data into two groups: states with a high urban population, and states with a low urban population. UrbanPop is a variable that says the percent of the state which is urban. The following code tells us some summary statistics:\n\nsummary(USArrests$UrbanPop)\n\nThe median percent urban is 66. Use that as the cutoff for urban versus non-urban. Use ifelse() to create a variable that equals 1 if the UrbanPop variable is greater than 66, and 0 if it is not."
  },
  {
    "objectID": "02_assignments/mini_lab-ttests.html#two-sample-t-tests",
    "href": "02_assignments/mini_lab-ttests.html#two-sample-t-tests",
    "title": "Mini Lab: T Tests in R",
    "section": "Two sample t tests",
    "text": "Two sample t tests\n\nExample\nIn the examples we’ve done so far, we test our sample mean against the mean for a larger population or theoretical distribution. Sometimes, we want to test whether two groups are different based on two different samples. I will test whether states in the west have significantly lower murder rates.\nHypothesis testing for two samples is slightly different, and the calculation for t is slightly different.\n\nWrite down hypotheses. \\[ H_0: \\mu_{west} \\geq \\mu_{non-west} \\] \\[ H_1: \\mu_{west} < \\mu_{non-west} \\] Which is equivalent to: \\[ H_0: \\mu_{west} - \\mu_{non-west} \\leq 0\\] \\[ H_1: \\mu_{west} - \\mu_{non-west} > 0 \\]\nPerform the t test. If I were to be calculating this by hand, the formula would be: \\[ t= \\frac{(\\bar x_{west}-\\bar x_{non-west})-(\\mu_{west}-\\mu_{non-west})}{{\\sqrt{\\frac{s_{west}^2}{n_{west}} + \\frac{s_{non-west}^2}{n_{non-west}} }}}\\] If you are interested, you can calculate this on your own. The degrees of freedom would be \\(df = n_{west} + n_{non-west}-2\\).\n\nI could calculate the mean, standard deviation, and sample size for each group like so:\n\nmean(USArrests$Murder[USArrests$west == 1])\nsd(USArrests$Murder[USArrests$west == 1])\nlength(USArrests$Murder[USArrests$west == 1])\n\nHowever, we can use the t test formula in R as well. The formula for two-sample t tests is:\n\nt.test(data1, data2, alternative, conf.level)\n\nThe data you want to compare are different rows in the same dataframe. You will have to subset your data, either within the formula using brackets, or by making new data frames using brackets or dplyr.\nIn this case, I would use the code:\n\nt.test(USArrests$Murder[USArrests$west == 1],\n       USArrests$Murder[USArrests$west == 0],\n       \"less\", conf.level = 0.95)\n\nI use “less” because I’m doing a one-sided hypothesis test. If I were doing a one-sided hypothesis the other way, I would use “greater”.\n\nWrite down your conclusions. The p-value is 0.1374. This means that there is a 13.74% chance we’d observe the data we do if the null hypothesis were true. This is higher than our significance level of 5%, so we fail to reject the null hypothesis. There is not sufficient evidence to say that murder arrest rates in the west are less than murder arrest rates in non-western states.\n\n\n\nExercise\nTry doing a two sample t-test to see whether more urban states (states with more than the median percent urban, like the dummy variable) have higher murder rates than less-urban states."
  },
  {
    "objectID": "02_assignments/lab-06.html#outline",
    "href": "02_assignments/lab-06.html#outline",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Outline",
    "text": "Outline\n\nObjectives\nIn this lab, you will analyze two stocks, Apple and Disney, to see how their daily returns compare to the market as measured by the NASDAQ. You will learn:\n\nA practical application of simple linear regression\nHow to run a regression in R\nHow to use R and dplyr to analyze stock data\n\n\n\nR Packages\ndplyr\nggplot2\ntidyquant\ntidyr\n\n\nData\nStock data: Daily stock prices for Apple, Disney, and the NASDAQ.\n\n\nGrade\nYou will turn in a short reflection on Canvas at the end of the lab."
  },
  {
    "objectID": "02_assignments/lab-06.html#step-1-download-and-format-stock-data",
    "href": "02_assignments/lab-06.html#step-1-download-and-format-stock-data",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 1: Download and Format Stock Data",
    "text": "Step 1: Download and Format Stock Data\nWe will use the tidyquant package to download stock data. If you want to know more about this package, watch this video. Use the following code to download the stock prices from 2019 to 2024 for Apple (AAPL), Disney (DIS), and the NASDAQ.\nThe NASDAQ indicator typically refers to the NASDAQ Composite Index (symbol: ^IXIC), which tracks the performance of over 3,000 stocks listed on the NASDAQ stock exchange. This index is heavily weighted toward technology companies but also includes firms from sectors like healthcare, consumer services, and finance. Stock market indicators, such as the NASDAQ, Dow Jones Industrial Average, and S&P 500, serve as benchmarks for the overall performance of the stock market or specific sectors. These indicators provide insights into market trends, investor sentiment, and the health of the economy, helping investors make informed decisions.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidyquant)\n\n\nstocks <- tq_get(c(\"AAPL\", \"DIS\", \"^IXIC\"),\n                 get  = \"stock.prices\",\n                 from = \" 2019-01-01\",\n                 to = \"2024-01-01\")\n\nWe need to use pivot_wider() from the tidyr package to reshape the data so it has a column for the date, and the stock price on each date for each stock. This function is very similar to pivot_longer(), but it changes the data so there are more columns and fewer rows. Here is the vignette about pivot_wider.\n\nstocks <- stocks |> \n  select(symbol, date, open) |> \n  pivot_wider(names_from = symbol,\n              values_from = open) |> \n  rename(nasdaq =`^IXIC`) # rename the nasdaq to be less difficult"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-2-calculate-return",
    "href": "02_assignments/lab-06.html#step-2-calculate-return",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 2: Calculate Return",
    "text": "Step 2: Calculate Return\nWe want to get daily stock returns (changes) rather than levels (prices), so we need to calculate them using dplyr. The formula for returns is: \\[return_t = \\frac{price_t-price_{t-1}}{price_{t-1}}*100 = (\\frac{price_t}{price_{t-1}}-1)*100\\]\nTo calculate this, we will use the following code:\n\nstocks <- stocks |> \n  mutate(apple_return = (AAPL/lag(AAPL)-1)*100,\n         disney_return = (DIS/lag(DIS)-1)*100,\n         nasdaq_return = (nasdaq/lag(nasdaq)-1)*100)\n\nThis tells R to add new variables that are calculated from the value and lagged values from other variables. You can use head(stocks) to see the returns for the first 5 observations.\n\nExercise\n\nUse the summary() function to check out the stock returns for the two stocks and indicator. What is the average daily return?\nUse the following code to find out which day had the lowest return: stocks$date[which.min(stocks$nasdaq_return)]. Does that make sense?\nCalculate the total return over the 5 years with the following code: (tail(stocks$nasdaq, 1)/head(stocks$nasdaq, 1)-1)*100. Does this amount of growth surprise you? How does that compare to the 5 year growth of Disney and Apple?"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-3-plot-the-data",
    "href": "02_assignments/lab-06.html#step-3-plot-the-data",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 3: Plot the Data",
    "text": "Step 3: Plot the Data\nMake a scatterplot with the NASDAQ return on the x axis and Apple return on the y axis using ggplot() and geom_point(). Label the axes using xlab() and ylab() and use theme_classic() to make the plot look like the one below.\n\n\n\n\n\n\nExercise\n\nWhat is the relationship between the NASDAQ and Apple? Is it positive or negative? Is it strong? Is it a one-to-one increase, or is the slope different from 1?\nCreate the same plot for Disney.\nCompare the plots. Which one seems to have a stronger relationship with the market?"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-4-use-a-regression-to-formally-analyze",
    "href": "02_assignments/lab-06.html#step-4-use-a-regression-to-formally-analyze",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 4: Use a Regression to Formally Analyze",
    "text": "Step 4: Use a Regression to Formally Analyze\nRather than just exploring charts to determine how closely the stock follows the market, regression can provide formality and precision. In R, the function for regression is lm(), which stands for “linear model”. It generally takes the form lm(y ~ x, data = dataset_name). Run the following regression:\n\napple_model <- lm(apple_return ~ nasdaq_return, data = stocks)\nsummary(apple_model)\n\nThis is an important regression in finance. It is generally written as: \\[ stock return = \\alpha + \\beta * indicator return \\]\n\\(\\beta\\) is an indicator of how volatile the stock is compared to the market. If \\(\\beta\\) is above 1, that means the stock is more volatile or risky than the market, and if it is below 1, the stock is less volatile or risky than the market. Tech stocks tend to be high beta, because the stocks are based on potential success. Stocks like Proctor & Gamble, which makes many household goods and has been around for a long time, tend to be low beta because they are stable and low-growth.\n\\(\\alpha\\) is an indicator of how the stock performs compared to the market. If \\(\\alpha\\) is greater than 0, then, on average, the stock grows even when the market growth is 0. If \\(\\alpha\\) is less than 0, the stock does worse than the market. One thing you’ll hear investors say is that they are “chasing alpha”, or trying to find stocks that outperform the market.\n\nExercise\n\nWhat are the \\(\\alpha\\) and \\(\\beta\\) coefficients for Apple?\nRun the same regression for Disney instead of Apple. Compare the coefficients. Do either of them outperform the market? Which one is more volatile? Does that match what you saw in the graphs?"
  },
  {
    "objectID": "02_assignments/lab-06.html#step-5-add-regression-line-to-plot",
    "href": "02_assignments/lab-06.html#step-5-add-regression-line-to-plot",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Step 5: Add Regression Line to Plot",
    "text": "Step 5: Add Regression Line to Plot\nTo add the regression line to the plot, use the same ggplot() code from before, but add another component. After one of the plus signs in your ggplot code, add this:\n\nstat_smooth(mapping = aes(x = nasdaq_return, y = apple_return), data = stocks,\n            method = \"lm\", geom = \"smooth\") +\n\nYou should get a regression line with confidence intervals in gray."
  },
  {
    "objectID": "02_assignments/lab-06.html#assignment",
    "href": "02_assignments/lab-06.html#assignment",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Assignment",
    "text": "Assignment\nWrite a few sentences describing how Disney and Apple stock compare to the overall market and to each other by interpreting their \\(\\alpha\\) and \\(\\beta\\) values. Turn this in on Canvas."
  },
  {
    "objectID": "02_assignments/lab-06.html#further-exploration",
    "href": "02_assignments/lab-06.html#further-exploration",
    "title": "Lab 6: Stock Prices and Simple Linear Regression",
    "section": "Further exploration",
    "text": "Further exploration\nHere are some more exercises if you want more practice. 1. Download the data for Tesla for the first 5 years it was public (it went public on June 29, 2010). Compare the \\(\\alpha\\) and \\(\\beta\\) to the recent data. 2. Explore a different stock of your choice, for example Petco (WOOF), Coca-Cola (KO), or Harley-Davidson (HOG). 3. Try creating a line graph of a stock over time using ggplot() and geom_line().\n\nMore Finance with R\nIf you want a longer introduction of how to do financial analysis with R, you can check out this site or this e-book."
  }
]