{
  "hash": "272dc91d0f932dad6593b057ff996789",
  "result": {
    "markdown": "---\ntitle: \"Lab 9: Transformations and Interactions\"\neditor: visual\n---\n\n\n\n\n## Outline\n\n### Objectives\n\n-   Perform and interpret a polynomial regression\n\n-   Log transform a variable, incorporate it into a regression, and interpret it\n\n-   Utilize and interpret interaction terms in regression\n\n### Data\n\nWe will load the `CASchools` dataset from an R package. This dataset includes data about 420 schools in California in 1996-1998. [This website](https://www.rdocumentation.org/packages/AER/versions/1.2-10/topics/CASchools) has information about each of the variables.\n\n### Packages\n\n-   `AER`: has the data we will use in the lab\n\n-   `ggplot2`: used for graphing\n\n### Grade\n\nAt the end of the demonstration, there is an assignment for you to do with different data. You will answer associated questions on Canvas.\n\n## Step 1: Load in the data\n\nTo do the lab, we will load in the data from the `AER` package. We also need to calculate the average class size, and create a score that is the average of the reading and math score for the school. This score will be our primary outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls()) # Clears the R environment\n\n# Read in libraries and data\nlibrary(ggplot2)\nlibrary(AER)\ndata(CASchools)\n\n# Calculate the number of students per teacher\nCASchools$size <- CASchools$students/CASchools$teachers\n# Calculate the average of math and reading score\nCASchools$score <- (CASchools$read + CASchools$math) / 2\n```\n:::\n\n\n## Step 2: Income and School Quality Analysis - Polynomials\n\n### Linear Regression\n\nThe first thing we are going to explore with the data is how the average income in the district is related to the reading and math score. What do you think the relationship between income and test scores would be?\n\nWe can first test this with a simple correlation: `cor(CASchools$score, CASchools$income)`. There is a positive relationship! But let's run a regression and plot the points to be more thorough.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit a simple linear model\nlinear_model <- lm(score ~ income, data = CASchools)\nsummary(linear_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = score ~ income, data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.574  -8.803   0.603   9.032  32.530 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 625.3836     1.5324  408.11   <2e-16 ***\nincome        1.8785     0.0905   20.76   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.39 on 418 degrees of freedom\nMultiple R-squared:  0.5076,\tAdjusted R-squared:  0.5064 \nF-statistic: 430.8 on 1 and 418 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nGreat! If the average income in a district is \\$1,000 higher, the expected score is 1.88 higher. Income ranges from 5,000 to 55,000, and the scores range from 605 to 707, so that is a pretty big effect! Going from the 1st quartile (\\$10,639) to the 3rd quartile (\\$17,629) would increase the score by two thirds of a standard deviation. \n\nNow, let's plot the data to see what it looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the observations\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~x, se = F, color = \"steelblue\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lab-09_files/figure-html/unnamed-chunk-3-1.png){fig-alt='A scatterplot with income on the x axis and score on the y axis. The points have a steep positive relationship from 0 to 10 thousand dollars, but past 10,000 dollars, the relationship is less strong. There is a linear regression line running through that does not fit the points well.' width=672}\n:::\n:::\n\n\nIn a linear regression, X and Y are supposed to be linearly related. That means that the effect that X has on Y should be the same at every value of X. However, in this example, it is clearly not the case! The points look curved, but the linear regression line is, well, linear. The regression predicts too high of test scores for low- and high-income districts, but too low of test scores for middle income districts.\n\nIncome tends to be strongly right skewed, meaning that most of the observations are grouped around the median, but there are some very high values that make the mean high.\n\nLet's plot a histogram of income to see if this income variable is right skewed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(CASchools, aes(x = income)) +\n  geom_histogram() +\n  xlab(\"Income\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lab-09_files/figure-html/unnamed-chunk-4-1.png){fig-alt='A right-skewed histogram of income.' width=672}\n:::\n:::\n\n\nThe data is right skewed, so we probably need to correct for that. We can try a few different specifications that will fit the data better.\n\n### Quadratic Regression\n\nWe can model test scores as a function of income and income squared. The corresponding regression model is:\n\n$$ TestScore_i = \\beta_0 + \\beta_1income_i + \\beta_2income_i^2 + \\varepsilon_i $$ This is called a **quadratic regression model**. $income^2$ is treated as an additional explanatory variable. Note that since $income^2$ is not a *linear* transformation of $income$, it does not create perfect multicollinearity. \n\nIf the relationship between test scores and district income is linear, then the coefficient on $income^2$ will not be significantly different from 0. If the relationship is quadratic, then the relationship will be significantly different from zero. This corresponds to the hypothesis test:\n\n$$ H_0: \\beta_2 = 0 $$ $$H_A: \\beta_2 \\neq 0 $$\n\nThe p-value we use to test this hypothesis is in the last column of the summary table.\n\nWe will use the `poly(x, k)` function, which will turn whatever `x` variable we input into a `k`th order polynomial.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = score ~ poly(income, 2), data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.416  -9.048   0.440   8.347  31.639 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)      654.1565     0.6209 1053.633  < 2e-16 ***\npoly(income, 2)1 277.8568    12.7238   21.838  < 2e-16 ***\npoly(income, 2)2 -85.9935    12.7238   -6.758 4.71e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.72 on 417 degrees of freedom\nMultiple R-squared:  0.5562,\tAdjusted R-squared:  0.554 \nF-statistic: 261.3 on 2 and 417 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe regression equation is:\n$$\\hat{TestScore}_i = 654.2 +277.9*income-85.99*income^2$$\n\n\nWe now draw the same scatter plot as for the linear model and add the regression line for the quadratic model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~x, se = F, color = \"steelblue\") +\n  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F, color = \"red\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lab-09_files/figure-html/unnamed-chunk-6-1.png){fig-alt='A scatterplot with income on the x axis and score on the y axis. There is a linear prediction line, which does not fit well, and a curved prediction line for the quadratic regression that fits better.' width=672}\n:::\n:::\n\n\nThe quadratic function clearly fits the data much better than the linear model.\n\n### Polynomial Regression\n\nWe could generalize the quadratic model to include any number of polynomial degrees. $$ \\hat{Y}_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i^2 + \\beta_3X_i^3 + ... + \\beta_kX_i^k $$ A cubic model, for instance, is a polynomial model with the square and cube of the variable, so k = 3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate a cubic model\ncubic_model <- lm(score ~ poly(income, 3), data = CASchools)\nsummary(cubic_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = score ~ poly(income, 3), data = CASchools)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44.28  -9.21   0.20   8.32  31.16 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)        654.16       0.62 1055.034  < 2e-16 ***\npoly(income, 3)1   277.86      12.71   21.867  < 2e-16 ***\npoly(income, 3)2   -85.99      12.71   -6.767 4.47e-11 ***\npoly(income, 3)3    18.46      12.71    1.452    0.147    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.71 on 416 degrees of freedom\nMultiple R-squared:  0.5584,\tAdjusted R-squared:  0.5552 \nF-statistic: 175.4 on 3 and 416 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nOne problem with including an extra polynomial is that, as we add additional dimensions, the model becomes harder and harder to interpret. Additionally, we only want to include predictors that make our model better. What happened to the $Adjusted R^2$ between the quadratic regression and the cubic regression? Is the cubic term significant?\n\n**See if you can add the line from the cubic equation to your scatterplot.**\n\n### Interpreting Polynomial Models\n\nPolynomials are difficult to interpret. One method for interpreting polynomials is to take the derivative of the equation. For instance, the cubic model has the following regression equation: $$ score_i = 654 + 278*income_i -86*income_i^2 +19*income_i^3 +\\varepsilon_i $$\n\nThe derivative of the equation with respect to income would be: $$ \\frac{\\partial{score_i}}{\\partial{income_i}} = 278-86*2*income_i+19*3*income_i^2$$\n$$=278-172*income_i+57*income_i^2 $$\n\nWe would get the following interpretation: \"A one-unit increase in income would lead to a $278-172income_i+57income_i^2$  increase in expected test score.\" Confusing!\n\nAlternatively, you can just show a graph of the relationship and predict the effects at a few different values. Let's predict what happens with the quadratic model when we change income at a few different levels. First, we will test what happens when income goes from 10,000 to 11,000. (Remember that in the data, income is in 1,000s.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up data for prediction\nnew_data <- data.frame(income = c(10, 11))\n# do the prediction\nY_hat <- predict(quadratic_model, newdata = new_data)\n# compute the difference\ndiff(Y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2 \n2.962517 \n```\n:::\n:::\n\n\nHow does that compare to the change if income goes from 40 to 41?\n\n## Step 3: Income and School Quality Analysis - Logarithms\n\nAnother way to specify a nonlinear regression function is to use the natural logarithm of X and/or Y. Logarithms convert changes in variables into percentage changes. This is convenient as many relationships are naturally expressed in terms of percentages. Often, if you are using a right-skewed variable like income, you should automatically take the logarithm.\n\nThere are three different kinds of models with log transformations:\n\n1.  Level-Log: Transform X to be log(X), but do not transform Y (use Y's *level*).\n\n2.  Log-Level: Transform Y to be log(Y), but do not transform X (use X's *level*).\n\n3.  Log-Log: Transform X and Y so that log(Y) \\~ log(X).\n\nThe interpretation of the regression coefficients is different in each case. This table shows the interpretation by case:\n\n| Case      | Equation                                                                                         | Interpretation of $\\beta_1$                                                                                            |\n|------------------|------------------|-------------------------------------|\n| Level-Log | $$                                                                                               \n                                                                 \\hat{Y}_i=\\beta_0+\\beta_1ln(X_i)              \n                                                                 $$                                            | A 1% change in X is associated with a $0.01*\\beta_1$ change in Y.                                                      |\n| Log-Level | $$                                                                                               \n                                                                 ln(\\hat{Y_i})=\\beta_0+\\beta_1X_i              \n                                                                 $$                                            | A 1 unit change in X is associated with a $100*\\beta_1\\%$ change in Y.                                                 |\n| Log-Log   | $$                                                                                               \n                                                                 ln(\\hat{Y_i})=\\beta_0+\\beta_1ln(X_i)          \n                                                                 $$                                            | A 1% change in X is associated with a $\\beta_1\\%$ change in Y. This is also the **elasticity** of Y with respect to X. |\n\n### Case 1: Log X, Level Y\n\nThe regression form of this model is: $$ \\hat{Y}_i=\\beta_0 + \\beta_1ln(X_i) $$\n\nWe do not have to create a new variable to use lm(). We can simply let R know that we want to log-transform X within the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate a level-log model\nLinearLog <- lm(score ~ log(income), data = CASchools) # log() is the natural log\nsummary(LinearLog)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = score ~ log(income), data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.256  -9.050   0.078   8.230  31.214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  557.832      4.200  132.81   <2e-16 ***\nlog(income)   36.420      1.571   23.18   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.62 on 418 degrees of freedom\nMultiple R-squared:  0.5625,\tAdjusted R-squared:  0.5615 \nF-statistic: 537.4 on 1 and 418 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# Plot the function\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~log(x), se = F, color = \"purple\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lab-09_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nUsing log transformations allows us to interpret the model using percent changes rather than unit changes. For instance, if the coefficient were 42.6, we would say that \"a 1% increase in income is associated with a 0.01\\*36.4=.364 expected increase in average score\".\n\nIf we wanted, we could also calculate the estimated effect at different values of X like we did for polynomials.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up new data\nnew_data <- data.frame(income = c(10, 11, 40, 41))\n# predict the outcomes\nY_hat <- predict(LinearLog, newdata = new_data)\n# compute the expected difference\nY_hat_matrix <- matrix(Y_hat, nrow = 2, byrow = TRUE)\nY_hat_matrix[, 2] - Y_hat_matrix[, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.471166 0.899297\n```\n:::\n:::\n\n\nGoing from 10 to 11 thousand dollars has a bigger effect than going from 40 to 41 thousand dollars.\n\n### Case 2: Level X, Log Y\n\nWe can just log the dependent variable and keep x level. The model is: $$ ln(Y_i) = \\beta_0 + \\beta_1X_i +\\varepsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate a log-linear model\nLogLinear <- lm(log(score) ~ income, data = CASchools)\nsummary(LogLinear)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(score) ~ income, data = CASchools)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.06285 -0.01340  0.00114  0.01413  0.04913 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.4393623  0.0023638 2724.16   <2e-16 ***\nincome      0.0028441  0.0001396   20.37   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02065 on 418 degrees of freedom\nMultiple R-squared:  0.4982,\tAdjusted R-squared:  0.497 \nF-statistic:   415 on 1 and 418 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe estimated regression function is: $$ ln(TestScore_i) = 6.439 + 0.00284*income_i+\\varepsilon_i $$ This means that if income increases by 1 unit (\\$1,000), then expected test score increases by $100*0.00284=0.284\\%$.\n\n### Case 3: Log X, Log Y\n\nThe log-log regression model is:\n\n$$\nln(Y_i) = \\beta_0+\\beta_1ln(X_i)=\\varepsilon_i\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate the log-log model\nLogLog <- lm(log(score) ~ log(income), data = CASchools)\nsummary(LogLog)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(score) ~ log(income), data = CASchools)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.066458 -0.013658  0.000508  0.012903  0.047856 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.336349   0.006453  981.90   <2e-16 ***\nlog(income) 0.055419   0.002414   22.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01938 on 418 degrees of freedom\nMultiple R-squared:  0.5578,\tAdjusted R-squared:  0.5567 \nF-statistic: 527.2 on 1 and 418 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe equation from this model is: $$ ln(TestScore_i) = 6.34 + 0.06*ln(income_i) +\\varepsilon_i $$\n\nA 1% increase in thousands of dollars of income is associated with a 0.06 % increase in test score.\n\nThe following code plots all of the log models on top of the scatterplot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCASchools$pred_log_level <- exp(predict(LogLinear))\nCASchools$pred_log_log <- exp(predict(LogLog))\n\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y ~ x, se = F, color = \"green4\") +\n  geom_smooth(method = 'lm', formula = y ~ log(x), se = F, color = \"purple\") +\n  geom_line(aes(y = pred_log_level), color = \"blue\", size = 1) +\n  geom_line(aes(y = pred_log_log), color = \"red3\", size = 1) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lab-09_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThe log-level and level-level models look essential the same, while the level-log and log-log models look the same.\n\n## Step 4: Picking the Best Model\n\nWe can compare the adjusted R2 to see if any model has a significantly better fit than the rest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the adj. R^2 for the nonlinear models\nadj_R2 <-rbind(\"linear\" = summary(linear_model)$adj.r.squared,\n\"quadratic\" = summary(quadratic_model)$adj.r.squared,\n\"cubic\" = summary(cubic_model)$adj.r.squared,\n\"LinearLog\" = summary(LinearLog)$adj.r.squared,\n\"LogLinear\" = summary(LogLinear)$adj.r.squared,\n\"LogLog\" = summary(LogLog)$adj.r.squared\n)\n# assign column names\ncolnames(adj_R2) <- \"adj_R2\"\nadj_R2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             adj_R2\nlinear    0.5063795\nquadratic 0.5540444\ncubic     0.5552279\nLinearLog 0.5614605\nLogLinear 0.4970106\nLogLog    0.5567251\n```\n:::\n:::\n\n\nThe Linear and Log-Linear models both have lower $R^2$ values, but the others are pretty similar. So how do we choose a model?\n\nLet's look at a plot that shows both the linear-log and quadratic models, potentially our two best models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the observations\nggplot(CASchools, aes(x = income, y = score)) +\n  geom_point() +\n  xlab(\"District Income (thousands of dollars)\") +\n  ylab(\"Test Score\") +\n  geom_smooth(method = 'lm', formula = y~poly(x,2), se = F, color = \"steelblue\") +\n  geom_smooth(method = 'lm', formula = y~log(x), se = F, color = \"red\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lab-09_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nNeither appears to be more appropriate for the data. I would then choose based on theory.\n\n1.  It's easier to interpret the linear-log model, and it makes sense that income would be interpreted as percent changes since it is right skewed.\n\n2.  In the quadratic model, if we continued to higher values of income, scores would eventually go down, which doesn't make much sense.\n\nI would go with the linear-log model.\n\n## Step 5: Class Size, Region, and Scores: Dummy Variable Interactions\n\nThe last section dealt with transformations to make non-linear data into linear data. Now, we'll look at interactions.\n\n\nI'm interested in whether class size affects test scores. I also think that the effect will vary based on region - in some regions like the bay area, students will be privileged enough that class size won't matter as much, but in poorer regions like the central valley, I think class size will have a larger effect.\n\nI will create a region variable, then do a regression with the interaction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create region variable\nCASchools$region <- \"Northern CA\"\nCASchools$region[CASchools$county %in% c(\"Los Angeles\",\n                                         \"San Bernardino\",\n                                         \"Riverside\",\n                                         \"San Diego\",\n                                         \"Imperial\")] <- \"Southern CA\"\nCASchools$region[CASchools$county %in% c(\"Alameda\", \n                                         \"Contra Costa\", \n                                         \"Marin\", \n                                         \"San Francisco\", \n                                         \"San Mateo\", \n                                         \"Santa Clara\", \n                                         \"Solano\")] <- \"Bay Area\"\nCASchools$region[CASchools$county %in% c(\"Fresno\", \n                                         \"Inyo\", \n                                         \"Kern\", \n                                         \"Kings\", \n                                         \"Tulare\", \n                                         \"Alpine\", \n                                         \"Amador\", \n                                         \"Calaveras\", \n                                         \"Madera\", \n                                         \"Mariposa\", \n                                         \"Merced\", \n                                         \"Mono\",\n                                         \"San Joaquin\",\n                                         \"Stanislaus\", \n                                         \"Tuolumne\")] <- \"Central CA\"\nCASchools$region[CASchools$county %in% c(\"Monterey\",\n                                         \"San Benito\",\n                                         \"San Luis Obispo\",\n                                         \"Santa Barbara\",\n                                         \"Santa Cruz\",\n                                         \"Ventura\")] <- \"Central Coast\"\ndummy_interaction <- lm(score ~ size + region + region*size, data = CASchools)\nsummary(dummy_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = score ~ size + region + region * size, data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.385 -11.779   0.117  11.505  39.283 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               798.139     24.954  31.985  < 2e-16 ***\nsize                       -6.745      1.312  -5.142 4.22e-07 ***\nregionCentral CA         -146.829     30.003  -4.894 1.42e-06 ***\nregionCentral Coast       -40.919     41.065  -0.996   0.3196    \nregionNorthern CA        -148.694     28.380  -5.239 2.58e-07 ***\nregionSouthern CA         -86.391     36.227  -2.385   0.0175 *  \nsize:regionCentral CA       6.294      1.556   4.046 6.22e-05 ***\nsize:regionCentral Coast    1.760      2.079   0.847   0.3978    \nsize:regionNorthern CA      7.206      1.489   4.840 1.85e-06 ***\nsize:regionSouthern CA      3.768      1.819   2.071   0.0390 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.22 on 410 degrees of freedom\nMultiple R-squared:  0.2906,\tAdjusted R-squared:  0.275 \nF-statistic: 18.66 on 9 and 410 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe reference category is the \"Bay Area\" because it is first alphabetically. If we wanted to use a different reference level, we can change that using the `relevel()` function. For isntance, if we want to change the reference level to be Central California, we would run:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCASchools$region <- relevel(as.factor(CASchools$region), ref = \"Central CA\")\n```\n:::\n\n\nHowever, we will stick with the Bay Area as our reference for this example.\n\n### Dummy Variables\n\nWe'll first look at the dummy variables. The coefficient on \"Central California\" is -146.8, and it is very significant. What this says is that, for every class size, the expected test score is 146.8 points lower in Central California than in the Bay Area. The range of the score variable is 605 to 707, so this is a huge difference! Northern California and Southern California also have significantly lower scores than the Bay Area.\n\n### Numeric Variable\n\nThe coefficient on class size is -6.7. For every additional child per teacher on average, the expected score lowers by almost 7 points. This makes sense because we know class size is an important determinant of student outcomes. Class size ranges from 14 to 26, so if a class jumps from the lowest class size to the highest, expected scores would decline by about 7\\*8 = 56 points.\n\nHowever, since we have interactions, the only place that has the coefficient -6.7 on class size is the Bay Area.\n\n### Interactions\n\nThe interaction between the Northern California region and class size is 7.2. That means that, in a district in Northern California, the coefficient on class size is $\\beta_1+\\beta_3=-6.7+7.2=0.5$. If average class size increases by one student, the expected test score increases by half a point. (In reality, if we ran a regression on Northern California alone, this probably wouldn't be a significant variable.)\n\nWhat we can take away from this is that larger classes have the most negative effect in the Bay Area, while in Northern California and Central California, they don't seem to make a difference. The opposite of my hypothesis!\n\n## Step 6: Poverty, Expenditures per Student, and Scores: Continuous Interactions\n\nIf we have an interaction between two continuous variables, the interpretation is a little more confusing. Let's try. I hypothesize that in schools where more of the students are on free or reduced lunch, expenditures will have a more positive effect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninteraction <- lm(score ~ lunch + expenditure + lunch*expenditure, data = CASchools)\nsummary(interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = score ~ lunch + expenditure + lunch * expenditure, \n    data = CASchools)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.975  -5.489   0.024   6.075  34.192 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        6.474e+02  6.127e+00 105.656  < 2e-16 ***\nlunch             -2.949e-01  1.280e-01  -2.304   0.0217 *  \nexpenditure        6.295e-03  1.116e-03   5.639 3.16e-08 ***\nlunch:expenditure -5.705e-05  2.341e-05  -2.436   0.0153 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.027 on 416 degrees of freedom\nMultiple R-squared:  0.7771,\tAdjusted R-squared:  0.7755 \nF-statistic: 483.5 on 3 and 416 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n-   The coefficient on lunch is -0.29, which means that if the percent of students on free lunch goes up by one percentage point, the expected score goes down by 0.29 points.\n\n-   The coefficient on expenditure per student is 0.006, which means that if expenditure per student goes up by \\$1,000, the expected score goes up by 6.\n\nHow do we interpret the interaction term? The term is very small and negative. This means that if there\\\nare more students on free lunch, the effect of more expenditures is less positive. Or, alternatively, if\\\nexpenditure is higher, then the effect of having more students on free lunch is more negative. (I would\\\nguess the first interpretation is more accurate.)\n\nJust like with polynomials, we could also use derivatives to calculate the effect.\n\n$$\n\\hat{Y}=\\beta_0+\\beta_1X+\\beta_2Z+\\beta_3X*Z\n$$\n\nTaking the derivative with respect to X:\n\n$$\n\\frac{\\partial{Y}}{\\partial{X}}=\\beta_1+\\beta_3*Z\n$$\n\nSo, the effect of X is dependent on Z. Let's calculate the effect of expenditure at the median value of free school lunch percent, 41.8.\n\n$$\nY=647.4+(-0.29-0.000057*41.8)*X=647.4-0.29X\n$$\n\nPractically, this effect isn't significant, even if it's statistically significant. (See those little stars?) However, this still gives you the idea of what the point of interaction terms is. You should only include interaction terms if you think there is a good reason for them to be included.\n\nIf I were to report these results to a policy expert, I would tell them that in areas with more students on free or reduced lunch, it will take more resources to get similar outcomes.\n\n## Assignment\n\nDo a similar analysis to the above using the life expectancy/insurance data. Follow these steps, and answer the questions on Canvas.\n\n### Step 1: Load in the data\n\nYou should have the code to load and merge the data from lab 7. (If you do not, you can email me telling me the importance of saving your R scripts, and I will give you the code or data.) You will need all of the sheets loaded and merged: life expectancy, income, insurance, and region. You will also need to calculate the uninsured rate like you did before.\n\n### Step 2: Income and Life Expectancy: Polynomial Regressions\n\nGraph the relationship between income per capita (x axis) and female life expectancy (y axis), like we did in step 2. Run a linear regression predicting female life expectancy from **income per capita**. Then, run a quadratic regression.\n\n### Step 3: Income and Life Expectancy: Log Transformations\n\nRun a regression where you log transform median household income, and one where you transform both median household income and life expectancy.\n\n### Step 4: Picking the Best Model\n\nCompare all the models you ran.\n\n### Step 5: Dummy-Numeric Interactions\n\nPerform a regression with female life expectancy as the outcome variable and **region**, **uninsured rate**, and the **interaction between region and uninsured rate** as your explanatory variables.\n\n### Step 6: Numeric-Numeric Interactions\n\nPerform a regression with life expectancy as the outcome variable and **uninsured rate**, **median household income**, and the **interaction between uninsured rate and median household income** as the predictor variables. (Do not take the log of income, even though you probably would in a real analysis.)\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "lab-09_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}